{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558fabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "import wandb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd740b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe8ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "from gymnasium import spaces \n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "from training_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd055c9a",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e98f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "def parse_args(default_config_path=\"./train_config/train_new_rew.yaml\"):\n",
    "    \"\"\"\n",
    "    Parse arguments from CLI or notebook.\n",
    "    - In notebook: usa il default se non passato\n",
    "    - In CLI: permette override dei parametri nel config\n",
    "    \"\"\"\n",
    "    # --- Gestione notebook: evita crash su ipykernel args ---\n",
    "    argv = sys.argv[1:]\n",
    "    # Se siamo in notebook o non è passato il config_path, inseriamo il default\n",
    "    if len(argv) == 0 or \"--f=\" in \" \".join(argv):\n",
    "        argv = [default_config_path]\n",
    "\n",
    "    # --- Pre-parser per leggere il config_path ---\n",
    "    pre_parser = argparse.ArgumentParser(add_help=False)\n",
    "    pre_parser.add_argument(\n",
    "        \"config_path\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=default_config_path,\n",
    "        help=\"Main config file path\"\n",
    "    )\n",
    "    initial_args, remaining_argv = pre_parser.parse_known_args(argv)\n",
    "    CONFIG_PATH = initial_args.config_path\n",
    "    print(f\"Config path: {CONFIG_PATH}\")\n",
    "\n",
    "    # --- Legge parametri dal file di config ---\n",
    "    file_config_dict = parse_config_file(CONFIG_PATH)\n",
    "\n",
    "    # --- Parser principale ---\n",
    "    parser = argparse.ArgumentParser(description=\"Training Script\")\n",
    "    parser.add_argument(\n",
    "        \"config_path\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=CONFIG_PATH,\n",
    "        help=\"Main config file path\"\n",
    "    )\n",
    "\n",
    "    # Aggiunge parametri dal config file, con tipi corretti\n",
    "    for key, value in file_config_dict.items():\n",
    "        if isinstance(value, bool):\n",
    "            parser.add_argument(f\"--{key}\", type=str2bool, default=value)\n",
    "        elif value is None:\n",
    "            parser.add_argument(f\"--{key}\", type=str, default=value)\n",
    "        else:\n",
    "            parser.add_argument(f\"--{key}\", type=type(value), default=value)\n",
    "\n",
    "    # --- Parse finale con remaining_argv per ignorare args extra Jupyter ---\n",
    "    args, unknown = parser.parse_known_args(remaining_argv)\n",
    "    if unknown:\n",
    "        print(\"Ignored unknown args:\", unknown)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ba1ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config path: ./train_config/train_new_rew.yaml\n",
      "Training with the following parameters:\n",
      "{'actor_network_layers': [128, 128, 128],\n",
      " 'agent_config_path': './train_config/agent_new_rew.yaml',\n",
      " 'alpha': 1.0,\n",
      " 'alpha_lr': 0.0002,\n",
      " 'autotune': True,\n",
      " 'base_time': 1765457030,\n",
      " 'batch_size': 512,\n",
      " 'bootstrap': False,\n",
      " 'bootstrap_batch_proportion': 0.8,\n",
      " 'buffer_size': 100000,\n",
      " 'build_path': './unity_build/6xnew_reward_wind/UASRL.exe',\n",
      " 'config_path': './train_config/train_new_rew.yaml',\n",
      " 'cuda': 0,\n",
      " 'curriculum_steps': 5,\n",
      " 'env_id': '6xstd',\n",
      " 'exp_name': 'forse_va',\n",
      " 'gamma': 0.99,\n",
      " 'headless': False,\n",
      " 'input_stack': 4,\n",
      " 'learning_starts': 1000,\n",
      " 'loss_log_interval': 500,\n",
      " 'machine_name': 'personal',\n",
      " 'metrics_log_interval': 5,\n",
      " 'metrics_smoothing': 0.96,\n",
      " 'min_episodes_per_curriculum': 50,\n",
      " 'min_success_rate': 0.8,\n",
      " 'n_envs': 6,\n",
      " 'obstacles_config_path': './train_config/obstacles_new_rew.yaml',\n",
      " 'other_config_path': './train_config/other_new_rew.yaml',\n",
      " 'policy_frequency': 1,\n",
      " 'policy_lr': 0.005,\n",
      " 'q_ensemble_n': 5,\n",
      " 'q_lr': 0.005,\n",
      " 'q_network_layers': [128, 128, 128],\n",
      " 'reward_scale': 1.0,\n",
      " 'seed': 16949,\n",
      " 'target_network_update_period': 2,\n",
      " 'tau': 0.005,\n",
      " 'test_lib': False,\n",
      " 'torch_deterministic': False,\n",
      " 'total_timesteps': 2000000,\n",
      " 'update_frequency': 2,\n",
      " 'wandb': True,\n",
      " 'worker_id': 0}\n",
      "agent_config:\n",
      "{'action_debug': False,\n",
      " 'action_penality': 0.01,\n",
      " 'cbf_debug': False,\n",
      " 'ema_debug': False,\n",
      " 'goal_reward': 10,\n",
      " 'hudHeight': 560,\n",
      " 'hudScale': 1.0,\n",
      " 'hudWidth': 320,\n",
      " 'hudX': 10,\n",
      " 'hudY': 10,\n",
      " 'hud_debug': False,\n",
      " 'max_movement_speed': 6,\n",
      " 'max_step': 1000,\n",
      " 'max_turn_speed': 140,\n",
      " 'move_smooth_time': 0.1,\n",
      " 'obstacle_test': False,\n",
      " 'print_debug': False,\n",
      " 'print_every_step': 2000,\n",
      " 'progress_reward': 0.1,\n",
      " 'step_after_goal': 10,\n",
      " 'time_penality': 0.002,\n",
      " 'wall_hit_penalty': 0}\n",
      "obstacles_config:\n",
      "{'check_path_try': 25,\n",
      " 'clear_threshold': 2,\n",
      " 'debug': False,\n",
      " 'external_walls': True,\n",
      " 'fill_threshold': 6,\n",
      " 'initial_fill_percentage': 0.42,\n",
      " 'moving_obstacles_count': 0,\n",
      " 'obstacles_total_width': 40,\n",
      " 'smoothing_iterations': 2,\n",
      " 'wall_height': 2,\n",
      " 'wall_resolution': 20}\n",
      "other_config:\n",
      "{'action_size': 2,\n",
      " 'behavior_name': 'NavigationAgent',\n",
      " 'max_action': 1.0,\n",
      " 'min_action': -1.0,\n",
      " 'rays_max_observation': 1.0,\n",
      " 'rays_min_observation': -1.0,\n",
      " 'rays_per_direction': 10,\n",
      " 'state_max_observation': 128.0,\n",
      " 'state_min_observation': -128.0,\n",
      " 'state_observation_size': 7,\n",
      " 'team': '0'}\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "agent_config = parse_config_file(args.agent_config_path)\n",
    "obstacles_config = parse_config_file(args.obstacles_config_path)\n",
    "other_config = parse_config_file(args.other_config_path)\n",
    "\n",
    "args.seed = random.randint(0, 2**16)\n",
    "# args.name = generate_funny_name()\n",
    "\n",
    "print('Training with the following parameters:')\n",
    "pprint(vars(args))\n",
    "\n",
    "print('agent_config:')\n",
    "pprint(agent_config)\n",
    "\n",
    "print('obstacles_config:')\n",
    "pprint(obstacles_config)\n",
    "\n",
    "print('other_config:')\n",
    "pprint(other_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8836c969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and args.cuda >= 0:\n",
    "    # F-string per inserire l'indice: diventa \"cuda:2\"\n",
    "    device_str = f\"cuda:{args.cuda}\"\n",
    "else:\n",
    "    device_str = \"cpu\"\n",
    "\n",
    "DEVICE = torch.device(device_str)\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1cd5e",
   "metadata": {},
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dcecf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 16949\n"
     ]
    }
   ],
   "source": [
    "# seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "print(f'Seed: {args.seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c69761",
   "metadata": {},
   "source": [
    "# Start Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ce3cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Unity settings from config...\n",
      "Curriculum step 1/5:\n",
      "\tsetting initial_fill_percentage to 0.20400000000000001\n",
      "Starting Unity Environment from build: ./unity_build/6xnew_reward_wind/UASRL.exe\n",
      "Unity Environment connected.\n"
     ]
    }
   ],
   "source": [
    "# Create the channel\n",
    "env_info = CustomChannel()\n",
    "param_channel = EnvironmentParametersChannel()\n",
    "\n",
    "print('Applying Unity settings from config...')\n",
    "apply_unity_settings(param_channel, agent_config, 'ag_')\n",
    "\n",
    "curriculum_step = 1\n",
    "curriculum_last_update = 0\n",
    "modify_config_for_curriculum(curriculum_step, args.curriculum_steps, obstacles_config)\n",
    "apply_unity_settings(param_channel, obstacles_config, 'obs_')\n",
    "\n",
    "if args.test_lib:\n",
    "    print('Testing Ended')\n",
    "    exit(0)\n",
    "\n",
    "# env setup\n",
    "print(f'Starting Unity Environment from build: {args.build_path}')\n",
    "# args.build_path\n",
    "env = UnityEnvironment(args.build_path, \n",
    "                       seed=args.seed, \n",
    "                       side_channels=[env_info, param_channel], \n",
    "                       no_graphics=args.headless,\n",
    "                       worker_id=args.worker_id)\n",
    "print('Unity Environment connected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "398130bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting environment...\n"
     ]
    }
   ],
   "source": [
    "print('Resetting environment...')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b073c",
   "metadata": {},
   "source": [
    "# Environment Variables and Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71f2cc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: forse_va_3031923\n",
      "Setting up wandb experiment tracking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgiacomoaru\u001b[0m (\u001b[33mgiacomo-aru\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\cicci\\Desktop\\UASRL\\wandb\\run-20260115_155554-z63fnt91</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giacomo-aru/UASRL/runs/z63fnt91' target=\"_blank\">forse_va_3031923</a></strong> to <a href='https://wandb.ai/giacomo-aru/UASRL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giacomo-aru/UASRL' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giacomo-aru/UASRL/runs/z63fnt91' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL/runs/z63fnt91</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = f\"{args.exp_name}_{int(time.time()) - args.base_time}\"\n",
    "args.run_name = run_name\n",
    "print(f\"Run name: {run_name}\")\n",
    "\n",
    "# wandb to track experiments\n",
    "# Start a new wandb run to track this script.\n",
    "if args.wandb:\n",
    "    print('Setting up wandb experiment tracking.')\n",
    "    wandb_run = wandb.init(\n",
    "        # Set the wandb entity where your project will be logged (generally your team name).\n",
    "        entity=\"giacomo-aru\",\n",
    "        # Set the wandb project where this run will be logged.\n",
    "        project=\"UASRL\",\n",
    "        # force the \n",
    "        name=args.run_name,\n",
    "        # Track hyperparameters and run metadata.\n",
    "        config={\n",
    "            \"training\": vars(args),\n",
    "            \"agent\": agent_config,\n",
    "            \"obstacles\": obstacles_config,\n",
    "            \"other\": other_config\n",
    "        }\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b43995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEHAVIOUR_NAME = other_config['behavior_name'] + '?team=' + other_config['team']\n",
    "\n",
    "RAY_PER_DIRECTION = other_config['rays_per_direction']\n",
    "RAYCAST_MIN = other_config['rays_min_observation']\n",
    "RAYCAST_MAX = other_config['rays_max_observation']\n",
    "RAYCAST_SIZE = 2*RAY_PER_DIRECTION + 1\n",
    "\n",
    "STATE_SIZE = other_config['state_observation_size'] - 1\n",
    "STATE_MIN = other_config['state_min_observation']\n",
    "STATE_MAX = other_config['state_max_observation']\n",
    "\n",
    "ACTION_SIZE = other_config['action_size']\n",
    "ACTION_MIN = other_config['min_action']\n",
    "ACTION_MAX = other_config['max_action']\n",
    "\n",
    "TOTAL_STATE_SIZE = (STATE_SIZE + RAYCAST_SIZE)*args.input_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4736a2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating actor and critic networks...\n"
     ]
    }
   ],
   "source": [
    "# creating the training networks\n",
    "print('Creating actor and critic networks...')\n",
    "actor = DenseActor(TOTAL_STATE_SIZE, ACTION_SIZE, ACTION_MIN, ACTION_MAX, args.actor_network_layers).to(DEVICE)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)\n",
    "\n",
    "qf_ensemble = [DenseSoftQNetwork(TOTAL_STATE_SIZE, ACTION_SIZE, args.q_network_layers).to(DEVICE) for _ in range(args.q_ensemble_n)]\n",
    "qf_ensemble_target = [DenseSoftQNetwork(TOTAL_STATE_SIZE, ACTION_SIZE, args.q_network_layers).to(DEVICE) for _ in range(args.q_ensemble_n)]\n",
    "for q_t, q in zip(qf_ensemble_target, qf_ensemble):\n",
    "    q_t.load_state_dict(q.state_dict())\n",
    "\n",
    "par = []\n",
    "for q in qf_ensemble:\n",
    "    par += list(q.parameters())\n",
    "qf_optimizer = torch.optim.Adam(\n",
    "    par,\n",
    "    lr=args.q_lr\n",
    ")\n",
    "\n",
    "obs_stack = DenseStackedObservations(args.input_stack, \n",
    "                                     STATE_SIZE + RAYCAST_SIZE, \n",
    "                                     args.n_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754acdf",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31990774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up replay buffer...\n"
     ]
    }
   ],
   "source": [
    "print('Setting up replay buffer...')\n",
    "observation_space = spaces.Box(\n",
    "    low=min(RAYCAST_MIN, STATE_MIN), \n",
    "    high=max(RAYCAST_MAX, STATE_MAX), \n",
    "    shape=(TOTAL_STATE_SIZE,), \n",
    "    dtype=np.float32\n",
    ")\n",
    "action_space = spaces.Box(\n",
    "    low=ACTION_MIN, \n",
    "    high=ACTION_MAX, \n",
    "    shape=(ACTION_SIZE,), \n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    buffer_size=args.buffer_size,\n",
    "    observation_space=observation_space,\n",
    "    action_space=action_space,\n",
    "    device=DEVICE,                \n",
    "    handle_timeout_termination=True,\n",
    "    n_envs=1 # necessario data la natura asincrona del'env   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a11d9",
   "metadata": {},
   "source": [
    "# start algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62f83fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autotune target_entropy: -2.0\n"
     ]
    }
   ],
   "source": [
    "# Automatic entropy tuning\n",
    "if args.autotune:\n",
    "    target_entropy = -torch.prod(torch.Tensor(action_space.shape).to(DEVICE)).item()\n",
    "    log_alpha = torch.tensor([-1.0], requires_grad=True, device=DEVICE)\n",
    "    alpha = log_alpha.exp().item()\n",
    "    a_optimizer = optim.Adam([log_alpha], lr=args.alpha_lr)\n",
    "    print(f'autotune target_entropy: {target_entropy}')\n",
    "else:\n",
    "    alpha = args.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8bde39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to path: ./models/forse_va_3031923\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "save_path = './models/' + run_name\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "print('saving to path:', save_path)\n",
    "\n",
    "training_stats = {\n",
    "    \"time/python_time\": RunningMean(),\n",
    "    \"time/unity_time\": RunningMean(),\n",
    "    \n",
    "    \"stats/action_saturation\": RunningMean(),\n",
    "    'stats/qf_mean': RunningMean(),\n",
    "    'stats/qf_std':RunningMean(),\n",
    "    'stats/actor_entropy': RunningMean(),\n",
    "    'stats/alpha': RunningMean(),\n",
    "    'stats/uncertainty': RunningMean(),\n",
    "    \n",
    "    'loss/critic_ens': RunningMean(),\n",
    "    'loss/actor': RunningMean(),\n",
    "    'loss/alpha': RunningMean(),\n",
    "}\n",
    "\n",
    "best_reward = -float('inf')\n",
    "\n",
    "episodic_stats = {}\n",
    "success_stats = {}\n",
    "failure_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9d6ed7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/2000000] Starting Training - run name: forse_va_3031923\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "unity_end_time = -1\n",
    "unity_start_time = -1\n",
    "\n",
    "env_step = 0\n",
    "gradient_step = 0\n",
    "print(f'[{env_step}/{args.total_timesteps}] Starting Training - run name: {run_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 67] aggiunta saltata? id:1.0\n",
      "[Step 67] aggiunta saltata? id:0.0\n",
      "[Step 67] aggiunta saltata? id:4.0\n",
      "[Step 67] aggiunta saltata? id:3.0\n",
      "[Step 67] aggiunta saltata? id:5.0\n",
      "[Step 268] aggiunta saltata? id:1.0\n",
      "[Step 268] aggiunta saltata? id:0.0\n",
      "[Step 268] aggiunta saltata? id:4.0\n",
      "[Step 268] aggiunta saltata? id:3.0\n",
      "[Step 268] aggiunta saltata? id:5.0\n",
      "[Step 469] aggiunta saltata? id:1.0\n",
      "[Step 469] aggiunta saltata? id:0.0\n",
      "[Step 469] aggiunta saltata? id:4.0\n",
      "[Step 469] aggiunta saltata? id:3.0\n",
      "[Step 469] aggiunta saltata? id:5.0\n",
      "[Step 670] aggiunta saltata? id:1.0\n",
      "[Step 670] aggiunta saltata? id:0.0\n",
      "[Step 670] aggiunta saltata? id:4.0\n",
      "[Step 670] aggiunta saltata? id:3.0\n",
      "[Step 670] aggiunta saltata? id:5.0\n",
      "[Step 826] aggiunta saltata? id:1.0\n",
      "[Step 826] aggiunta saltata? id:0.0\n",
      "[Step 826] aggiunta saltata? id:4.0\n",
      "[Step 826] aggiunta saltata? id:3.0\n",
      "[Step 826] aggiunta saltata? id:5.0\n",
      "[1001/2000000] Computing Z-score normalization parameters...\n",
      "Normalization updated: Actor and Q-Nets are now synchronized.\n",
      "[1005/2000000] |success: 0.00000|reward: 1.81699|collisions: 3.63419|length: 999.00000|SPL: 0.00000| SPS: 9\n",
      "[1005/2000000] Logged episodic stats to wandb\n",
      "[1005/2000000] Models saved, suffix: _c1_best\n",
      "[Step 1027] aggiunta saltata? id:1.0\n",
      "[Step 1027] aggiunta saltata? id:0.0\n",
      "[Step 1027] aggiunta saltata? id:4.0\n",
      "[Step 1027] aggiunta saltata? id:3.0\n",
      "[Step 1027] aggiunta saltata? id:5.0\n",
      "[1028/2000000] Models saved, suffix: _c1_best\n",
      "[1206/2000000] |success: 0.00000|reward: 3.32112|collisions: 3.29696|length: 999.00000|SPL: 0.00000| SPS: 10\n",
      "[1206/2000000] Logged episodic stats to wandb\n",
      "[1206/2000000] Models saved, suffix: _c1_best\n",
      "[Step 1228] aggiunta saltata? id:1.0\n",
      "[Step 1228] aggiunta saltata? id:0.0\n",
      "[Step 1228] aggiunta saltata? id:4.0\n",
      "[Step 1228] aggiunta saltata? id:3.0\n",
      "[Step 1228] aggiunta saltata? id:5.0\n",
      "[1229/2000000] Models saved, suffix: _c1_best\n",
      "[1250/2000000] Logged training stats to wandb\n",
      "[1407/2000000] |success: 0.00000|reward: 3.22936|collisions: 3.01911|length: 999.00000|SPL: 0.00000| SPS: 10\n",
      "[1407/2000000] Logged episodic stats to wandb\n",
      "[Step 1429] aggiunta saltata? id:1.0\n",
      "[Step 1429] aggiunta saltata? id:0.0\n",
      "[Step 1429] aggiunta saltata? id:4.0\n",
      "[Step 1429] aggiunta saltata? id:3.0\n",
      "[Step 1429] aggiunta saltata? id:5.0\n",
      "[1500/2000000] Logged training stats to wandb\n",
      "[1608/2000000] |success: 0.00000|reward: 2.27697|collisions: 2.64503|length: 999.00000|SPL: 0.00000| SPS: 10\n",
      "[1608/2000000] Logged episodic stats to wandb\n",
      "[Step 1630] aggiunta saltata? id:1.0\n",
      "[Step 1630] aggiunta saltata? id:0.0\n",
      "[Step 1630] aggiunta saltata? id:4.0\n",
      "[Step 1630] aggiunta saltata? id:3.0\n",
      "[Step 1630] aggiunta saltata? id:5.0\n",
      "[1750/2000000] Logged training stats to wandb\n",
      "[1809/2000000] |success: 0.00000|reward: 2.21033|collisions: 2.51998|length: 999.00000|SPL: 0.00000| SPS: 10\n",
      "[1809/2000000] Logged episodic stats to wandb\n",
      "[Step 1831] aggiunta saltata? id:1.0\n",
      "[Step 1831] aggiunta saltata? id:0.0\n",
      "[Step 1831] aggiunta saltata? id:4.0\n",
      "[Step 1831] aggiunta saltata? id:3.0\n",
      "[Step 1831] aggiunta saltata? id:5.0\n",
      "[1832/2000000] |success: 0.00000|reward: 2.06981|collisions: 2.38711|length: 999.00000|SPL: 0.00000| SPS: 10\n",
      "[1832/2000000] Logged episodic stats to wandb\n",
      "[2000/2000000] Logged training stats to wandb\n",
      "[2010/2000000] |success: 0.00000|reward: 3.07331|collisions: 2.35097|length: 999.00000|SPL: 0.00000| SPS: 10\n",
      "[2010/2000000] Logged episodic stats to wandb\n",
      "[2010/2000000] Models saved, suffix: _c1_best\n",
      "[Step 2032] aggiunta saltata? id:1.0\n",
      "[Step 2032] aggiunta saltata? id:0.0\n",
      "[Step 2032] aggiunta saltata? id:4.0\n",
      "[Step 2032] aggiunta saltata? id:3.0\n",
      "[Step 2032] aggiunta saltata? id:5.0\n",
      "[2211/2000000] |success: 0.00000|reward: 2.62431|collisions: 2.32482|length: 999.00000|SPL: 0.00000| SPS: 10\n",
      "[2211/2000000] Logged episodic stats to wandb\n",
      "[Step 2233] aggiunta saltata? id:1.0\n",
      "[Step 2233] aggiunta saltata? id:0.0\n",
      "[Step 2233] aggiunta saltata? id:4.0\n",
      "[Step 2233] aggiunta saltata? id:3.0\n",
      "[Step 2233] aggiunta saltata? id:5.0\n",
      "[2250/2000000] Logged training stats to wandb\n",
      "[Step 2285] aggiunta saltata? id:1.0\n",
      "[Step 2285] aggiunta saltata? id:4.0\n",
      "[Step 2285] aggiunta saltata? id:2.0\n",
      "[Step 2285] aggiunta saltata? id:3.0\n",
      "[Step 2285] aggiunta saltata? id:5.0\n",
      "[Step 2402] aggiunta saltata? id:1.0\n",
      "[Step 2402] aggiunta saltata? id:4.0\n",
      "[Step 2402] aggiunta saltata? id:2.0\n",
      "[Step 2402] aggiunta saltata? id:3.0\n",
      "[Step 2402] aggiunta saltata? id:5.0\n",
      "[2414/2000000] |success: 0.03686|reward: 1.91414|collisions: 2.15893|length: 959.57549|SPL: 0.02892| SPS: 10\n",
      "[2414/2000000] Logged episodic stats to wandb\n",
      "[Step 2436] aggiunta saltata? id:1.0\n",
      "[Step 2436] aggiunta saltata? id:0.0\n",
      "[Step 2436] aggiunta saltata? id:4.0\n",
      "[Step 2436] aggiunta saltata? id:3.0\n",
      "[Step 2436] aggiunta saltata? id:5.0\n",
      "[2500/2000000] Logged training stats to wandb\n",
      "[2604/2000000] |success: 0.03006|reward: 2.27083|collisions: 2.02041|length: 966.85433|SPL: 0.02358| SPS: 10\n",
      "[2604/2000000] Logged episodic stats to wandb\n",
      "[Step 2604] aggiunta saltata? id:1.0\n",
      "[Step 2604] aggiunta saltata? id:4.0\n",
      "[Step 2604] aggiunta saltata? id:2.0\n",
      "[Step 2604] aggiunta saltata? id:3.0\n",
      "[Step 2604] aggiunta saltata? id:5.0\n",
      "[Step 2638] aggiunta saltata? id:1.0\n",
      "[Step 2638] aggiunta saltata? id:0.0\n",
      "[Step 2638] aggiunta saltata? id:4.0\n",
      "[Step 2638] aggiunta saltata? id:3.0\n",
      "[Step 2638] aggiunta saltata? id:5.0\n",
      "[2639/2000000] |success: 0.02451|reward: 1.94332|collisions: 1.90728|length: 972.78930|SPL: 0.01922| SPS: 10\n",
      "[2639/2000000] Logged episodic stats to wandb\n",
      "[2750/2000000] Logged training stats to wandb\n",
      "[Step 2806] aggiunta saltata? id:1.0\n",
      "[Step 2806] aggiunta saltata? id:4.0\n",
      "[Step 2806] aggiunta saltata? id:2.0\n",
      "[Step 2806] aggiunta saltata? id:3.0\n",
      "[Step 2806] aggiunta saltata? id:5.0\n",
      "[2818/2000000] |success: 0.01998|reward: 2.75526|collisions: 1.81214|length: 977.62851|SPL: 0.01567| SPS: 10\n",
      "[2818/2000000] Logged episodic stats to wandb\n",
      "[Step 2840] aggiunta saltata? id:1.0\n",
      "[Step 2840] aggiunta saltata? id:0.0\n",
      "[Step 2840] aggiunta saltata? id:4.0\n",
      "[Step 2840] aggiunta saltata? id:3.0\n",
      "[Step 2840] aggiunta saltata? id:5.0\n",
      "[3000/2000000] Logged training stats to wandb\n",
      "[Step 3008] aggiunta saltata? id:1.0\n",
      "[Step 3008] aggiunta saltata? id:4.0\n",
      "[Step 3008] aggiunta saltata? id:2.0\n",
      "[Step 3008] aggiunta saltata? id:3.0\n",
      "[Step 3008] aggiunta saltata? id:5.0\n",
      "[3020/2000000] |success: 0.01629|reward: 3.49004|collisions: 1.76535|length: 981.57427|SPL: 0.01278| SPS: 10\n",
      "[3020/2000000] Logged episodic stats to wandb\n",
      "[3020/2000000] Models saved, suffix: _c1_best\n",
      "[Step 3042] aggiunta saltata? id:1.0\n",
      "[Step 3042] aggiunta saltata? id:0.0\n",
      "[Step 3042] aggiunta saltata? id:4.0\n",
      "[Step 3042] aggiunta saltata? id:3.0\n",
      "[Step 3042] aggiunta saltata? id:5.0\n",
      "[3043/2000000] Models saved, suffix: _c1_best\n",
      "[3210/2000000] Models saved, suffix: _c1_best\n",
      "[Step 3210] aggiunta saltata? id:1.0\n",
      "[Step 3210] aggiunta saltata? id:4.0\n",
      "[Step 3210] aggiunta saltata? id:2.0\n",
      "[Step 3210] aggiunta saltata? id:3.0\n",
      "[Step 3210] aggiunta saltata? id:5.0\n",
      "[3222/2000000] |success: 0.01329|reward: 3.56710|collisions: 1.63019|length: 984.79154|SPL: 0.01042| SPS: 10\n",
      "[3222/2000000] Logged episodic stats to wandb\n",
      "[Step 3244] aggiunta saltata? id:1.0\n",
      "[Step 3244] aggiunta saltata? id:0.0\n",
      "[Step 3244] aggiunta saltata? id:4.0\n",
      "[Step 3244] aggiunta saltata? id:3.0\n",
      "[Step 3244] aggiunta saltata? id:5.0\n",
      "[3250/2000000] Logged training stats to wandb\n",
      "[3412/2000000] |success: 0.04923|reward: 3.54107|collisions: 1.58456|length: 953.66121|SPL: 0.04098| SPS: 10\n",
      "[3412/2000000] Logged episodic stats to wandb\n",
      "[Step 3412] aggiunta saltata? id:1.0\n",
      "[Step 3412] aggiunta saltata? id:4.0\n",
      "[Step 3412] aggiunta saltata? id:2.0\n",
      "[Step 3412] aggiunta saltata? id:3.0\n",
      "[Step 3412] aggiunta saltata? id:5.0\n",
      "[Step 3446] aggiunta saltata? id:1.0\n",
      "[Step 3446] aggiunta saltata? id:0.0\n",
      "[Step 3446] aggiunta saltata? id:4.0\n",
      "[Step 3446] aggiunta saltata? id:3.0\n",
      "[Step 3446] aggiunta saltata? id:5.0\n",
      "[3449/2000000] |success: 0.04014|reward: 3.38143|collisions: 1.62428|length: 962.03199|SPL: 0.03341| SPS: 10\n",
      "[3449/2000000] Logged episodic stats to wandb\n",
      "[3500/2000000] Logged training stats to wandb\n",
      "[Step 3614] aggiunta saltata? id:1.0\n",
      "[Step 3614] aggiunta saltata? id:4.0\n",
      "[Step 3614] aggiunta saltata? id:2.0\n",
      "[Step 3614] aggiunta saltata? id:3.0\n",
      "[Step 3614] aggiunta saltata? id:5.0\n",
      "[3626/2000000] Models saved, suffix: _c1_best\n",
      "[Step 3648] aggiunta saltata? id:1.0\n",
      "[Step 3648] aggiunta saltata? id:0.0\n",
      "[Step 3648] aggiunta saltata? id:4.0\n",
      "[Step 3648] aggiunta saltata? id:3.0\n",
      "[Step 3648] aggiunta saltata? id:5.0\n",
      "[3649/2000000] |success: 0.03273|reward: 4.13124|collisions: 1.50607|length: 968.85729|SPL: 0.02724| SPS: 10\n",
      "[3649/2000000] Logged episodic stats to wandb\n",
      "[3649/2000000] Models saved, suffix: _c1_best\n",
      "[3651/2000000] Models saved, suffix: _c1_best\n",
      "[3750/2000000] Logged training stats to wandb\n",
      "[Step 3816] aggiunta saltata? id:1.0\n",
      "[Step 3816] aggiunta saltata? id:4.0\n",
      "[Step 3816] aggiunta saltata? id:2.0\n",
      "[Step 3816] aggiunta saltata? id:3.0\n",
      "[Step 3816] aggiunta saltata? id:5.0\n",
      "[3828/2000000] |success: 0.02669|reward: 4.91746|collisions: 1.34198|length: 974.42246|SPL: 0.02221| SPS: 10\n",
      "[3828/2000000] Logged episodic stats to wandb\n",
      "[3828/2000000] Models saved, suffix: _c1_best\n",
      "[Step 3850] aggiunta saltata? id:1.0\n",
      "[Step 3850] aggiunta saltata? id:0.0\n",
      "[Step 3850] aggiunta saltata? id:4.0\n",
      "[Step 3850] aggiunta saltata? id:3.0\n",
      "[Step 3850] aggiunta saltata? id:5.0\n",
      "[3851/2000000] Models saved, suffix: _c1_best\n",
      "[4000/2000000] Logged training stats to wandb\n",
      "[Step 4018] aggiunta saltata? id:1.0\n",
      "[Step 4018] aggiunta saltata? id:4.0\n",
      "[Step 4018] aggiunta saltata? id:2.0\n",
      "[Step 4018] aggiunta saltata? id:3.0\n",
      "[Step 4018] aggiunta saltata? id:5.0\n",
      "[4030/2000000] |success: 0.02176|reward: 4.51287|collisions: 1.13108|length: 978.96014|SPL: 0.01811| SPS: 10\n",
      "[4030/2000000] Logged episodic stats to wandb\n",
      "[4030/2000000] Models saved, suffix: _c1_best\n",
      "[Step 4040] aggiunta saltata? id:1.0\n",
      "[Step 4040] aggiunta saltata? id:0.0\n",
      "[Step 4040] aggiunta saltata? id:4.0\n",
      "[Step 4040] aggiunta saltata? id:3.0\n",
      "[Step 4040] aggiunta saltata? id:5.0\n",
      "[4041/2000000] Models saved, suffix: _c1_best\n",
      "[Step 4199] aggiunta saltata? id:1.0\n",
      "[Step 4199] aggiunta saltata? id:0.0\n",
      "[Step 4199] aggiunta saltata? id:4.0\n",
      "[Step 4199] aggiunta saltata? id:2.0\n",
      "[Step 4199] aggiunta saltata? id:5.0\n",
      "[4200/2000000] |success: 0.09313|reward: 6.32637|collisions: 0.92225|length: 958.24708|SPL: 0.04941| SPS: 10\n",
      "[4200/2000000] Logged episodic stats to wandb\n",
      "[4213/2000000] Models saved, suffix: _c1_best\n",
      "[Step 4221] aggiunta saltata? id:1.0\n",
      "[Step 4221] aggiunta saltata? id:4.0\n",
      "[Step 4221] aggiunta saltata? id:2.0\n",
      "[Step 4221] aggiunta saltata? id:3.0\n",
      "[Step 4221] aggiunta saltata? id:5.0\n",
      "[4233/2000000] Models saved, suffix: _c1_best\n",
      "[Step 4243] aggiunta saltata? id:1.0\n",
      "[Step 4243] aggiunta saltata? id:0.0\n",
      "[Step 4243] aggiunta saltata? id:4.0\n",
      "[Step 4243] aggiunta saltata? id:3.0\n",
      "[Step 4243] aggiunta saltata? id:5.0\n",
      "[4244/2000000] |success: 0.10991|reward: 6.88750|collisions: 0.86884|length: 933.90370|SPL: 0.07181| SPS: 10\n",
      "[4244/2000000] Logged episodic stats to wandb\n",
      "[4250/2000000] Logged training stats to wandb\n",
      "[Step 4308] aggiunta saltata? id:1.0\n",
      "[Step 4308] aggiunta saltata? id:0.0\n",
      "[Step 4308] aggiunta saltata? id:4.0\n",
      "[Step 4308] aggiunta saltata? id:2.0\n",
      "[Step 4308] aggiunta saltata? id:3.0\n",
      "[4309/2000000] Models saved, suffix: _c1_best\n",
      "[Step 4407] aggiunta saltata? id:1.0\n",
      "[Step 4407] aggiunta saltata? id:0.0\n",
      "[Step 4407] aggiunta saltata? id:4.0\n",
      "[Step 4407] aggiunta saltata? id:2.0\n",
      "[Step 4407] aggiunta saltata? id:3.0\n",
      "[Step 4425] aggiunta saltata? id:1.0\n",
      "[Step 4425] aggiunta saltata? id:4.0\n",
      "[Step 4425] aggiunta saltata? id:2.0\n",
      "[Step 4425] aggiunta saltata? id:3.0\n",
      "[Step 4425] aggiunta saltata? id:5.0\n",
      "[4437/2000000] |success: 0.12359|reward: 7.03770|collisions: 0.89889|length: 916.83497|SPL: 0.07782| SPS: 10\n",
      "[4437/2000000] Logged episodic stats to wandb\n",
      "[Step 4447] aggiunta saltata? id:1.0\n",
      "[Step 4447] aggiunta saltata? id:0.0\n",
      "[Step 4447] aggiunta saltata? id:4.0\n",
      "[Step 4447] aggiunta saltata? id:3.0\n",
      "[Step 4447] aggiunta saltata? id:5.0\n",
      "[4500/2000000] Logged training stats to wandb\n",
      "[4510/2000000] Models saved, suffix: _c1_best\n",
      "[Step 4547] aggiunta saltata? id:1.0\n",
      "[Step 4547] aggiunta saltata? id:0.0\n",
      "[Step 4547] aggiunta saltata? id:2.0\n",
      "[Step 4547] aggiunta saltata? id:3.0\n",
      "[Step 4547] aggiunta saltata? id:5.0\n",
      "[Step 4611] aggiunta saltata? id:1.0\n",
      "[Step 4611] aggiunta saltata? id:0.0\n",
      "[Step 4611] aggiunta saltata? id:4.0\n",
      "[Step 4611] aggiunta saltata? id:2.0\n",
      "[Step 4611] aggiunta saltata? id:3.0\n",
      "[4612/2000000] |success: 0.13764|reward: 7.66395|collisions: 0.76832|length: 894.63246|SPL: 0.09614| SPS: 10\n",
      "[4612/2000000] Logged episodic stats to wandb\n",
      "[Step 4629] aggiunta saltata? id:1.0\n",
      "[Step 4629] aggiunta saltata? id:4.0\n",
      "[Step 4629] aggiunta saltata? id:2.0\n",
      "[Step 4629] aggiunta saltata? id:3.0\n",
      "[Step 4629] aggiunta saltata? id:5.0\n",
      "[Step 4651] aggiunta saltata? id:1.0\n",
      "[Step 4651] aggiunta saltata? id:0.0\n",
      "[Step 4651] aggiunta saltata? id:4.0\n",
      "[Step 4651] aggiunta saltata? id:3.0\n",
      "[Step 4651] aggiunta saltata? id:5.0\n",
      "[4750/2000000] Logged training stats to wandb\n",
      "[Step 4751] aggiunta saltata? id:1.0\n",
      "[Step 4751] aggiunta saltata? id:0.0\n",
      "[Step 4751] aggiunta saltata? id:2.0\n",
      "[Step 4751] aggiunta saltata? id:3.0\n",
      "[Step 4751] aggiunta saltata? id:5.0\n",
      "[4752/2000000] |success: 0.11223|reward: 7.00648|collisions: 0.66333|length: 913.90156|SPL: 0.07839| SPS: 10\n",
      "[4752/2000000] Logged episodic stats to wandb\n",
      "[Step 4815] aggiunta saltata? id:1.0\n",
      "[Step 4815] aggiunta saltata? id:0.0\n",
      "[Step 4815] aggiunta saltata? id:4.0\n",
      "[Step 4815] aggiunta saltata? id:2.0\n",
      "[Step 4815] aggiunta saltata? id:3.0\n",
      "[Step 4833] aggiunta saltata? id:1.0\n",
      "[Step 4833] aggiunta saltata? id:4.0\n",
      "[Step 4833] aggiunta saltata? id:2.0\n",
      "[Step 4833] aggiunta saltata? id:3.0\n",
      "[Step 4833] aggiunta saltata? id:5.0\n",
      "[4844/2000000] An error occurred: Communicator has exited.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cicci\\AppData\\Local\\Temp\\ipykernel_3096\\1703633758.py\", line 35, in <module>\n",
      "    env.step()\n",
      "  File \"c:\\Users\\cicci\\Desktop\\UASRL\\.venv\\lib\\site-packages\\mlagents_envs\\timers.py\", line 305, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\cicci\\Desktop\\UASRL\\.venv\\lib\\site-packages\\mlagents_envs\\environment.py\", line 350, in step\n",
      "    raise UnityCommunicatorStoppedException(\"Communicator has exited.\")\n",
      "mlagents_envs.exception.UnityCommunicatorStoppedException: Communicator has exited.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    decision_obs, terminal_obs = observe_batch(env, BEHAVIOUR_NAME, (STATE_SIZE + RAYCAST_SIZE)) \n",
    "    obs_stack.add_observation(terminal_obs[1], terminal_obs[0])\n",
    "    terminal_obs[1] = obs_stack.get_stacked_observations(terminal_obs[0])\n",
    "    obs_stack.reset(terminal_obs[0]) # reset for the NEXT step\n",
    "    \n",
    "    obs_stack.add_observation(decision_obs[1], decision_obs[0])\n",
    "    decision_obs[1] = obs_stack.get_stacked_observations(decision_obs[0])\n",
    "    \n",
    "    # observe_batch_stacked(env, BEHAVIOUR_NAME, args.input_stack, TOTAL_STATE_SIZE)\n",
    "    while env_step < args.total_timesteps:\n",
    "        # --- ACTION SELECTION ---\n",
    "        if env_step < args.learning_starts: # warm-up with random actions\n",
    "            action = get_initial_action_batch(decision_obs[0])\n",
    "\n",
    "        else:\n",
    "            obs_tensor = torch.as_tensor(obs_stack.get_stacked_observations(decision_obs[0]), dtype=torch.float32).to(DEVICE)\n",
    "            \n",
    "            actor.eval()\n",
    "            with torch.no_grad():\n",
    "                action, _, _ = actor.get_action(obs_tensor)\n",
    "            action = action.detach().cpu().numpy()\n",
    "        \n",
    "        # Action Taken\n",
    "        # decision_obs.append(action) \n",
    "        if len(action) > 0: \n",
    "            a = ActionTuple(continuous=action)\n",
    "            env.set_actions(BEHAVIOUR_NAME, a)\n",
    "        \n",
    "        # --- ENVIRONMENT STEP ---\n",
    "        unity_start_time = time.time()\n",
    "        if unity_end_time > 0 and env_step > args.learning_starts:\n",
    "            training_stats['time/python_time'].update(unity_start_time - unity_end_time)\n",
    "        \n",
    "        env.step()\n",
    "        unity_end_time = time.time()\n",
    "        if env_step > args.learning_starts:\n",
    "            training_stats['time/unity_time'].update(unity_end_time - unity_start_time)\n",
    "        \n",
    "        next_decision_obs, next_terminal_obs = observe_batch(env, BEHAVIOUR_NAME, (STATE_SIZE + RAYCAST_SIZE))\n",
    "        obs_stack.add_observation(next_terminal_obs[1], next_terminal_obs[0])\n",
    "        \n",
    "        dummy_var = obs_stack.get_stacked_observations(next_terminal_obs[0])\n",
    "        if not np.allclose(dummy_var[:,-(STATE_SIZE + RAYCAST_SIZE):], next_terminal_obs[1], atol=1e-8):\n",
    "            print(f'Warning: State part of observation changed at step {env_step}. Possible error in obs stacking.')\n",
    "            print(f'dummy_var state part: {dummy_var}')\n",
    "            print(f'next_terminal_obs state part: {next_terminal_obs[1]}')\n",
    "            raise ValueError(\"State part of observation changed between steps.\")\n",
    "        \n",
    "        next_terminal_obs[1] = obs_stack.get_stacked_observations(next_terminal_obs[0])\n",
    "        obs_stack.reset(next_terminal_obs[0]) # reset for the NEXT step\n",
    "        \n",
    "        obs_stack.add_observation(next_decision_obs[1], next_decision_obs[0])\n",
    "        \n",
    "        dummy_var = obs_stack.get_stacked_observations(next_decision_obs[0])\n",
    "        if not np.allclose(dummy_var[:,-(STATE_SIZE + RAYCAST_SIZE):], next_decision_obs[1], atol=1e-8):\n",
    "            print(f'Warning: State part of observation at step {env_step}. Possible error in obs stacking.')\n",
    "            print(f'dummy_var state part: {dummy_var}')\n",
    "            print(f'next_decision_obs state part: {next_decision_obs[1]}')\n",
    "            raise ValueError(\"State part of observation changed between steps.\")\n",
    "        \n",
    "        next_decision_obs[1] = obs_stack.get_stacked_observations(next_decision_obs[0])\n",
    "        \n",
    "\n",
    "        # --- BUFFER DATA ---\n",
    "        # for each agent in decision_obs, try to store its transition if it is present in next_decision_obs or next_terminal_obs\n",
    "        for i, id in enumerate(decision_obs[0]):\n",
    "            if id in next_terminal_obs[0]:\n",
    "                new_idx = np.where(next_terminal_obs[0] == id)[0][0]\n",
    "                \n",
    "                reward = next_terminal_obs[2][new_idx]\n",
    "                done = 1 # next_terminal_obs[3][new_idx]\n",
    "                next_obs = next_terminal_obs[1][new_idx]\n",
    "            elif id in next_decision_obs[0]:\n",
    "                new_idx = np.where(next_decision_obs[0] == id)[0][0]\n",
    "                \n",
    "                reward = next_decision_obs[2][new_idx]\n",
    "                done = 0 # next_decision_obs[3][new_idx]\n",
    "                next_obs = next_decision_obs[1][new_idx]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            pre_obs = decision_obs[1][i]\n",
    "            act = action[i]\n",
    "            \n",
    "            if not np.allclose(pre_obs[(STATE_SIZE + RAYCAST_SIZE):], next_obs[:-(STATE_SIZE + RAYCAST_SIZE)], atol=1e-8):\n",
    "                print(f'Warning: State part of observation changed for agent {id} at step {env_step}. Possible error in obs stacking.')\n",
    "                print(f'pre_obs state part: {pre_obs}')\n",
    "                print(f'next_obs state part: {next_obs}')\n",
    "                raise ValueError(\"State part of observation changed between steps.\")\n",
    "            \n",
    "            rb.add(\n",
    "                pre_obs, \n",
    "                next_obs, \n",
    "                act, \n",
    "                reward * args.reward_scale,\n",
    "                done, \n",
    "                [{}]\n",
    "            )\n",
    "            \n",
    "\n",
    "        # update current obs\n",
    "        decision_obs = next_decision_obs\n",
    "        terminal_obs = next_terminal_obs\n",
    "        env_step += 1\n",
    "        \n",
    "        # --- STATS UPDATE MIGLIORATO ---\n",
    "        while env_info.stop_msg_queue:\n",
    "            msg = env_info.stop_msg_queue.pop()\n",
    "            \n",
    "            if env_step >= args.learning_starts:\n",
    "                update_stats_from_message(episodic_stats, success_stats, failure_stats, msg, args.metrics_smoothing)        \n",
    "                if episodic_stats['ep_count'] % args.metrics_log_interval == 0:\n",
    "                    print_update(env_step, args.total_timesteps, start_time, episodic_stats)\n",
    "                    if args.wandb:\n",
    "                        log_stats_to_wandb(wandb_run, \n",
    "                                        [episodic_stats, success_stats, failure_stats],\n",
    "                                        ['all_ep', 'success_ep', 'failure_ep'],\n",
    "                                        env_step)\n",
    "                        print(f\"[{env_step}/{args.total_timesteps}] Logged episodic stats to wandb\")\n",
    "\n",
    "                enough_episodes_passed = curriculum_last_update  > args.min_episodes_per_curriculum\n",
    "                is_performance_good = episodic_stats['success'] > args.min_success_rate\n",
    "                not_last_step = curriculum_step < args.curriculum_steps\n",
    "\n",
    "                if enough_episodes_passed and is_performance_good and not_last_step:    \n",
    "                    save_models(actor, qf_ensemble, qf_ensemble_target, save_path, suffix=f'_c{curriculum_step}_final')\n",
    "                    print(f\"[{env_step}/{args.total_timesteps}] Models saved, suffix: _c{curriculum_step}_final\")\n",
    "                    \n",
    "                    curriculum_step += 1\n",
    "                    curriculum_last_update = episodic_stats['ep_count']\n",
    "                    best_reward = -float('inf') # to save new best model for new curriculum step\n",
    "                    modify_config_for_curriculum(curriculum_step, args.curriculum_steps, obstacles_config)\n",
    "                    apply_unity_settings(param_channel, obstacles_config, 'obs_')\n",
    "\n",
    "        # Save best models based on reward\n",
    "        if episodic_stats != {} and episodic_stats[\"reward\"] > best_reward:\n",
    "            best_reward = episodic_stats[\"reward\"]\n",
    "            save_models(actor, qf_ensemble, qf_ensemble_target, save_path, suffix=f'_c{curriculum_step}_best')\n",
    "            print(f\"[{env_step}/{args.total_timesteps}] Models saved, suffix: _c{curriculum_step}_best\")\n",
    "\n",
    "        # normalizzazione input reti\n",
    "        if env_step == args.learning_starts + 1:\n",
    "            print(f\"[{env_step}/{args.total_timesteps}] Computing Z-score normalization parameters...\")\n",
    "\n",
    "            # Recuperiamo tutte le osservazioni raccolte finora nel replay buffer\n",
    "            current_len = rb.buffer_size if rb.full else rb.pos\n",
    "            full_batch = rb.sample(current_len)\n",
    "            observations = full_batch.observations\n",
    "\n",
    "            # Calcoliamo Media e Deviazione Standard per ogni feature\n",
    "            obs_mean = torch.mean(observations, dim=0)\n",
    "            obs_std = torch.std(observations, dim=0)\n",
    "\n",
    "            # Gestione delle feature costanti (es. sensori che leggono sempre 0)\n",
    "            # Se la std è 0, la impostiamo a 1.0 per evitare la divisione per zero\n",
    "            const_mask = (obs_std == 0)\n",
    "            if const_mask.any():\n",
    "                print(f\"[{env_step}/{args.total_timesteps}] WARNING: {const_mask.sum().item()} constant features detected.\")\n",
    "                obs_std[const_mask] = 1.0\n",
    "\n",
    "            # --- AGGIORNAMENTO MODELLI ---\n",
    "            \n",
    "            # 1. Actor (usa solo osservazioni)\n",
    "            actor.set_normalization_params(obs_mean.to(DEVICE), obs_std.to(DEVICE))\n",
    "\n",
    "            # 3. Aggiorna Ensemble corrente\n",
    "            for q_net in qf_ensemble:\n",
    "                q_net.set_normalization_params(obs_mean, obs_std)\n",
    "\n",
    "            # 4. Aggiorna Ensemble Target\n",
    "            for q_target_net in qf_ensemble_target:\n",
    "                q_target_net.set_normalization_params(obs_mean, obs_std)\n",
    "            \n",
    "            print(\"Normalization updated: Actor and Q-Nets are now synchronized.\")              \n",
    "        \n",
    "        # --- DIAGNOSTIC CHECKS ---\n",
    "\n",
    "\n",
    "        # --- TRAINING LOOP ---       \n",
    "        if env_step > args.learning_starts:\n",
    "            \n",
    "            actor.train()\n",
    "            for q in qf_ensemble: q.train()\n",
    "            \n",
    "            for _ in range(args.update_frequency):\n",
    "                \n",
    "                gradient_step += 1\n",
    "                data = rb.sample(args.batch_size)\n",
    "                \n",
    "                # --- CALCOLO SATURAZIONE ---\n",
    "                saturation = data.actions.detach().cpu().numpy()\n",
    "                saturation = (np.abs(saturation) > 0.99).mean()\n",
    "                training_stats[\"stats/action_saturation\"].update(saturation)\n",
    "                \n",
    "                # --- 1. CRITIC UPDATE ---\n",
    "                with torch.no_grad():\n",
    "                    next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)\n",
    "                    \n",
    "                    qf_next_target = []\n",
    "                    for q_target in qf_ensemble_target:\n",
    "                        q_val = q_target(data.next_observations, next_state_actions)\n",
    "                        qf_next_target.append(q_val)\n",
    "                    qf_next_target = torch.stack(qf_next_target)\n",
    "                    \n",
    "                    min_qf_next_target = qf_next_target.min(dim=0).values - alpha * next_state_log_pi\n",
    "                    next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).reshape(-1)\n",
    "\n",
    "                # Calcolo Loss Critici\n",
    "                qf_values = []\n",
    "                qf_losses = []\n",
    "                for i, q in enumerate(qf_ensemble):\n",
    "                    current_qf_val = q(data.observations, data.actions).reshape(-1)\n",
    "                    qf_values.append(current_qf_val)\n",
    "                    \n",
    "                    current_loss = F.mse_loss(current_qf_val, next_q_value)\n",
    "                    qf_losses.append(current_loss)\n",
    "                qf_loss = torch.stack(qf_losses).mean()\n",
    "                \n",
    "                # optimize the model\n",
    "                qf_optimizer.zero_grad()\n",
    "                qf_loss.backward()\n",
    "                qf_optimizer.step()\n",
    "            \n",
    "                training_stats['stats/qf_mean'].update(torch.stack(qf_values).mean())\n",
    "                training_stats['stats/qf_std'].update(torch.stack(qf_values).std())\n",
    "                training_stats['loss/critic_ens'].update(qf_loss.item())\n",
    "                \n",
    "                # --- 2. ACTOR UPDATE (Delayed) ---\n",
    "                if gradient_step % args.policy_frequency == 0:\n",
    "                    for _ in range(args.policy_frequency):\n",
    "                        \n",
    "                        pi, log_pi, _ = actor.get_action(data.observations)\n",
    "\n",
    "                        q_pi_vals = [q(data.observations, pi) for q in qf_ensemble]\n",
    "                        \n",
    "                        q_pi_vals = torch.stack(q_pi_vals, dim=0)   # [n_q, batch]\n",
    "                        min_qf_pi = q_pi_vals.min(dim=0).values \n",
    "\n",
    "                        actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "                        \n",
    "                        actor_optimizer.zero_grad()\n",
    "                        actor_loss.backward()\n",
    "                        actor_optimizer.step()\n",
    "\n",
    "                        training_stats['stats/uncertainty'].update((q_pi_vals.std(dim=0)).mean())\n",
    "                        training_stats['stats/actor_entropy'].update(-log_pi.mean())\n",
    "                        training_stats['loss/actor'].update(actor_loss.item())\n",
    "                        \n",
    "                        # --- 3. ALPHA AUTO-TUNING ---\n",
    "                        if args.autotune:\n",
    "                            # data_pi = rb.sample(args.batch_size)\n",
    "                            with torch.no_grad():\n",
    "                                _, log_pi_alpha, _ = actor.get_action(data.observations)\n",
    "                            alpha_loss = (-log_alpha.exp() * (log_pi_alpha + target_entropy)).mean()\n",
    "\n",
    "                            a_optimizer.zero_grad()\n",
    "                            alpha_loss.backward()\n",
    "                            a_optimizer.step()\n",
    "                            \n",
    "                            alpha = log_alpha.exp().item()\n",
    "                            \n",
    "                            training_stats['loss/alpha'].update(alpha_loss.item())\n",
    "\n",
    "                        training_stats['stats/alpha'].update(alpha)\n",
    "\n",
    "                # --- 4. TARGET UPDATE (Soft Update) ---\n",
    "                if gradient_step % args.target_network_update_period == 0:\n",
    "                    for q, q_t in zip(qf_ensemble, qf_ensemble_target):\n",
    "                        for param, target_param in zip(q.parameters(), q_t.parameters()):\n",
    "                            target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "                # --- 5. LOGGING LOSS (METODO SNAPSHOT/ISTANTANEO) ---\n",
    "                if gradient_step % args.loss_log_interval == 0:\n",
    "\n",
    "                    # COSTRUZIONE DIZIONARIO SNAPSHOT\n",
    "                    training_stats_divided = {}\n",
    "                    for key in training_stats:\n",
    "                        splitted = key.split('/')\n",
    "                        if splitted[0] not in training_stats_divided:\n",
    "                            training_stats_divided[splitted[0]] = {}\n",
    "                        training_stats_divided[splitted[0]][splitted[1]] = training_stats[key].mean()\n",
    "                        \n",
    "                        # reset\n",
    "                        training_stats[key].reset()\n",
    "                        \n",
    "                    current_time = time.time()\n",
    "                    training_stats_divided['time']['SPS'] = env_step / (current_time - start_time + 1e-6)\n",
    "                    \n",
    "                    # log stats su wandb\n",
    "                    if args.wandb:\n",
    "                        log_stats_to_wandb(wandb_run, list(training_stats_divided.values()), list(training_stats_divided.keys()), env_step)\n",
    "                        print(f\"[{env_step}/{args.total_timesteps}] Logged training stats to wandb\")\n",
    "            \n",
    "            '''if env_step % 100 == 0:  # ogni 100 step\n",
    "                # 1. Reward batch\n",
    "                print(f\"[Step {env_step}] Reward batch stats: mean={data.rewards.mean():.4f}, std={data.rewards.std():.4f}, min={data.rewards.min():.4f}, max={data.rewards.max():.4f}\")\n",
    "\n",
    "                # 2. Critic target\n",
    "                print(f\"[Step {env_step}] next_q_value stats: mean={next_q_value.mean():.4f}, std={next_q_value.std():.4f}, min={next_q_value.min():.4f}, max={next_q_value.max():.4f}\")\n",
    "\n",
    "                # 3. Critic outputs\n",
    "                q_outputs = torch.stack([q(data.observations, data.actions).view(-1) for q in qf_ensemble])\n",
    "                print(f\"[Step {env_step}] Q ensemble outputs: mean={q_outputs.mean():.4f}, std={q_outputs.std():.4f}, min={q_outputs.min():.4f}, max={q_outputs.max():.4f}\")\n",
    "\n",
    "                # 4. Actor outputs (mean Q under policy)\n",
    "                with torch.no_grad():\n",
    "                    pi, log_pi, _ = actor.get_action(data.observations)\n",
    "                    q_pi_vals = torch.stack([q(data.observations, pi) for q in qf_ensemble])\n",
    "                print(f\"[Step {env_step}] Q under actor: mean={q_pi_vals.mean():.4f}, std={q_pi_vals.std():.4f}, min={q_pi_vals.min():.4f}, max={q_pi_vals.max():.4f}\")\n",
    "                print(f\"[Step {env_step}] Actor log_pi stats: mean={log_pi.mean():.4f}, std={log_pi.std():.4f}\")\n",
    "\n",
    "                # 5. Alpha\n",
    "                print(f\"[Step {env_step}] Alpha: {alpha:.4f}\")'''             \n",
    "                        \n",
    "except Exception as e:  \n",
    "    print(f\"[{env_step}/{args.total_timesteps}] An error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c8b7d",
   "metadata": {},
   "source": [
    "# Close Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb5d4909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing environment\n",
      "Closing wandb run\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>all_ep/SPL</td><td>▁▁▁▁▁▁▁▁▃▃▂▂▂▂▄▃▃▃▂▅▆▇█▇</td></tr><tr><td>all_ep/average_speed</td><td>▁▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅████</td></tr><tr><td>all_ep/collisions</td><td>█▇▇▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▁▂▁▁</td></tr><tr><td>all_ep/distance_traveled</td><td>█▆▅▅▆▄▃▃▂▂▂▂▁▃▂▁▁▃▅▇▆▅▇█</td></tr><tr><td>all_ep/ep_count</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██</td></tr><tr><td>all_ep/global_avg_dispersion</td><td>▂▁▁▃▃▃▃▄▄▄▃▄▄▅▆▆▇█▇▆▆▇▆▅</td></tr><tr><td>all_ep/global_avg_dist_obstacle</td><td>▂▁▁▃▄▄▃▂▂▂▁▂▂▃▅▅▆▇▆▅▅█▆▆</td></tr><tr><td>all_ep/global_avg_visibility</td><td>▁▁▁▃▃▄▄▄▅▅▄▅▅▆▇▇▇█▇▇▇▇▆▅</td></tr><tr><td>all_ep/global_characteristic_dimension</td><td>▁▁▂▃▄▄▄▄▅▅▅▅▅▆▇▇▇█▇▇▇█▇▇</td></tr><tr><td>all_ep/length</td><td>████████▅▆▆▇▇▇▅▆▆▆▇▅▄▂▁▂</td></tr><tr><td>+47</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>all_ep/SPL</td><td>0.07839</td></tr><tr><td>all_ep/average_speed</td><td>52.01755</td></tr><tr><td>all_ep/collisions</td><td>0.66333</td></tr><tr><td>all_ep/distance_traveled</td><td>35.66051</td></tr><tr><td>all_ep/ep_count</td><td>120</td></tr><tr><td>all_ep/global_avg_dispersion</td><td>10.84914</td></tr><tr><td>all_ep/global_avg_dist_obstacle</td><td>6.94066</td></tr><tr><td>all_ep/global_avg_visibility</td><td>17.42583</td></tr><tr><td>all_ep/global_characteristic_dimension</td><td>21.78021</td></tr><tr><td>all_ep/length</td><td>913.90156</td></tr><tr><td>+47</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">forse_va_3031923</strong> at: <a href='https://wandb.ai/giacomo-aru/UASRL/runs/z63fnt91' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL/runs/z63fnt91</a><br> View project at: <a href='https://wandb.ai/giacomo-aru/UASRL' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260115_155554-z63fnt91\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Closing environment\")\n",
    "env.close()\n",
    "\n",
    "print(\"Closing wandb run\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a243c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4844/2000000] Models saved, suffix: _final\n"
     ]
    }
   ],
   "source": [
    "# save trained networks, actor and critics\n",
    "save_models(actor, qf_ensemble, qf_ensemble_target, save_path, suffix='_final')\n",
    "print(f\"[{env_step}/{args.total_timesteps}] Models saved, suffix: _final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
