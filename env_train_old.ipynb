{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558fabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "import wandb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd740b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe8ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "from gymnasium import spaces \n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "from training_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd055c9a",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e98f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "def parse_args(default_config_path=\"./config/train_new_obs.yaml\"):\n",
    "    \"\"\"\n",
    "    Parse arguments from CLI or notebook.\n",
    "    - In notebook: usa il default se non passato\n",
    "    - In CLI: permette override dei parametri nel config\n",
    "    \"\"\"\n",
    "    # --- Gestione notebook: evita crash su ipykernel args ---\n",
    "    argv = sys.argv[1:]\n",
    "    # Se siamo in notebook o non Ã¨ passato il config_path, inseriamo il default\n",
    "    if len(argv) == 0 or \"--f=\" in \" \".join(argv):\n",
    "        argv = [default_config_path]\n",
    "\n",
    "    # --- Pre-parser per leggere il config_path ---\n",
    "    pre_parser = argparse.ArgumentParser(add_help=False)\n",
    "    pre_parser.add_argument(\n",
    "        \"config_path\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=default_config_path,\n",
    "        help=\"Main config file path\"\n",
    "    )\n",
    "    initial_args, remaining_argv = pre_parser.parse_known_args(argv)\n",
    "    CONFIG_PATH = initial_args.config_path\n",
    "    print(f\"Config path: {CONFIG_PATH}\")\n",
    "\n",
    "    # --- Legge parametri dal file di config ---\n",
    "    file_config_dict = parse_config_file(CONFIG_PATH)\n",
    "\n",
    "    # --- Parser principale ---\n",
    "    parser = argparse.ArgumentParser(description=\"Training Script\")\n",
    "    parser.add_argument(\n",
    "        \"config_path\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=CONFIG_PATH,\n",
    "        help=\"Main config file path\"\n",
    "    )\n",
    "\n",
    "    # Aggiunge parametri dal config file, con tipi corretti\n",
    "    for key, value in file_config_dict.items():\n",
    "        if isinstance(value, bool):\n",
    "            parser.add_argument(f\"--{key}\", type=str2bool, default=value)\n",
    "        elif value is None:\n",
    "            parser.add_argument(f\"--{key}\", type=str, default=value)\n",
    "        else:\n",
    "            parser.add_argument(f\"--{key}\", type=type(value), default=value)\n",
    "\n",
    "    # --- Parse finale con remaining_argv per ignorare args extra Jupyter ---\n",
    "    args, unknown = parser.parse_known_args(remaining_argv)\n",
    "    if unknown:\n",
    "        print(\"Ignored unknown args:\", unknown)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ba1ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config path: ./config/train_new_obs.yaml\n",
      "Training with the following parameters:\n",
      "{'actor_network_layers': [128, 128, 128],\n",
      " 'agent_config_path': './config/agent.yaml',\n",
      " 'alpha': 0.2,\n",
      " 'alpha_lr': 0.0001,\n",
      " 'autotune': True,\n",
      " 'base_time': 1765457030,\n",
      " 'batch_size': 256,\n",
      " 'bootstrap': True,\n",
      " 'bootstrap_batch_proportion': 0.8,\n",
      " 'buffer_size': 120000,\n",
      " 'build_path': './unity_build/3xnew_obs_wind/UASRL.exe',\n",
      " 'config_path': './config/train_new_obs.yaml',\n",
      " 'cuda': 0,\n",
      " 'env_id': '3xnew_obs',\n",
      " 'exp_name': 'old_pers',\n",
      " 'gamma': 0.995,\n",
      " 'headless': False,\n",
      " 'input_stack': 4,\n",
      " 'learning_starts': 1000,\n",
      " 'loss_log_interval': 500,\n",
      " 'machine_name': 'personal',\n",
      " 'metrics_log_interval': 10,\n",
      " 'metrics_smoothing': 0.98,\n",
      " 'n_envs': 3,\n",
      " 'noise_clip': 0.5,\n",
      " 'obstacles_config_path': './config/obstacles_simple.yaml',\n",
      " 'other_config_path': './config/other_new_obs.yaml',\n",
      " 'policy_frequency': 4,\n",
      " 'policy_lr': 0.0001,\n",
      " 'q_ensemble_n': 5,\n",
      " 'q_lr': 0.0001,\n",
      " 'q_network_layers': [128, 128, 128],\n",
      " 'reward_scale': 1.0,\n",
      " 'seed': 17042,\n",
      " 'target_entropy': -1.5,\n",
      " 'target_network_update_period': 1,\n",
      " 'tau': 0.005,\n",
      " 'test_lib': False,\n",
      " 'torch_deterministic': False,\n",
      " 'total_timesteps': 100000,\n",
      " 'update_frequency': 1,\n",
      " 'wandb': True,\n",
      " 'worker_id': 0}\n",
      "agent_config:\n",
      "{'action_debug': False,\n",
      " 'cbf_debug': False,\n",
      " 'ema_debug': False,\n",
      " 'ema_range_penalty': 15,\n",
      " 'ema_smoothing': 0.011,\n",
      " 'goal_reward': 10,\n",
      " 'hudHeight': 560,\n",
      " 'hudScale': 1.0,\n",
      " 'hudWidth': 320,\n",
      " 'hudX': 10,\n",
      " 'hudY': 10,\n",
      " 'hud_debug': False,\n",
      " 'max_movement_speed': 6,\n",
      " 'max_step': 1000,\n",
      " 'max_turn_speed': 120,\n",
      " 'move_smooth_time': 0.1,\n",
      " 'obstacle_test': False,\n",
      " 'print_debug': False,\n",
      " 'print_every_step': 10000,\n",
      " 'progress_reward': 0.05,\n",
      " 'stagnation_penalty': -0.02,\n",
      " 'step_after_goal': 10,\n",
      " 'wall_hit_penalty': 0}\n",
      "obstacles_config:\n",
      "{'check_path_try': 25,\n",
      " 'clear_threshold': 3,\n",
      " 'debug': False,\n",
      " 'external_walls': True,\n",
      " 'fill_threshold': 6,\n",
      " 'initial_fill_percentage': 0.47,\n",
      " 'moving_obstacles_count': 0,\n",
      " 'obstacles_total_width': 40,\n",
      " 'smoothing_iterations': 3,\n",
      " 'wall_height': 2,\n",
      " 'wall_resolution': 22}\n",
      "other_config:\n",
      "{'action_size': 2,\n",
      " 'behavior_name': 'NavigationAgent',\n",
      " 'max_action': 1.0,\n",
      " 'min_action': -1.0,\n",
      " 'rays_max_observation': 1.0,\n",
      " 'rays_min_observation': -1.0,\n",
      " 'rays_per_direction': 10,\n",
      " 'state_max_observation': 128.0,\n",
      " 'state_min_observation': -128.0,\n",
      " 'state_observation_size': 10,\n",
      " 'team': '0'}\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "agent_config = parse_config_file(args.agent_config_path)\n",
    "obstacles_config = parse_config_file(args.obstacles_config_path)\n",
    "other_config = parse_config_file(args.other_config_path)\n",
    "\n",
    "args.seed = random.randint(0, 2**16)\n",
    "# args.name = generate_funny_name()\n",
    "\n",
    "print('Training with the following parameters:')\n",
    "pprint(vars(args))\n",
    "\n",
    "print('agent_config:')\n",
    "pprint(agent_config)\n",
    "\n",
    "print('obstacles_config:')\n",
    "pprint(obstacles_config)\n",
    "\n",
    "print('other_config:')\n",
    "pprint(other_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8836c969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and args.cuda >= 0:\n",
    "    # F-string per inserire l'indice: diventa \"cuda:2\"\n",
    "    device_str = f\"cuda:{args.cuda}\"\n",
    "else:\n",
    "    device_str = \"cpu\"\n",
    "\n",
    "DEVICE = torch.device(device_str)\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1cd5e",
   "metadata": {},
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dcecf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 17042\n"
     ]
    }
   ],
   "source": [
    "# seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "print(f'Seed: {args.seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c69761",
   "metadata": {},
   "source": [
    "# Start Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce3cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Unity settings from config...\n",
      "Starting Unity Environment from build: ./unity_build/3xnew_obs_wind/UASRL.exe\n",
      "Unity Environment connected.\n"
     ]
    }
   ],
   "source": [
    "# Create the channel\n",
    "env_info = CustomChannel()\n",
    "param_channel = EnvironmentParametersChannel()\n",
    "\n",
    "print('Applying Unity settings from config...')\n",
    "apply_unity_settings(param_channel, agent_config, 'ag_')\n",
    "apply_unity_settings(param_channel, obstacles_config, 'obs_')\n",
    "\n",
    "if args.test_lib:\n",
    "    print('Testing Ended')\n",
    "    exit(0)\n",
    "\n",
    "# env setup\n",
    "print(f'Starting Unity Environment from build: {args.build_path}')\n",
    "# args.build_path\n",
    "env = UnityEnvironment(args.build_path, \n",
    "                       seed=args.seed, \n",
    "                       side_channels=[env_info, param_channel], \n",
    "                       no_graphics=args.headless,\n",
    "                       worker_id=args.worker_id)\n",
    "print('Unity Environment connected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "398130bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting environment...\n"
     ]
    }
   ],
   "source": [
    "print('Resetting environment...')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b073c",
   "metadata": {},
   "source": [
    "# Environment Variables and Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71f2cc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: old_pers_3372250\n",
      "Setting up wandb experiment tracking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgiacomoaru\u001b[0m (\u001b[33mgiacomo-aru\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\cicci\\Desktop\\UASRL\\wandb\\run-20260119_142801-5dkd3t5i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giacomo-aru/UASRL/runs/5dkd3t5i' target=\"_blank\">old_pers_3372250</a></strong> to <a href='https://wandb.ai/giacomo-aru/UASRL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giacomo-aru/UASRL' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giacomo-aru/UASRL/runs/5dkd3t5i' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL/runs/5dkd3t5i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = f\"{args.exp_name}_{int(time.time()) - args.base_time}\"\n",
    "args.run_name = run_name\n",
    "print(f\"Run name: {run_name}\")\n",
    "\n",
    "# wandb to track experiments\n",
    "# Start a new wandb run to track this script.\n",
    "if args.wandb:\n",
    "    print('Setting up wandb experiment tracking.')\n",
    "    wandb_run = wandb.init(\n",
    "        # Set the wandb entity where your project will be logged (generally your team name).\n",
    "        entity=\"giacomo-aru\",\n",
    "        # Set the wandb project where this run will be logged.\n",
    "        project=\"UASRL\",\n",
    "        # force the \n",
    "        name=args.run_name,\n",
    "        # Track hyperparameters and run metadata.\n",
    "        config={\n",
    "            \"training\": vars(args),\n",
    "            \"agent\": agent_config,\n",
    "            \"obstacles\": obstacles_config,\n",
    "            \"other\": other_config\n",
    "        }\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b43995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEHAVIOUR_NAME = other_config['behavior_name'] + '?team=' + other_config['team']\n",
    "\n",
    "RAY_PER_DIRECTION = other_config['rays_per_direction']\n",
    "RAYCAST_MIN = other_config['rays_min_observation']\n",
    "RAYCAST_MAX = other_config['rays_max_observation']\n",
    "RAYCAST_SIZE = 2*RAY_PER_DIRECTION + 1\n",
    "\n",
    "STATE_SIZE = other_config['state_observation_size'] - 1\n",
    "STATE_MIN = other_config['state_min_observation']\n",
    "STATE_MAX = other_config['state_max_observation']\n",
    "\n",
    "ACTION_SIZE = other_config['action_size']\n",
    "ACTION_MIN = other_config['min_action']\n",
    "ACTION_MAX = other_config['max_action']\n",
    "\n",
    "TOTAL_STATE_SIZE = (STATE_SIZE + RAYCAST_SIZE)*args.input_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736a2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating actor and critic networks...\n"
     ]
    }
   ],
   "source": [
    "# creating the training networks\n",
    "print('Creating actor and critic networks...')\n",
    "actor = OldDenseActor(TOTAL_STATE_SIZE, ACTION_SIZE, ACTION_MIN, ACTION_MAX, args.actor_network_layers).to(DEVICE)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)\n",
    "\n",
    "qf_ensemble = [OldDenseSoftQNetwork(TOTAL_STATE_SIZE, ACTION_SIZE, args.q_network_layers).to(DEVICE) for _ in range(args.q_ensemble_n)]\n",
    "qf_ensemble_target = [OldDenseSoftQNetwork(TOTAL_STATE_SIZE, ACTION_SIZE, args.q_network_layers).to(DEVICE) for _ in range(args.q_ensemble_n)]\n",
    "for q_t, q in zip(qf_ensemble_target, qf_ensemble):\n",
    "    q_t.load_state_dict(q.state_dict())\n",
    "\n",
    "par = []\n",
    "for q in qf_ensemble:\n",
    "    par += list(q.parameters())\n",
    "qf_optimizer = torch.optim.Adam(\n",
    "    par,\n",
    "    lr=args.q_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754acdf",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31990774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up replay buffer...\n"
     ]
    }
   ],
   "source": [
    "print('Setting up replay buffer...')\n",
    "observation_space = spaces.Box(\n",
    "    low=min(RAYCAST_MIN, STATE_MIN), \n",
    "    high=max(RAYCAST_MAX, STATE_MAX), \n",
    "    shape=(TOTAL_STATE_SIZE,), \n",
    "    dtype=np.float32\n",
    ")\n",
    "action_space = spaces.Box(\n",
    "    low=ACTION_MIN, \n",
    "    high=ACTION_MAX, \n",
    "    shape=(ACTION_SIZE,), \n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    buffer_size=args.buffer_size,\n",
    "    observation_space=observation_space,\n",
    "    action_space=action_space,\n",
    "    device=DEVICE,                \n",
    "    handle_timeout_termination=True,\n",
    "    n_envs=1 # necessario data la natura asincrona del'env   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a11d9",
   "metadata": {},
   "source": [
    "# start algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62f83fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autotune target_entropy: -1.5\n"
     ]
    }
   ],
   "source": [
    "# Automatic entropy tuning\n",
    "if args.autotune:\n",
    "    target_entropy = args.target_entropy\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=DEVICE)\n",
    "    alpha = log_alpha.exp().item()\n",
    "    a_optimizer = optim.Adam([log_alpha], lr=args.alpha_lr)\n",
    "    print(f'autotune target_entropy: {target_entropy}')\n",
    "else:\n",
    "    alpha = args.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8bde39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to path: ./models/old_pers_3372250\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "save_path = './models/' + run_name\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "print('saving to path:', save_path)\n",
    "\n",
    "training_stats = {\n",
    "    \"time/python_time\": RunningMean(),\n",
    "    \"time/unity_time\": RunningMean(),\n",
    "    \n",
    "    \"stats/action_saturation\": RunningMean(),\n",
    "    'stats/qf_mean': RunningMean(),\n",
    "    'stats/qf_std':RunningMean(),\n",
    "    'stats/actor_entropy': RunningMean(),\n",
    "    'stats/alpha': RunningMean(),\n",
    "    'stats/uncertainty': RunningMean(),\n",
    "    \n",
    "    'loss/critic_ens': RunningMean(),\n",
    "    'loss/actor': RunningMean(),\n",
    "    'loss/alpha': RunningMean(),\n",
    "}\n",
    "\n",
    "best_reward = -float('inf')\n",
    "\n",
    "episodic_stats = {}\n",
    "success_stats = {}\n",
    "failure_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9d6ed7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100000] Starting Training - run name: old_pers_3372250\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "unity_end_time = -1\n",
    "unity_start_time = -1\n",
    "\n",
    "global_step = 0\n",
    "print(f'[{global_step}/{args.total_timesteps}] Starting Training - run name: {run_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5cc390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1198/100000] Models saved, suffix: _c1_best\n",
      "[1500/100000] Logged training stats to wandb\n",
      "[1598/100000] |success: 0.00000|reward: -17.21633|collisions: 4.43124|length: 999.00000|SPL: 0.00000| SPS: 7\n",
      "[1598/100000] Logged episodic stats to wandb\n",
      "[1598/100000] Models saved, suffix: _c1_best\n",
      "[1998/100000] Models saved, suffix: _c1_best\n",
      "[2000/100000] Logged training stats to wandb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cicci\\AppData\\Local\\Temp\\ipykernel_23936\\1406042002.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  action, _, _ = actor.get_action(torch.Tensor([obs[id][0]]).to(DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2398/100000] |success: 0.00000|reward: -16.77410|collisions: 3.95276|length: 999.00000|SPL: 0.00000| SPS: 9\n",
      "[2398/100000] Logged episodic stats to wandb\n",
      "[2398/100000] Models saved, suffix: _c1_best\n",
      "[2500/100000] Logged training stats to wandb\n",
      "[2798/100000] |success: 0.00000|reward: -16.18581|collisions: 3.92769|length: 999.00000|SPL: 0.00000| SPS: 10\n",
      "[2798/100000] Logged episodic stats to wandb\n",
      "[2798/100000] Models saved, suffix: _c1_best\n",
      "[3000/100000] Logged training stats to wandb\n",
      "[3198/100000] Models saved, suffix: _c1_best\n",
      "[3500/100000] Logged training stats to wandb\n",
      "[3598/100000] |success: 0.00000|reward: -15.47549|collisions: 3.53050|length: 999.00000|SPL: 0.00000| SPS: 11\n",
      "[3598/100000] Logged episodic stats to wandb\n",
      "[3598/100000] Models saved, suffix: _c1_best\n",
      "[3998/100000] Models saved, suffix: _c1_best\n",
      "[4000/100000] Logged training stats to wandb\n",
      "[4398/100000] |success: 0.00000|reward: -14.04717|collisions: 3.34794|length: 999.00000|SPL: 0.00000| SPS: 12\n",
      "[4398/100000] Logged episodic stats to wandb\n",
      "[4398/100000] Models saved, suffix: _c1_best\n",
      "[4500/100000] Logged training stats to wandb\n",
      "[4798/100000] |success: 0.00000|reward: -13.47484|collisions: 3.12131|length: 999.00000|SPL: 0.00000| SPS: 13\n",
      "[4798/100000] Logged episodic stats to wandb\n",
      "[4798/100000] Models saved, suffix: _c1_best\n",
      "[5000/100000] Logged training stats to wandb\n",
      "[5116/100000] Models saved, suffix: _c1_best\n",
      "[5500/100000] Logged training stats to wandb\n",
      "[5518/100000] Models saved, suffix: _c1_best\n",
      "[5602/100000] |success: 0.03397|reward: -12.59397|collisions: 2.88627|length: 992.00138|SPL: 0.02684| SPS: 13\n",
      "[5602/100000] Logged episodic stats to wandb\n",
      "[6000/100000] Logged training stats to wandb\n",
      "[6004/100000] Models saved, suffix: _c1_best\n",
      "[6270/100000] |success: 0.06770|reward: -11.49800|collisions: 2.65328|length: 979.89352|SPL: 0.05419| SPS: 14\n",
      "[6270/100000] Logged episodic stats to wandb\n",
      "[6270/100000] Models saved, suffix: _c1_best\n",
      "[6324/100000] Models saved, suffix: _c1_best\n",
      "[6500/100000] Logged training stats to wandb\n",
      "[6674/100000] Models saved, suffix: _c1_best\n",
      "[6812/100000] |success: 0.05520|reward: -11.45422|collisions: 2.52818|length: 983.42110|SPL: 0.04418| SPS: 14\n",
      "[6812/100000] Logged episodic stats to wandb\n",
      "[7000/100000] Logged training stats to wandb\n",
      "[7500/100000] Logged training stats to wandb\n",
      "[7536/100000] |success: 0.04501|reward: -11.31595|collisions: 2.36591|length: 986.29739|SPL: 0.03602| SPS: 15\n",
      "[7536/100000] Logged episodic stats to wandb\n",
      "[7620/100000] Models saved, suffix: _c1_best\n",
      "[7842/100000] Models saved, suffix: _c1_best\n",
      "[8000/100000] Logged training stats to wandb\n",
      "[8024/100000] Models saved, suffix: _c1_best\n",
      "[8246/100000] |success: 0.07209|reward: -9.93679|collisions: 2.21270|length: 984.71441|SPL: 0.06115| SPS: 15\n",
      "[8246/100000] Logged episodic stats to wandb\n",
      "[8246/100000] Models saved, suffix: _c1_best\n",
      "[8344/100000] Models saved, suffix: _c1_best\n",
      "[8500/100000] Logged training stats to wandb\n",
      "[8832/100000] |success: 0.05878|reward: -10.37630|collisions: 2.33066|length: 987.35192|SPL: 0.04986| SPS: 15\n",
      "[8832/100000] Logged episodic stats to wandb\n",
      "[9000/100000] Logged training stats to wandb\n",
      "[9500/100000] Logged training stats to wandb\n",
      "[9556/100000] |success: 0.04793|reward: -10.21001|collisions: 2.42525|length: 989.50247|SPL: 0.04065| SPS: 15\n",
      "[9556/100000] Logged episodic stats to wandb\n",
      "[10000/100000] Logged training stats to wandb\n",
      "[10206/100000] |success: 0.07908|reward: -9.78830|collisions: 2.30816|length: 967.49598|SPL: 0.04588| SPS: 16\n",
      "[10206/100000] Logged episodic stats to wandb\n",
      "[10206/100000] Models saved, suffix: _c1_best\n",
      "[10404/100000] Models saved, suffix: _c1_best\n",
      "[10500/100000] Models saved, suffix: _c1_best\n",
      "[10500/100000] Logged training stats to wandb\n",
      "[10672/100000] |success: 0.13974|reward: -8.36109|collisions: 2.13329|length: 925.18038|SPL: 0.09354| SPS: 16\n",
      "[10672/100000] Logged episodic stats to wandb\n",
      "[10672/100000] Models saved, suffix: _c1_best\n",
      "[10906/100000] Models saved, suffix: _c1_best\n",
      "[11000/100000] Logged training stats to wandb\n",
      "[11312/100000] |success: 0.11394|reward: -8.81259|collisions: 2.11600|length: 938.80950|SPL: 0.07627| SPS: 16\n",
      "[11312/100000] Logged episodic stats to wandb\n",
      "[11500/100000] Logged training stats to wandb\n",
      "[11988/100000] |success: 0.09291|reward: -8.70507|collisions: 1.88213|length: 949.92231|SPL: 0.06219| SPS: 16\n",
      "[11988/100000] Logged episodic stats to wandb\n",
      "[12000/100000] Logged training stats to wandb\n",
      "[12338/100000] Models saved, suffix: _c1_best\n",
      "[12396/100000] Models saved, suffix: _c1_best\n",
      "[12500/100000] Logged training stats to wandb\n",
      "[12532/100000] |success: 0.11262|reward: -8.41972|collisions: 1.79336|length: 925.84266|SPL: 0.08757| SPS: 16\n",
      "[12532/100000] Logged episodic stats to wandb\n",
      "[13000/100000] Logged training stats to wandb\n",
      "[13204/100000] |success: 0.12580|reward: -8.76821|collisions: 1.71016|length: 935.57840|SPL: 0.08830| SPS: 16\n",
      "[13204/100000] Logged episodic stats to wandb\n",
      "[13500/100000] Logged training stats to wandb\n",
      "[13560/100000] Models saved, suffix: _c1_best\n",
      "[13910/100000] |success: 0.13944|reward: -7.84949|collisions: 1.64398|length: 930.47777|SPL: 0.10269| SPS: 17\n",
      "[13910/100000] Logged episodic stats to wandb\n",
      "[13910/100000] Models saved, suffix: _c1_best\n",
      "[14000/100000] Logged training stats to wandb\n",
      "[14014/100000] Models saved, suffix: _c1_best\n",
      "[14418/100000] |success: 0.11369|reward: -7.61073|collisions: 1.52195|length: 943.12885|SPL: 0.08373| SPS: 17\n",
      "[14418/100000] Logged episodic stats to wandb\n",
      "[14500/100000] Logged training stats to wandb\n",
      "[15000/100000] Logged training stats to wandb\n",
      "[15176/100000] |success: 0.09270|reward: -7.81225|collisions: 1.54251|length: 953.44419|SPL: 0.06827| SPS: 17\n",
      "[15176/100000] Logged episodic stats to wandb\n",
      "[15226/100000] Models saved, suffix: _c1_best\n",
      "[15500/100000] Logged training stats to wandb\n",
      "[15580/100000] Models saved, suffix: _c1_best\n",
      "[15930/100000] |success: 0.11245|reward: -6.88506|collisions: 1.40087|length: 961.85503|SPL: 0.09130| SPS: 17\n",
      "[15930/100000] Logged episodic stats to wandb\n",
      "[15930/100000] Models saved, suffix: _c1_best\n",
      "[16000/100000] Logged training stats to wandb\n",
      "[16282/100000] Models saved, suffix: _c1_best\n",
      "[16334/100000] Models saved, suffix: _c1_best\n",
      "[16436/100000] |success: 0.12855|reward: -6.74320|collisions: 1.24858|length: 959.12837|SPL: 0.10452| SPS: 17\n",
      "[16436/100000] Logged episodic stats to wandb\n",
      "[16500/100000] Logged training stats to wandb\n",
      "[16528/100000] Models saved, suffix: _c1_best\n",
      "[16686/100000] Models saved, suffix: _c1_best\n",
      "[17000/100000] Logged training stats to wandb\n",
      "[17088/100000] |success: 0.13879|reward: -6.28918|collisions: 1.16705|length: 940.12604|SPL: 0.11890| SPS: 17\n",
      "[17088/100000] Logged episodic stats to wandb\n",
      "[17500/100000] Logged training stats to wandb\n",
      "[17734/100000] |success: 0.15003|reward: -6.71463|collisions: 1.21910|length: 932.30574|SPL: 0.10318| SPS: 17\n",
      "[17734/100000] Logged episodic stats to wandb\n",
      "[18000/100000] Logged training stats to wandb\n",
      "[18300/100000] |success: 0.15920|reward: -6.44060|collisions: 1.10768|length: 944.17695|SPL: 0.11880| SPS: 17\n",
      "[18300/100000] Logged episodic stats to wandb\n",
      "[18500/100000] Logged training stats to wandb\n",
      "[18956/100000] |success: 0.12980|reward: -6.82595|collisions: 1.01377|length: 954.29878|SPL: 0.09687| SPS: 17\n",
      "[18956/100000] Logged episodic stats to wandb\n",
      "[19000/100000] Logged training stats to wandb\n",
      "[19500/100000] Logged training stats to wandb\n",
      "[19514/100000] |success: 0.13981|reward: -6.48834|collisions: 0.90033|length: 930.14078|SPL: 0.11296| SPS: 17\n",
      "[19514/100000] Logged episodic stats to wandb\n",
      "[20000/100000] Logged training stats to wandb\n",
      "[20176/100000] |success: 0.15240|reward: -7.03407|collisions: 1.12842|length: 939.59007|SPL: 0.11506| SPS: 17\n",
      "[20176/100000] Logged episodic stats to wandb\n",
      "[20500/100000] Logged training stats to wandb\n",
      "[20940/100000] |success: 0.12426|reward: -7.13939|collisions: 1.06022|length: 950.55877|SPL: 0.09381| SPS: 17\n",
      "[20940/100000] Logged episodic stats to wandb\n",
      "[21000/100000] Logged training stats to wandb\n",
      "[21450/100000] |success: 0.13671|reward: -6.39737|collisions: 0.86448|length: 929.42132|SPL: 0.10350| SPS: 18\n",
      "[21450/100000] Logged episodic stats to wandb\n",
      "[21500/100000] Logged training stats to wandb\n",
      "[21552/100000] Models saved, suffix: _c1_best\n",
      "[21704/100000] Models saved, suffix: _c1_best\n",
      "[21790/100000] Models saved, suffix: _c1_best\n",
      "[21858/100000] |success: 0.22212|reward: -5.09042|collisions: 0.97245|length: 887.90607|SPL: 0.18154| SPS: 18\n",
      "[21858/100000] Logged episodic stats to wandb\n",
      "[22000/100000] Logged training stats to wandb\n",
      "[22146/100000] Models saved, suffix: _c1_best\n",
      "[22500/100000] Logged training stats to wandb\n",
      "[22552/100000] |success: 0.21650|reward: -5.44129|collisions: 0.98971|length: 903.99336|SPL: 0.16874| SPS: 18\n",
      "[22552/100000] Logged episodic stats to wandb\n",
      "[23000/100000] Logged training stats to wandb\n",
      "[23076/100000] |success: 0.21493|reward: -5.36240|collisions: 1.02239|length: 893.07978|SPL: 0.17599| SPS: 18\n",
      "[23076/100000] Logged episodic stats to wandb\n",
      "[23500/100000] Logged training stats to wandb\n",
      "[23732/100000] |success: 0.24922|reward: -4.19302|collisions: 1.05653|length: 885.87785|SPL: 0.20574| SPS: 18\n",
      "[23732/100000] Logged episodic stats to wandb\n",
      "[23732/100000] Models saved, suffix: _c1_best\n",
      "[23774/100000] Models saved, suffix: _c1_best\n",
      "[23890/100000] Models saved, suffix: _c1_best\n",
      "[24000/100000] Logged training stats to wandb\n",
      "[24296/100000] |success: 0.23718|reward: -4.39729|collisions: 0.97986|length: 876.11887|SPL: 0.19507| SPS: 18\n",
      "[24296/100000] Logged episodic stats to wandb\n",
      "[24440/100000] Models saved, suffix: _c1_best\n",
      "[24500/100000] Logged training stats to wandb\n",
      "[24844/100000] |success: 0.26276|reward: -4.32024|collisions: 0.95102|length: 873.69656|SPL: 0.21865| SPS: 18\n",
      "[24844/100000] Logged episodic stats to wandb\n",
      "[25000/100000] Logged training stats to wandb\n",
      "[25500/100000] Logged training stats to wandb\n",
      "[25502/100000] |success: 0.25424|reward: -4.99966|collisions: 1.02776|length: 890.51100|SPL: 0.21132| SPS: 18\n",
      "[25502/100000] Logged episodic stats to wandb\n",
      "[25944/100000] |success: 0.28417|reward: -3.85429|collisions: 0.94879|length: 871.28989|SPL: 0.24034| SPS: 18\n",
      "[25944/100000] Logged episodic stats to wandb\n",
      "[26000/100000] Logged training stats to wandb\n",
      "[26210/100000] Models saved, suffix: _c1_best\n",
      "[26310/100000] Models saved, suffix: _c1_best\n",
      "[26500/100000] Logged training stats to wandb\n",
      "[26700/100000] |success: 0.30568|reward: -3.29161|collisions: 0.96101|length: 880.95328|SPL: 0.22974| SPS: 18\n",
      "[26700/100000] Logged episodic stats to wandb\n",
      "[26964/100000] Models saved, suffix: _c1_best\n",
      "[27000/100000] Logged training stats to wandb\n",
      "[27008/100000] Models saved, suffix: _c1_best\n",
      "[27334/100000] |success: 0.36149|reward: -2.11779|collisions: 0.82198|length: 870.60492|SPL: 0.27037| SPS: 18\n",
      "[27334/100000] Logged episodic stats to wandb\n",
      "[27334/100000] Models saved, suffix: _c1_best\n",
      "[27500/100000] Logged training stats to wandb\n",
      "[27816/100000] |success: 0.29475|reward: -3.68427|collisions: 0.95948|length: 894.31016|SPL: 0.22045| SPS: 18\n",
      "[27816/100000] Logged episodic stats to wandb\n",
      "[28000/100000] Logged training stats to wandb\n",
      "[28500/100000] Logged training stats to wandb\n",
      "[28548/100000] |success: 0.27873|reward: -3.87674|collisions: 0.89772|length: 890.56036|SPL: 0.20292| SPS: 18\n",
      "[28548/100000] Logged episodic stats to wandb\n",
      "[29000/100000] Logged training stats to wandb\n",
      "[29146/100000] |success: 0.22727|reward: -4.48496|collisions: 0.87218|length: 910.58128|SPL: 0.16546| SPS: 18\n",
      "[29146/100000] Logged episodic stats to wandb\n",
      "[29500/100000] Logged training stats to wandb\n",
      "[29660/100000] |success: 0.29756|reward: -2.99078|collisions: 0.74654|length: 889.75846|SPL: 0.22333| SPS: 18\n",
      "[29660/100000] Logged episodic stats to wandb\n",
      "[30000/100000] Logged training stats to wandb\n",
      "[30362/100000] |success: 0.31500|reward: -3.10318|collisions: 0.67949|length: 884.74527|SPL: 0.22373| SPS: 18\n",
      "[30362/100000] Logged episodic stats to wandb\n",
      "[30500/100000] Logged training stats to wandb\n",
      "[30996/100000] |success: 0.25684|reward: -4.14967|collisions: 0.66623|length: 905.83981|SPL: 0.18243| SPS: 18\n",
      "[30996/100000] Logged episodic stats to wandb\n",
      "[31000/100000] Logged training stats to wandb\n",
      "[31500/100000] Logged training stats to wandb\n",
      "[31582/100000] |success: 0.24782|reward: -4.52718|collisions: 0.94448|length: 921.27333|SPL: 0.18150| SPS: 18\n",
      "[31582/100000] Logged episodic stats to wandb\n",
      "[32000/100000] Logged training stats to wandb\n",
      "[32216/100000] |success: 0.23604|reward: -4.99317|collisions: 0.84383|length: 919.75800|SPL: 0.18146| SPS: 18\n",
      "[32216/100000] Logged episodic stats to wandb\n",
      "[32500/100000] Logged training stats to wandb\n",
      "[32938/100000] |success: 0.26625|reward: -4.26572|collisions: 0.80177|length: 929.32755|SPL: 0.20302| SPS: 18\n",
      "[32938/100000] Logged episodic stats to wandb\n",
      "[33000/100000] Logged training stats to wandb\n",
      "[33500/100000] Logged training stats to wandb\n",
      "[33538/100000] |success: 0.25248|reward: -3.64596|collisions: 0.69060|length: 916.53364|SPL: 0.20057| SPS: 18\n",
      "[33538/100000] Logged episodic stats to wandb\n",
      "[34000/100000] Logged training stats to wandb\n",
      "[34120/100000] |success: 0.24587|reward: -3.52106|collisions: 0.74465|length: 928.43918|SPL: 0.20119| SPS: 18\n",
      "[34120/100000] Logged episodic stats to wandb\n",
      "[34500/100000] Logged training stats to wandb\n",
      "[34822/100000] |success: 0.20047|reward: -3.70290|collisions: 0.79340|length: 941.46664|SPL: 0.16404| SPS: 18\n",
      "[34822/100000] Logged episodic stats to wandb\n",
      "[35000/100000] Logged training stats to wandb\n",
      "[35352/100000] |success: 0.27282|reward: -2.99270|collisions: 0.68089|length: 902.12375|SPL: 0.22409| SPS: 18\n",
      "[35352/100000] Logged episodic stats to wandb\n",
      "[35500/100000] Logged training stats to wandb\n",
      "[35870/100000] |success: 0.29471|reward: -2.58847|collisions: 0.62601|length: 883.74295|SPL: 0.24605| SPS: 18\n",
      "[35870/100000] Logged episodic stats to wandb\n",
      "[36000/100000] Logged training stats to wandb\n",
      "[36350/100000] |success: 0.31267|reward: -2.96677|collisions: 0.61961|length: 882.54573|SPL: 0.26951| SPS: 18\n",
      "[36350/100000] Logged episodic stats to wandb\n",
      "[36500/100000] Logged training stats to wandb\n",
      "[36536/100000] Models saved, suffix: _c1_best\n",
      "[36940/100000] |success: 0.32431|reward: -2.28065|collisions: 0.54362|length: 856.41360|SPL: 0.28567| SPS: 18\n",
      "[36940/100000] Logged episodic stats to wandb\n",
      "[37000/100000] Logged training stats to wandb\n",
      "[37408/100000] |success: 0.34283|reward: -2.16367|collisions: 0.51722|length: 833.11974|SPL: 0.30981| SPS: 18\n",
      "[37408/100000] Logged episodic stats to wandb\n",
      "[37500/100000] Logged training stats to wandb\n",
      "[38000/100000] Logged training stats to wandb\n",
      "[38202/100000] |success: 0.27953|reward: -4.01387|collisions: 0.63726|length: 863.74577|SPL: 0.25261| SPS: 18\n",
      "[38202/100000] Logged episodic stats to wandb\n",
      "[38500/100000] Logged training stats to wandb\n",
      "[38764/100000] |success: 0.26479|reward: -4.00019|collisions: 0.51960|length: 878.80097|SPL: 0.24073| SPS: 18\n",
      "[38764/100000] Logged episodic stats to wandb\n",
      "[39000/100000] Logged training stats to wandb\n",
      "[39142/100000] |success: 0.32969|reward: -3.09186|collisions: 0.67588|length: 836.31577|SPL: 0.27990| SPS: 18\n",
      "[39142/100000] Logged episodic stats to wandb\n",
      "[39500/100000] Logged training stats to wandb\n",
      "[39586/100000] |success: 0.33818|reward: -3.05611|collisions: 0.66507|length: 819.35596|SPL: 0.28996| SPS: 18\n",
      "[39586/100000] Logged episodic stats to wandb\n",
      "[40000/100000] Logged training stats to wandb\n",
      "[40126/100000] |success: 0.38511|reward: -2.38802|collisions: 0.68678|length: 799.00712|SPL: 0.33452| SPS: 18\n",
      "[40126/100000] Logged episodic stats to wandb\n",
      "[40272/100000] Models saved, suffix: _c1_best\n",
      "[40288/100000] Models saved, suffix: _c1_best\n",
      "[40322/100000] Models saved, suffix: _c1_best\n",
      "[40380/100000] Models saved, suffix: _c1_best\n",
      "[40500/100000] Logged training stats to wandb\n",
      "[40652/100000] |success: 0.49864|reward: -0.12310|collisions: 0.59537|length: 752.68542|SPL: 0.43730| SPS: 19\n",
      "[40652/100000] Logged episodic stats to wandb\n",
      "[40652/100000] Models saved, suffix: _c1_best\n",
      "[41000/100000] Logged training stats to wandb\n",
      "[41102/100000] |success: 0.40657|reward: -2.20111|collisions: 0.67162|length: 798.16181|SPL: 0.35656| SPS: 19\n",
      "[41102/100000] Logged episodic stats to wandb\n",
      "[41500/100000] Logged training stats to wandb\n",
      "[41868/100000] |success: 0.36548|reward: -3.71196|collisions: 0.61840|length: 827.66586|SPL: 0.31136| SPS: 19\n",
      "[41868/100000] Logged episodic stats to wandb\n",
      "[42000/100000] Logged training stats to wandb\n",
      "[42500/100000] Logged training stats to wandb\n",
      "[42510/100000] |success: 0.37339|reward: -3.84465|collisions: 0.50423|length: 844.11139|SPL: 0.31577| SPS: 19\n",
      "[42510/100000] Logged episodic stats to wandb\n",
      "[43000/100000] Logged training stats to wandb\n",
      "[43118/100000] |success: 0.34132|reward: -4.89818|collisions: 0.41113|length: 860.32176|SPL: 0.28643| SPS: 19\n",
      "[43118/100000] Logged episodic stats to wandb\n",
      "[43500/100000] Logged training stats to wandb\n",
      "[43818/100000] |success: 0.27830|reward: -6.73687|collisions: 0.36920|length: 885.92555|SPL: 0.23354| SPS: 19\n",
      "[43818/100000] Logged episodic stats to wandb\n",
      "[44000/100000] Logged training stats to wandb\n",
      "[44386/100000] |success: 0.22692|reward: -7.49565|collisions: 0.37630|length: 906.80218|SPL: 0.19043| SPS: 19\n",
      "[44386/100000] Logged episodic stats to wandb\n",
      "[44500/100000] Logged training stats to wandb\n",
      "[44924/100000] |success: 0.25586|reward: -7.57776|collisions: 0.41901|length: 868.06286|SPL: 0.20827| SPS: 19\n",
      "[44924/100000] Logged episodic stats to wandb\n",
      "[45000/100000] Logged training stats to wandb\n",
      "[45478/100000] |success: 0.24702|reward: -8.26316|collisions: 0.34165|length: 859.40543|SPL: 0.20517| SPS: 19\n",
      "[45478/100000] Logged episodic stats to wandb\n",
      "[45500/100000] Logged training stats to wandb\n",
      "[46000/100000] Logged training stats to wandb\n",
      "[46198/100000] |success: 0.20142|reward: -9.79766|collisions: 0.39697|length: 885.17840|SPL: 0.16729| SPS: 19\n",
      "[46198/100000] Logged episodic stats to wandb\n",
      "[46500/100000] Logged training stats to wandb\n",
      "[46714/100000] |success: 0.20423|reward: -9.87502|collisions: 0.32368|length: 902.27298|SPL: 0.17162| SPS: 19\n",
      "[46714/100000] Logged episodic stats to wandb\n",
      "[47000/100000] Logged training stats to wandb\n",
      "[47348/100000] |success: 0.23589|reward: -9.27535|collisions: 0.33765|length: 870.39086|SPL: 0.20727| SPS: 19\n",
      "[47348/100000] Logged episodic stats to wandb\n",
      "[47500/100000] Logged training stats to wandb\n",
      "[47906/100000] |success: 0.22920|reward: -10.06365|collisions: 0.50289|length: 884.36666|SPL: 0.19753| SPS: 19\n",
      "[47906/100000] Logged episodic stats to wandb\n",
      "[48000/100000] Logged training stats to wandb\n",
      "[48494/100000] |success: 0.25625|reward: -9.86442|collisions: 0.41004|length: 867.95884|SPL: 0.22623| SPS: 19\n",
      "[48494/100000] Logged episodic stats to wandb\n",
      "[48500/100000] Logged training stats to wandb\n",
      "[49000/100000] Logged training stats to wandb\n",
      "[49162/100000] |success: 0.20894|reward: -11.14217|collisions: 0.40228|length: 892.15262|SPL: 0.18446| SPS: 19\n",
      "[49162/100000] Logged episodic stats to wandb\n",
      "[49500/100000] Logged training stats to wandb\n",
      "[49742/100000] |success: 0.17036|reward: -12.25516|collisions: 0.32801|length: 911.87956|SPL: 0.15041| SPS: 19\n",
      "[49742/100000] Logged episodic stats to wandb\n",
      "[50000/100000] Logged training stats to wandb\n",
      "[50486/100000] |success: 0.17891|reward: -11.92116|collisions: 0.34745|length: 898.40437|SPL: 0.15325| SPS: 19\n",
      "[50486/100000] Logged episodic stats to wandb\n",
      "[50500/100000] Logged training stats to wandb\n",
      "[50962/100000] |success: 0.18274|reward: -11.54837|collisions: 0.31728|length: 891.09854|SPL: 0.16182| SPS: 19\n",
      "[50962/100000] Logged episodic stats to wandb\n",
      "[51000/100000] Logged training stats to wandb\n",
      "[51500/100000] Logged training stats to wandb\n",
      "[51744/100000] |success: 0.14900|reward: -12.85637|collisions: 0.25870|length: 911.02010|SPL: 0.13194| SPS: 19\n",
      "[51744/100000] Logged episodic stats to wandb\n",
      "[52000/100000] Logged training stats to wandb\n",
      "[52234/100000] |success: 0.12149|reward: -13.13259|collisions: 0.42929|length: 927.26359|SPL: 0.10758| SPS: 19\n",
      "[52234/100000] Logged episodic stats to wandb\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 181\u001b[0m\n\u001b[0;32m    178\u001b[0m alpha_loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mlog_alpha \u001b[38;5;241m*\u001b[39m (log_pi \u001b[38;5;241m+\u001b[39m target_entropy))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    180\u001b[0m a_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 181\u001b[0m \u001b[43malpha_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m a_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    183\u001b[0m alpha \u001b[38;5;241m=\u001b[39m log_alpha\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\cicci\\Desktop\\UASRL\\.venv\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cicci\\Desktop\\UASRL\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cicci\\Desktop\\UASRL\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    obs = collect_data_after_step(env, BEHAVIOUR_NAME, STATE_SIZE)\n",
    "    \n",
    "    \n",
    "    while global_step < args.total_timesteps:\n",
    "\n",
    "        # actions for each agent in the environment\n",
    "        # dim = (naagents, action_space)\n",
    "        for id in obs:\n",
    "            agent_obs = obs[id]\n",
    "            \n",
    "            # terminated agents are not considered\n",
    "            if agent_obs[3]:\n",
    "                continue\n",
    "            \n",
    "            # algo logic\n",
    "            if global_step < args.learning_starts * 2:\n",
    "                # change this to use the handcrafted starting policy or a previously trained policy\n",
    "                \n",
    "                action = get_initial_action(id)\n",
    "                # action, _, _ = old_actor.get_action(torch.Tensor([obs[id][0]]), \n",
    "                #                                 torch.Tensor([obs[id][1]]),\n",
    "                #                                 0.5)\n",
    "                # action = action[0].detach().numpy()\n",
    "            else:\n",
    "                # training policy\n",
    "                action, _, _ = actor.get_action(torch.Tensor([obs[id][0]]).to(DEVICE))\n",
    "                action = action[0].detach().cpu().numpy()\n",
    "            \n",
    "            # memorize the action taken for the next step\n",
    "            agent_obs[2] = action\n",
    "            \n",
    "            # the first dimention of the action is the \"number of agent\"\n",
    "            # Always 1 if \"set_action_for_agent\" is used\n",
    "            a = ActionTuple(continuous=np.array([action]))\n",
    "            env.set_action_for_agent(BEHAVIOUR_NAME, id, a)\n",
    "        \n",
    "        # --- ENVIRONMENT STEP ---\n",
    "        unity_start_time = time.time()\n",
    "        if unity_end_time > 0 and global_step > args.learning_starts:\n",
    "            training_stats['time/python_time'].update(unity_start_time - unity_end_time)\n",
    "        \n",
    "        env.step()\n",
    "        unity_end_time = time.time()\n",
    "        if global_step > args.learning_starts:\n",
    "            training_stats['time/unity_time'].update(unity_end_time - unity_start_time)\n",
    "\n",
    "        next_obs = collect_data_after_step(env, BEHAVIOUR_NAME, STATE_SIZE)\n",
    "        \n",
    "        while env_info.stop_msg_queue:\n",
    "                msg = env_info.stop_msg_queue.pop()\n",
    "                \n",
    "                if global_step >= args.learning_starts:\n",
    "                    update_stats_from_message(episodic_stats, success_stats, failure_stats, msg, args.metrics_smoothing)        \n",
    "                    if episodic_stats['ep_count'] % args.metrics_log_interval == 0:\n",
    "                        print_update(global_step, args.total_timesteps, start_time, episodic_stats)\n",
    "                        if args.wandb:\n",
    "                            log_stats_to_wandb(wandb_run, \n",
    "                                            [episodic_stats, success_stats, failure_stats],\n",
    "                                            ['all_ep', 'success_ep', 'failure_ep'],\n",
    "                                            global_step)\n",
    "                            print(f\"[{global_step}/{args.total_timesteps}] Logged episodic stats to wandb\")\n",
    "                        \n",
    "        # save data to reply buffer; handle `terminal_observation`\n",
    "        for id in obs:\n",
    "            prev_agent_obs = obs[id]\n",
    "            # consider every agent that in the previous step was not terminated\n",
    "            # in this way are excluded the agents that are already considered before and don't have a \n",
    "            # couple prev_obs - next_obs and a reward\n",
    "            if prev_agent_obs[3] or id not in next_obs:\n",
    "                continue\n",
    "                \n",
    "            next_agent_obs = next_obs[id]\n",
    "            \n",
    "            # add the data to the replay buffer\n",
    "            rb.add(obs = prev_agent_obs[0], \n",
    "                next_obs = next_agent_obs[0],\n",
    "                action = np.array(prev_agent_obs[2]), \n",
    "                reward = next_agent_obs[1], \n",
    "                done = next_agent_obs[3],\n",
    "                infos = [{}])\n",
    "            \n",
    "        # crucial step, easy to overlook, update the previous observation\n",
    "        obs = next_obs\n",
    "        \n",
    "        # Save best models based on reward\n",
    "        if episodic_stats != {} and episodic_stats[\"reward\"] > best_reward:\n",
    "            best_reward = episodic_stats[\"reward\"]\n",
    "            save_models(actor, qf_ensemble, qf_ensemble_target, save_path, suffix=f'_best')\n",
    "            print(f\"[{global_step}/{args.total_timesteps}] Models saved, suffix: _best\")\n",
    "                \n",
    "        # Training loop\n",
    "        for _ in range(args.update_frequency):\n",
    "\n",
    "            # Start learning after a warm-up phase\n",
    "            if global_step > args.learning_starts:\n",
    "\n",
    "                # Sample a batch from replay buffer\n",
    "                data = rb.sample(args.batch_size)\n",
    "\n",
    "                # --- CALCOLO SATURAZIONE ---\n",
    "                saturation = data.actions.detach().cpu().numpy()\n",
    "                saturation = (np.abs(saturation) > 0.99).mean()\n",
    "                training_stats[\"stats/action_saturation\"].update(saturation)\n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    # Compute target action with exploration noise\n",
    "                    next_action, next_log_pi, _ = actor.get_action(\n",
    "                        data.next_observations\n",
    "                    )\n",
    "\n",
    "                    if args.noise_clip > 0:\n",
    "                        noise = torch.randn_like(next_action) * args.noise_clip\n",
    "                        next_action = torch.clamp(next_action + noise, -1, 1)\n",
    "\n",
    "                    # Compute target Q-value (min over ensemble)\n",
    "                    target_q_values = []\n",
    "                    for q_target in qf_ensemble_target:\n",
    "                        q_val = q_target(\n",
    "                            data.next_observations, \n",
    "                            next_action\n",
    "                        )\n",
    "                        target_q_values.append(q_val)\n",
    "                    stacked_target_q = torch.stack(target_q_values)\n",
    "                    min_qf_next_target = stacked_target_q.min(dim=0).values - alpha * next_log_pi\n",
    "                    next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * min_qf_next_target.view(-1)\n",
    "\n",
    "                # Q-function updates (with bootstrapping)\n",
    "                q_losses = []\n",
    "                q_vals = []\n",
    "                batch_size = int(data.actions.shape[0] * args.bootstrap_batch_proportion)\n",
    "                for q in qf_ensemble:\n",
    "                    # Bootstrap indices\n",
    "                    indices = torch.randint(0, batch_size, (batch_size,), device=data.actions.device)\n",
    "                    \n",
    "                    observation = data.observations[indices]\n",
    "                    actions = data.actions[indices]\n",
    "                    target = next_q_value[indices]\n",
    "\n",
    "                    # Compute Q loss\n",
    "                    q_val = q(observation, actions).view(-1)\n",
    "                    loss = F.mse_loss(q_val, target)\n",
    "                    q_losses.append(loss)\n",
    "                    q_vals.append(q_val)\n",
    "                    \n",
    "                total_q_loss = torch.stack(q_losses).mean()\n",
    "                qf_optimizer.zero_grad()\n",
    "                total_q_loss.backward()\n",
    "                qf_optimizer.step()\n",
    "                \n",
    "                # Track Q-value statistics\n",
    "                all_q_values = torch.cat(q_vals)\n",
    "                training_stats['stats/qf_mean'].update(all_q_values.mean().item())\n",
    "                training_stats['stats/qf_std'].update(all_q_values.std().item())\n",
    "                training_stats['loss/critic_ens'].update(total_q_loss.item())\n",
    "                \n",
    "                # Delayed policy (actor) update\n",
    "                if global_step % args.policy_frequency == 0:\n",
    "                    for _ in range(args.policy_frequency):\n",
    "                        pi, log_pi, _ = actor.get_action(data.observations)\n",
    "                        actor_entropy = - (log_pi.exp() * log_pi).sum(dim=-1).mean()\n",
    "\n",
    "                        q_pi_vals = [q(data.observations, pi) for q in qf_ensemble]\n",
    "                        min_qf_pi = torch.min(torch.stack(q_pi_vals), dim=0).values.view(-1)\n",
    "\n",
    "                        actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "                        actor_optimizer.zero_grad()\n",
    "                        actor_loss.backward()\n",
    "                        actor_optimizer.step()\n",
    "\n",
    "                        # 1. Calcolo Incertezza (Disaccordo tra i critici)\n",
    "                        q_pi_stack = torch.stack(q_pi_vals) \n",
    "                        with torch.no_grad():\n",
    "                            uncertainty = q_pi_stack.std(dim=0).mean().item()\n",
    "                        training_stats['stats/uncertainty'].update(uncertainty)\n",
    "\n",
    "                        # 2. Log Entropia e Loss Attore\n",
    "                        training_stats['stats/actor_entropy'].update(-log_pi.mean().item())\n",
    "                        training_stats['loss/actor'].update(actor_loss.item())\n",
    "                        \n",
    "                        # Automatic entropy tuning (if enabled)\n",
    "                        if args.autotune:\n",
    "                            with torch.no_grad():\n",
    "                                _, log_pi, _ = actor.get_action(data.observations)\n",
    "                            alpha_loss = (-log_alpha * (log_pi + target_entropy)).mean()\n",
    "\n",
    "                            a_optimizer.zero_grad()\n",
    "                            alpha_loss.backward()\n",
    "                            a_optimizer.step()\n",
    "                            alpha = log_alpha.exp().item()\n",
    "                            \n",
    "                            training_stats['loss/alpha'].update(alpha_loss.item())\n",
    "\n",
    "                        training_stats['stats/alpha'].update(alpha)\n",
    "                        \n",
    "                # Soft update target Q-networks\n",
    "                if global_step % args.target_network_update_period == 0:\n",
    "                    for q, q_t in zip(qf_ensemble, qf_ensemble_target):\n",
    "                        for param, target_param in zip(q.parameters(), q_t.parameters()):\n",
    "                            target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "                # --- 5. LOGGING LOSS (METODO SNAPSHOT/ISTANTANEO) ---\n",
    "                if global_step % args.loss_log_interval == 0:\n",
    "\n",
    "                    # COSTRUZIONE DIZIONARIO SNAPSHOT\n",
    "                    training_stats_divided = {}\n",
    "                    for key in training_stats:\n",
    "                        splitted = key.split('/')\n",
    "                        if splitted[0] not in training_stats_divided:\n",
    "                            training_stats_divided[splitted[0]] = {}\n",
    "                        training_stats_divided[splitted[0]][splitted[1]] = training_stats[key].mean()\n",
    "                        \n",
    "                        # reset\n",
    "                        training_stats[key].reset()\n",
    "                        \n",
    "                    current_time = time.time()\n",
    "                    training_stats_divided['time']['SPS'] = global_step / (current_time - start_time + 1e-6)\n",
    "                    \n",
    "                    # log stats su wandb\n",
    "                    if args.wandb:\n",
    "                        log_stats_to_wandb(wandb_run, list(training_stats_divided.values()), list(training_stats_divided.keys()), global_step)\n",
    "                        print(f\"[{global_step}/{args.total_timesteps}] Logged training stats to wandb\")  \n",
    "                            \n",
    "            elif global_step == args.learning_starts:\n",
    "                print(\"Start Learning\")\n",
    "\n",
    "            # Step counter\n",
    "            global_step += 1\n",
    "            \n",
    "except Exception as e:  \n",
    "    print(f\"[{global_step}/{args.total_timesteps}] An error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c8b7d",
   "metadata": {},
   "source": [
    "# Close Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d4909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing environment\n",
      "Closing wandb run\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">old_pers_3192855</strong> at: <a href='https://wandb.ai/giacomo-aru/UASRL/runs/k3gtyi3l' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL/runs/k3gtyi3l</a><br> View project at: <a href='https://wandb.ai/giacomo-aru/UASRL' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260117_123805-k3gtyi3l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Closing environment\")\n",
    "env.close()\n",
    "\n",
    "print(\"Closing wandb run\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a243c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100000] Models saved, suffix: _final\n"
     ]
    }
   ],
   "source": [
    "# save trained networks, actor and critics\n",
    "save_models(actor, qf_ensemble, qf_ensemble_target, save_path, suffix='_final')\n",
    "print(f\"[{global_step}/{args.total_timesteps}] Models saved, suffix: _final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
