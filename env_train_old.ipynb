{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558fabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "import wandb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd740b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe8ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "from gymnasium import spaces \n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "from training_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd055c9a",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e98f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "def parse_args(default_config_path=\"./config/train.yaml\"):\n",
    "    \"\"\"\n",
    "    Parse arguments from CLI or notebook.\n",
    "    - In notebook: usa il default se non passato\n",
    "    - In CLI: permette override dei parametri nel config\n",
    "    \"\"\"\n",
    "    # --- Gestione notebook: evita crash su ipykernel args ---\n",
    "    argv = sys.argv[1:]\n",
    "    # Se siamo in notebook o non è passato il config_path, inseriamo il default\n",
    "    if len(argv) == 0 or \"--f=\" in \" \".join(argv):\n",
    "        argv = [default_config_path]\n",
    "\n",
    "    # --- Pre-parser per leggere il config_path ---\n",
    "    pre_parser = argparse.ArgumentParser(add_help=False)\n",
    "    pre_parser.add_argument(\n",
    "        \"config_path\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=default_config_path,\n",
    "        help=\"Main config file path\"\n",
    "    )\n",
    "    initial_args, remaining_argv = pre_parser.parse_known_args(argv)\n",
    "    CONFIG_PATH = initial_args.config_path\n",
    "    print(f\"Config path: {CONFIG_PATH}\")\n",
    "\n",
    "    # --- Legge parametri dal file di config ---\n",
    "    file_config_dict = parse_config_file(CONFIG_PATH)\n",
    "\n",
    "    # --- Parser principale ---\n",
    "    parser = argparse.ArgumentParser(description=\"Training Script\")\n",
    "    parser.add_argument(\n",
    "        \"config_path\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=CONFIG_PATH,\n",
    "        help=\"Main config file path\"\n",
    "    )\n",
    "\n",
    "    # Aggiunge parametri dal config file, con tipi corretti\n",
    "    for key, value in file_config_dict.items():\n",
    "        if isinstance(value, bool):\n",
    "            parser.add_argument(f\"--{key}\", type=str2bool, default=value)\n",
    "        elif value is None:\n",
    "            parser.add_argument(f\"--{key}\", type=str, default=value)\n",
    "        else:\n",
    "            parser.add_argument(f\"--{key}\", type=type(value), default=value)\n",
    "\n",
    "    # --- Parse finale con remaining_argv per ignorare args extra Jupyter ---\n",
    "    args, unknown = parser.parse_known_args(remaining_argv)\n",
    "    if unknown:\n",
    "        print(\"Ignored unknown args:\", unknown)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ba1ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config path: ./config/train.yaml\n",
      "Training with the following parameters:\n",
      "{'actor_network_layers': [128, 128, 128],\n",
      " 'agent_config_path': './config/agent.yaml',\n",
      " 'alpha': 0.2,\n",
      " 'alpha_lr': 0.0001,\n",
      " 'autotune': True,\n",
      " 'base_time': 1765457030,\n",
      " 'batch_size': 256,\n",
      " 'bootstrap': True,\n",
      " 'bootstrap_batch_proportion': 0.8,\n",
      " 'buffer_size': 120000,\n",
      " 'build_path': './unity_build/3xold_wind/UASRL.exe',\n",
      " 'config_path': './config/train.yaml',\n",
      " 'cuda': 0,\n",
      " 'env_id': '3xold',\n",
      " 'exp_name': 'old_pers',\n",
      " 'gamma': 0.995,\n",
      " 'headless': False,\n",
      " 'input_stack': 4,\n",
      " 'learning_starts': 1000,\n",
      " 'loss_log_interval': 500,\n",
      " 'machine_name': 'personal',\n",
      " 'metrics_log_interval': 10,\n",
      " 'metrics_smoothing': 0.98,\n",
      " 'n_envs': 3,\n",
      " 'noise_clip': 0.0,\n",
      " 'obstacles_config_path': './config/obstacles_simple.yaml',\n",
      " 'other_config_path': './config/other.yaml',\n",
      " 'policy_frequency': 4,\n",
      " 'policy_lr': 0.0001,\n",
      " 'q_ensemble_n': 5,\n",
      " 'q_lr': 0.0001,\n",
      " 'q_network_layers': [128, 128, 128],\n",
      " 'reward_scale': 1.0,\n",
      " 'seed': 58984,\n",
      " 'target_entropy': -1.0,\n",
      " 'target_network_update_period': 1,\n",
      " 'tau': 0.005,\n",
      " 'test_lib': False,\n",
      " 'torch_deterministic': False,\n",
      " 'total_timesteps': 100000,\n",
      " 'update_frequency': 1,\n",
      " 'wandb': True,\n",
      " 'worker_id': 0}\n",
      "agent_config:\n",
      "{'ema_range_penalty': 15,\n",
      " 'ema_smoothing': 0.011,\n",
      " 'goal_reward': 10,\n",
      " 'max_movement_speed': 1,\n",
      " 'max_step': 1000,\n",
      " 'max_turn_speed': 90,\n",
      " 'move_smooth_time': 0.1,\n",
      " 'progress_reward': 0.05,\n",
      " 'stagnation_penalty': -0.02,\n",
      " 'step_after_goal': 10,\n",
      " 'wall_hit_penalty': 0}\n",
      "obstacles_config:\n",
      "{'clear_threshold': 3,\n",
      " 'external_walls': True,\n",
      " 'fill_threshold': 6,\n",
      " 'initial_fill_percentage': 0.47,\n",
      " 'moving_obstacles_count': 0,\n",
      " 'smoothing_iterations': 3,\n",
      " 'wall_resolution': 25}\n",
      "other_config:\n",
      "{'action_size': 2,\n",
      " 'behavior_name': 'NavigationAgent',\n",
      " 'max_action': 1.0,\n",
      " 'min_action': -1.0,\n",
      " 'rays_max_observation': 1.0,\n",
      " 'rays_min_observation': -1.0,\n",
      " 'rays_per_direction': 10,\n",
      " 'state_max_observation': 128.0,\n",
      " 'state_min_observation': -128.0,\n",
      " 'state_observation_size': 8,\n",
      " 'team': '0'}\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "agent_config = parse_config_file(args.agent_config_path)\n",
    "obstacles_config = parse_config_file(args.obstacles_config_path)\n",
    "other_config = parse_config_file(args.other_config_path)\n",
    "\n",
    "args.seed = random.randint(0, 2**16)\n",
    "# args.name = generate_funny_name()\n",
    "\n",
    "print('Training with the following parameters:')\n",
    "pprint(vars(args))\n",
    "\n",
    "print('agent_config:')\n",
    "pprint(agent_config)\n",
    "\n",
    "print('obstacles_config:')\n",
    "pprint(obstacles_config)\n",
    "\n",
    "print('other_config:')\n",
    "pprint(other_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8836c969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and args.cuda >= 0:\n",
    "    # F-string per inserire l'indice: diventa \"cuda:2\"\n",
    "    device_str = f\"cuda:{args.cuda}\"\n",
    "else:\n",
    "    device_str = \"cpu\"\n",
    "\n",
    "DEVICE = torch.device(device_str)\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1cd5e",
   "metadata": {},
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dcecf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 58984\n"
     ]
    }
   ],
   "source": [
    "# seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "print(f'Seed: {args.seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c69761",
   "metadata": {},
   "source": [
    "# Start Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ce3cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Unity settings from config...\n",
      "Starting Unity Environment from build: ./unity_build/3xold_wind/UASRL.exe\n",
      "Unity Environment connected.\n"
     ]
    }
   ],
   "source": [
    "# Create the channel\n",
    "env_info = CustomChannel()\n",
    "param_channel = EnvironmentParametersChannel()\n",
    "\n",
    "print('Applying Unity settings from config...')\n",
    "apply_unity_settings(param_channel, agent_config, 'ag_')\n",
    "apply_unity_settings(param_channel, obstacles_config, 'obs_')\n",
    "\n",
    "if args.test_lib:\n",
    "    print('Testing Ended')\n",
    "    exit(0)\n",
    "\n",
    "# env setup\n",
    "print(f'Starting Unity Environment from build: {args.build_path}')\n",
    "# args.build_path\n",
    "env = UnityEnvironment(None, \n",
    "                       seed=args.seed, \n",
    "                       side_channels=[env_info, param_channel], \n",
    "                       no_graphics=args.headless,\n",
    "                       worker_id=args.worker_id)\n",
    "print('Unity Environment connected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "398130bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting environment...\n"
     ]
    }
   ],
   "source": [
    "print('Resetting environment...')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b073c",
   "metadata": {},
   "source": [
    "# Environment Variables and Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2cc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: old_pers_3987336\n",
      "Setting up wandb experiment tracking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgiacomoaru\u001b[0m (\u001b[33mgiacomo-aru\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\cicci\\Desktop\\UASRL\\wandb\\run-20260126_171926-l2gzk4gv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giacomo-aru/UASRL/runs/l2gzk4gv' target=\"_blank\">old_pers_3987336</a></strong> to <a href='https://wandb.ai/giacomo-aru/UASRL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giacomo-aru/UASRL' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giacomo-aru/UASRL/runs/l2gzk4gv' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL/runs/l2gzk4gv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = f\"{args.exp_name}_{int(time.time()) - args.base_time}\"\n",
    "args.run_name = run_name\n",
    "print(f\"Run name: {run_name}\")\n",
    "\n",
    "# wandb to track experiments\n",
    "# Start a new wandb run to track this script.\n",
    "if args.wandb:\n",
    "    print('Setting up wandb experiment tracking.')\n",
    "    wandb_run = wandb.init(\n",
    "        # Set the wandb entity where your project will be logged (generally your team name).\n",
    "        entity=\"giacomo-aru\",\n",
    "        # Set the wandb project where this run will be logged.\n",
    "        project=\"UASRL\",\n",
    "        # force the \n",
    "        name=args.run_name,\n",
    "        # Track hyperparameters and run metadata.\n",
    "        config={\n",
    "            \"training\": vars(args),\n",
    "            \"agent\": agent_config,\n",
    "            \"obstacles\": obstacles_config,\n",
    "            \"other\": other_config\n",
    "        }\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b43995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEHAVIOUR_NAME = other_config['behavior_name'] + '?team=' + other_config['team']\n",
    "\n",
    "RAY_PER_DIRECTION = other_config['rays_per_direction']\n",
    "RAYCAST_MIN = other_config['rays_min_observation']\n",
    "RAYCAST_MAX = other_config['rays_max_observation']\n",
    "RAYCAST_SIZE = 2*RAY_PER_DIRECTION + 1\n",
    "\n",
    "STATE_SIZE = other_config['state_observation_size'] - 1\n",
    "STATE_MIN = other_config['state_min_observation']\n",
    "STATE_MAX = other_config['state_max_observation']\n",
    "\n",
    "ACTION_SIZE = other_config['action_size']\n",
    "ACTION_MIN = other_config['min_action']\n",
    "ACTION_MAX = other_config['max_action']\n",
    "\n",
    "TOTAL_STATE_SIZE = (STATE_SIZE + RAYCAST_SIZE)*args.input_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4736a2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating actor and critic networks...\n"
     ]
    }
   ],
   "source": [
    "# creating the training networks\n",
    "print('Creating actor and critic networks...')\n",
    "actor = OldDenseActor(TOTAL_STATE_SIZE, ACTION_SIZE, ACTION_MIN, ACTION_MAX, args.actor_network_layers).to(DEVICE)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)\n",
    "\n",
    "qf_ensemble = [OldDenseSoftQNetwork(TOTAL_STATE_SIZE, ACTION_SIZE, args.q_network_layers).to(DEVICE) for _ in range(args.q_ensemble_n)]\n",
    "qf_ensemble_target = [OldDenseSoftQNetwork(TOTAL_STATE_SIZE, ACTION_SIZE, args.q_network_layers).to(DEVICE) for _ in range(args.q_ensemble_n)]\n",
    "for q_t, q in zip(qf_ensemble_target, qf_ensemble):\n",
    "    q_t.load_state_dict(q.state_dict())\n",
    "\n",
    "par = []\n",
    "for q in qf_ensemble:\n",
    "    par += list(q.parameters())\n",
    "qf_optimizer = torch.optim.Adam(\n",
    "    par,\n",
    "    lr=args.q_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754acdf",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31990774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up replay buffer...\n"
     ]
    }
   ],
   "source": [
    "print('Setting up replay buffer...')\n",
    "observation_space = spaces.Box(\n",
    "    low=min(RAYCAST_MIN, STATE_MIN), \n",
    "    high=max(RAYCAST_MAX, STATE_MAX), \n",
    "    shape=(TOTAL_STATE_SIZE,), \n",
    "    dtype=np.float32\n",
    ")\n",
    "action_space = spaces.Box(\n",
    "    low=ACTION_MIN, \n",
    "    high=ACTION_MAX, \n",
    "    shape=(ACTION_SIZE,), \n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    buffer_size=args.buffer_size,\n",
    "    observation_space=observation_space,\n",
    "    action_space=action_space,\n",
    "    device=DEVICE,                \n",
    "    handle_timeout_termination=True,\n",
    "    n_envs=1 # necessario data la natura asincrona del'env   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a11d9",
   "metadata": {},
   "source": [
    "# start algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62f83fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autotune target_entropy: -1.0\n"
     ]
    }
   ],
   "source": [
    "# Automatic entropy tuning\n",
    "if args.autotune:\n",
    "    target_entropy = args.target_entropy\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=DEVICE)\n",
    "    alpha = log_alpha.exp().item()\n",
    "    a_optimizer = optim.Adam([log_alpha], lr=args.alpha_lr)\n",
    "    print(f'autotune target_entropy: {target_entropy}')\n",
    "else:\n",
    "    alpha = args.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8bde39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to path: ./models/old_pers_3987336\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "save_path = './models/' + run_name\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "print('saving to path:', save_path)\n",
    "\n",
    "training_stats = {\n",
    "    \"time/python_time\": RunningMean(),\n",
    "    \"time/unity_time\": RunningMean(),\n",
    "    \n",
    "    \"stats/action_saturation\": RunningMean(),\n",
    "    'stats/qf_mean': RunningMean(),\n",
    "    'stats/qf_std':RunningMean(),\n",
    "    'stats/actor_entropy': RunningMean(),\n",
    "    'stats/alpha': RunningMean(),\n",
    "    'stats/uncertainty': RunningMean(),\n",
    "    \n",
    "    'loss/critic_ens': RunningMean(),\n",
    "    'loss/actor': RunningMean(),\n",
    "    'loss/alpha': RunningMean(),\n",
    "}\n",
    "\n",
    "best_reward = -float('inf')\n",
    "\n",
    "episodic_stats = {}\n",
    "success_stats = {}\n",
    "failure_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9d6ed7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100000] Starting Training - run name: old_pers_3987336\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "unity_end_time = -1\n",
    "unity_start_time = -1\n",
    "\n",
    "global_step = 0\n",
    "print(f'[{global_step}/{args.total_timesteps}] Starting Training - run name: {run_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5cc390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Learning\n",
      "[1199/100000] Models saved, suffix: _best\n",
      "[1399/100000] Models saved, suffix: _best\n",
      "[1500/100000] Logged training stats to wandb\n",
      "[1599/100000] Models saved, suffix: _best\n",
      "[1799/100000] |success: 0.00000|reward: -19.39581|collisions: 3.27195|length: 999.00000|SPL: 0.00000| SPS: 9\n",
      "[1799/100000] Logged episodic stats to wandb\n",
      "[1799/100000] Models saved, suffix: _best\n",
      "[1999/100000] Models saved, suffix: _best\n",
      "[2000/100000] Logged training stats to wandb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cicci\\AppData\\Local\\Temp\\ipykernel_13896\\438019796.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  action, _, _ = actor.get_action(torch.Tensor([obs[id][0]]).to(DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2399/100000] |success: 0.00000|reward: -19.27406|collisions: 3.75876|length: 999.00000|SPL: 0.00000| SPS: 9\n",
      "[2399/100000] Logged episodic stats to wandb\n",
      "[2399/100000] Models saved, suffix: _best\n",
      "[2500/100000] Logged training stats to wandb\n",
      "[2999/100000] |success: 0.00000|reward: -19.21135|collisions: 3.85709|length: 999.00000|SPL: 0.00000| SPS: 9\n",
      "[2999/100000] Logged episodic stats to wandb\n",
      "[2999/100000] Models saved, suffix: _best\n",
      "[3000/100000] Logged training stats to wandb\n",
      "[3399/100000] Models saved, suffix: _best\n",
      "[3500/100000] Logged training stats to wandb\n",
      "[3599/100000] Models saved, suffix: _best\n",
      "[3799/100000] |success: 0.00000|reward: -18.68291|collisions: 3.90895|length: 999.00000|SPL: 0.00000| SPS: 9\n",
      "[3799/100000] Logged episodic stats to wandb\n",
      "[3999/100000] Models saved, suffix: _best\n",
      "[4000/100000] Logged training stats to wandb\n",
      "[4399/100000] |success: 0.00000|reward: -18.49331|collisions: 4.02248|length: 999.00000|SPL: 0.00000| SPS: 9\n",
      "[4399/100000] Logged episodic stats to wandb\n",
      "[4399/100000] Models saved, suffix: _best\n",
      "[4500/100000] Logged training stats to wandb\n",
      "[4644/100000] Models saved, suffix: _best\n",
      "[4800/100000] Models saved, suffix: _best\n",
      "[4844/100000] Models saved, suffix: _best\n",
      "[5000/100000] Logged training stats to wandb\n",
      "[5001/100000] |success: 0.01808|reward: -17.99711|collisions: 4.28518|length: 984.93499|SPL: 0.01808| SPS: 9\n",
      "[5001/100000] Logged episodic stats to wandb\n",
      "[5246/100000] Models saved, suffix: _best\n",
      "[5500/100000] Logged training stats to wandb\n",
      "[5615/100000] |success: 0.03477|reward: -17.32383|collisions: 4.44347|length: 968.58786|SPL: 0.03477| SPS: 9\n",
      "[5615/100000] Logged episodic stats to wandb\n",
      "[5615/100000] Models saved, suffix: _best\n",
      "[6000/100000] Logged training stats to wandb\n",
      "[6255/100000] |success: 0.02841|reward: -17.63725|collisions: 4.21933|length: 974.15107|SPL: 0.02841| SPS: 9\n",
      "[6255/100000] Logged episodic stats to wandb\n",
      "[6500/100000] Logged training stats to wandb\n",
      "[7000/100000] Logged training stats to wandb\n",
      "[7018/100000] |success: 0.02321|reward: -17.77830|collisions: 4.31418|length: 978.69662|SPL: 0.02321| SPS: 10\n",
      "[7018/100000] Logged episodic stats to wandb\n",
      "[7500/100000] Logged training stats to wandb\n",
      "[7635/100000] |success: 0.01897|reward: -17.68027|collisions: 4.35199|length: 982.41066|SPL: 0.01897| SPS: 10\n",
      "[7635/100000] Logged episodic stats to wandb\n",
      "[8000/100000] Logged training stats to wandb\n",
      "[8275/100000] |success: 0.01550|reward: -17.86672|collisions: 4.28190|length: 985.44530|SPL: 0.01550| SPS: 10\n",
      "[8275/100000] Logged episodic stats to wandb\n",
      "[8500/100000] Logged training stats to wandb\n",
      "[9000/100000] Logged training stats to wandb\n",
      "[9038/100000] |success: 0.01266|reward: -17.88084|collisions: 4.12405|length: 987.92482|SPL: 0.01266| SPS: 10\n",
      "[9038/100000] Logged episodic stats to wandb\n",
      "[9500/100000] Logged training stats to wandb\n",
      "[9655/100000] |success: 0.01035|reward: -17.93221|collisions: 4.66601|length: 989.95077|SPL: 0.01035| SPS: 10\n",
      "[9655/100000] Logged episodic stats to wandb\n",
      "[10000/100000] Logged training stats to wandb\n",
      "[10295/100000] |success: 0.00845|reward: -18.03926|collisions: 4.53693|length: 991.60612|SPL: 0.00845| SPS: 10\n",
      "[10295/100000] Logged episodic stats to wandb\n",
      "[10500/100000] Logged training stats to wandb\n",
      "[11000/100000] Logged training stats to wandb\n",
      "[11058/100000] |success: 0.00691|reward: -18.20778|collisions: 5.27025|length: 992.95866|SPL: 0.00691| SPS: 10\n",
      "[11058/100000] Logged episodic stats to wandb\n",
      "[11500/100000] Logged training stats to wandb\n",
      "[11675/100000] |success: 0.00564|reward: -17.94588|collisions: 5.07910|length: 994.06379|SPL: 0.00564| SPS: 10\n",
      "[11675/100000] Logged episodic stats to wandb\n",
      "[12000/100000] Logged training stats to wandb\n",
      "[12315/100000] |success: 0.00461|reward: -17.90535|collisions: 4.79453|length: 994.96676|SPL: 0.00461| SPS: 10\n",
      "[12315/100000] Logged episodic stats to wandb\n",
      "[12500/100000] Logged training stats to wandb\n",
      "[13000/100000] Logged training stats to wandb\n",
      "[13078/100000] |success: 0.00377|reward: -17.81986|collisions: 4.68682|length: 995.70455|SPL: 0.00377| SPS: 10\n",
      "[13078/100000] Logged episodic stats to wandb\n",
      "[13500/100000] Logged training stats to wandb\n",
      "[13695/100000] |success: 0.00308|reward: -17.86301|collisions: 4.64284|length: 996.30737|SPL: 0.00308| SPS: 10\n",
      "[13695/100000] Logged episodic stats to wandb\n",
      "[14000/100000] Logged training stats to wandb\n",
      "[14335/100000] |success: 0.00252|reward: -17.95643|collisions: 4.62939|length: 996.79993|SPL: 0.00252| SPS: 10\n",
      "[14335/100000] Logged episodic stats to wandb\n",
      "[14500/100000] Logged training stats to wandb\n",
      "[15000/100000] Logged training stats to wandb\n",
      "[15098/100000] |success: 0.00206|reward: -17.73655|collisions: 4.95495|length: 997.20238|SPL: 0.00206| SPS: 10\n",
      "[15098/100000] Logged episodic stats to wandb\n",
      "[15500/100000] Logged training stats to wandb\n",
      "[15715/100000] |success: 0.00168|reward: -17.72537|collisions: 4.91880|length: 997.53121|SPL: 0.00168| SPS: 10\n",
      "[15715/100000] Logged episodic stats to wandb\n",
      "[16000/100000] Logged training stats to wandb\n",
      "[16355/100000] |success: 0.00137|reward: -17.45005|collisions: 4.58799|length: 997.79990|SPL: 0.00137| SPS: 10\n",
      "[16355/100000] Logged episodic stats to wandb\n",
      "[16500/100000] Logged training stats to wandb\n",
      "[16982/100000] An error occurred: The Unity environment took too long to respond. Make sure that :\n",
      "\t The environment does not need user interaction to launch\n",
      "\t The Agents' Behavior Parameters > Behavior Type is set to \"Default\"\n",
      "\t The environment and the Python interface have compatible versions.\n",
      "\t If you're running on a headless server without graphics support, turn off display by either passing --no-graphics option or build your Unity executable as server build.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cicci\\AppData\\Local\\Temp\\ipykernel_13896\\438019796.py\", line 43, in <module>\n",
      "    env.step()\n",
      "  File \"c:\\Users\\cicci\\Desktop\\UASRL\\.venv\\lib\\site-packages\\mlagents_envs\\timers.py\", line 305, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\cicci\\Desktop\\UASRL\\.venv\\lib\\site-packages\\mlagents_envs\\environment.py\", line 348, in step\n",
      "    outputs = self._communicator.exchange(step_input, self._poll_process)\n",
      "  File \"c:\\Users\\cicci\\Desktop\\UASRL\\.venv\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\", line 142, in exchange\n",
      "    self.poll_for_timeout(poll_callback)\n",
      "  File \"c:\\Users\\cicci\\Desktop\\UASRL\\.venv\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\", line 114, in poll_for_timeout\n",
      "    raise UnityTimeOutException(\n",
      "mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :\n",
      "\t The environment does not need user interaction to launch\n",
      "\t The Agents' Behavior Parameters > Behavior Type is set to \"Default\"\n",
      "\t The environment and the Python interface have compatible versions.\n",
      "\t If you're running on a headless server without graphics support, turn off display by either passing --no-graphics option or build your Unity executable as server build.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    obs = collect_data_after_step(env, BEHAVIOUR_NAME, STATE_SIZE)\n",
    "    \n",
    "    \n",
    "    while global_step < args.total_timesteps:\n",
    "\n",
    "        # actions for each agent in the environment\n",
    "        # dim = (naagents, action_space)\n",
    "        for id in obs:\n",
    "            agent_obs = obs[id]\n",
    "            \n",
    "            # terminated agents are not considered\n",
    "            if agent_obs[3]:\n",
    "                continue\n",
    "            \n",
    "            # algo logic\n",
    "            if global_step < args.learning_starts * 2:\n",
    "                # change this to use the handcrafted starting policy or a previously trained policy\n",
    "                \n",
    "                action = get_initial_action(id)\n",
    "                # action, _, _ = old_actor.get_action(torch.Tensor([obs[id][0]]), \n",
    "                #                                 torch.Tensor([obs[id][1]]),\n",
    "                #                                 0.5)\n",
    "                # action = action[0].detach().numpy()\n",
    "            else:\n",
    "                # training policy\n",
    "                action, _, _ = actor.get_action(torch.Tensor([obs[id][0]]).to(DEVICE))\n",
    "                action = action[0].detach().cpu().numpy()\n",
    "            \n",
    "            # memorize the action taken for the next step\n",
    "            agent_obs[2] = action\n",
    "            \n",
    "            # the first dimention of the action is the \"number of agent\"\n",
    "            # Always 1 if \"set_action_for_agent\" is used\n",
    "            a = ActionTuple(continuous=np.array([action]))\n",
    "            env.set_action_for_agent(BEHAVIOUR_NAME, id, a)\n",
    "        \n",
    "        # --- ENVIRONMENT STEP ---\n",
    "        unity_start_time = time.time()\n",
    "        if unity_end_time > 0 and global_step > args.learning_starts:\n",
    "            training_stats['time/python_time'].update(unity_start_time - unity_end_time)\n",
    "        \n",
    "        env.step()\n",
    "        unity_end_time = time.time()\n",
    "        if global_step > args.learning_starts:\n",
    "            training_stats['time/unity_time'].update(unity_end_time - unity_start_time)\n",
    "\n",
    "        next_obs = collect_data_after_step(env, BEHAVIOUR_NAME, STATE_SIZE)\n",
    "        \n",
    "        while env_info.stop_msg_queue:\n",
    "                msg = env_info.stop_msg_queue.pop()\n",
    "                \n",
    "                if global_step >= args.learning_starts:\n",
    "                    update_stats_from_message(episodic_stats, success_stats, failure_stats, msg, args.metrics_smoothing)        \n",
    "                    if episodic_stats['ep_count'] % args.metrics_log_interval == 0:\n",
    "                        print_update(global_step, args.total_timesteps, start_time, episodic_stats)\n",
    "                        if args.wandb:\n",
    "                            log_stats_to_wandb(wandb_run, \n",
    "                                            [episodic_stats, success_stats, failure_stats],\n",
    "                                            ['all_ep', 'success_ep', 'failure_ep'],\n",
    "                                            global_step)\n",
    "                            print(f\"[{global_step}/{args.total_timesteps}] Logged episodic stats to wandb\")\n",
    "                        \n",
    "        # save data to reply buffer; handle `terminal_observation`\n",
    "        for id in obs:\n",
    "            prev_agent_obs = obs[id]\n",
    "            # consider every agent that in the previous step was not terminated\n",
    "            # in this way are excluded the agents that are already considered before and don't have a \n",
    "            # couple prev_obs - next_obs and a reward\n",
    "            if prev_agent_obs[3] or id not in next_obs:\n",
    "                continue\n",
    "                \n",
    "            next_agent_obs = next_obs[id]\n",
    "            \n",
    "            # add the data to the replay buffer\n",
    "            rb.add(obs = prev_agent_obs[0], \n",
    "                next_obs = next_agent_obs[0],\n",
    "                action = np.array(prev_agent_obs[2]), \n",
    "                reward = next_agent_obs[1], \n",
    "                done = next_agent_obs[3],\n",
    "                infos = [{}])\n",
    "            \n",
    "        # crucial step, easy to overlook, update the previous observation\n",
    "        obs = next_obs\n",
    "        \n",
    "        # Save best models based on reward\n",
    "        if episodic_stats != {} and episodic_stats[\"reward\"] > best_reward:\n",
    "            best_reward = episodic_stats[\"reward\"]\n",
    "            save_models(actor, qf_ensemble, qf_ensemble_target, save_path, suffix=f'_best')\n",
    "            print(f\"[{global_step}/{args.total_timesteps}] Models saved, suffix: _best\")\n",
    "                \n",
    "        # Training loop\n",
    "        for _ in range(args.update_frequency):\n",
    "\n",
    "            # Start learning after a warm-up phase\n",
    "            if global_step > args.learning_starts:\n",
    "\n",
    "                # Sample a batch from replay buffer\n",
    "                data = rb.sample(args.batch_size)\n",
    "\n",
    "                # --- CALCOLO SATURAZIONE ---\n",
    "                saturation = data.actions.detach().cpu().numpy()\n",
    "                saturation = (np.abs(saturation) > 0.99).mean()\n",
    "                training_stats[\"stats/action_saturation\"].update(saturation)\n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    # Compute target action with exploration noise\n",
    "                    next_action, next_log_pi, _ = actor.get_action(\n",
    "                        data.next_observations\n",
    "                    )\n",
    "\n",
    "                    if args.noise_clip > 0:\n",
    "                        noise = torch.randn_like(next_action) * args.noise_clip\n",
    "                        next_action = torch.clamp(next_action + noise, -1, 1)\n",
    "\n",
    "                    # Compute target Q-value (min over ensemble)\n",
    "                    target_q_values = []\n",
    "                    for q_target in qf_ensemble_target:\n",
    "                        q_val = q_target(\n",
    "                            data.next_observations, \n",
    "                            next_action\n",
    "                        )\n",
    "                        target_q_values.append(q_val)\n",
    "                    stacked_target_q = torch.stack(target_q_values)\n",
    "                    min_qf_next_target = stacked_target_q.min(dim=0).values - alpha * next_log_pi\n",
    "                    next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * min_qf_next_target.view(-1)\n",
    "\n",
    "                # Q-function updates (with bootstrapping)\n",
    "                q_losses = []\n",
    "                q_vals = []\n",
    "                batch_size = int(data.actions.shape[0] * args.bootstrap_batch_proportion)\n",
    "                for q in qf_ensemble:\n",
    "                    # Bootstrap indices\n",
    "                    indices = torch.randint(0, batch_size, (batch_size,), device=data.actions.device)\n",
    "                    \n",
    "                    observation = data.observations[indices]\n",
    "                    actions = data.actions[indices]\n",
    "                    target = next_q_value[indices]\n",
    "\n",
    "                    # Compute Q loss\n",
    "                    q_val = q(observation, actions).view(-1)\n",
    "                    loss = F.mse_loss(q_val, target)\n",
    "                    q_losses.append(loss)\n",
    "                    q_vals.append(q_val)\n",
    "                    \n",
    "                total_q_loss = torch.stack(q_losses).mean()\n",
    "                qf_optimizer.zero_grad()\n",
    "                total_q_loss.backward()\n",
    "                qf_optimizer.step()\n",
    "                \n",
    "                # Track Q-value statistics\n",
    "                all_q_values = torch.cat(q_vals)\n",
    "                training_stats['stats/qf_mean'].update(all_q_values.mean().item())\n",
    "                training_stats['stats/qf_std'].update(all_q_values.std().item())\n",
    "                training_stats['loss/critic_ens'].update(total_q_loss.item())\n",
    "                \n",
    "                # Delayed policy (actor) update\n",
    "                if global_step % args.policy_frequency == 0:\n",
    "                    for _ in range(args.policy_frequency):\n",
    "                        pi, log_pi, _ = actor.get_action(data.observations)\n",
    "                        actor_entropy = - (log_pi.exp() * log_pi).sum(dim=-1).mean()\n",
    "\n",
    "                        q_pi_vals = [q(data.observations, pi) for q in qf_ensemble]\n",
    "                        min_qf_pi = torch.min(torch.stack(q_pi_vals), dim=0).values.view(-1)\n",
    "\n",
    "                        actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "                        actor_optimizer.zero_grad()\n",
    "                        actor_loss.backward()\n",
    "                        actor_optimizer.step()\n",
    "\n",
    "                        # 1. Calcolo Incertezza (Disaccordo tra i critici)\n",
    "                        q_pi_stack = torch.stack(q_pi_vals) \n",
    "                        with torch.no_grad():\n",
    "                            uncertainty = q_pi_stack.std(dim=0).mean().item()\n",
    "                        training_stats['stats/uncertainty'].update(uncertainty)\n",
    "\n",
    "                        # 2. Log Entropia e Loss Attore\n",
    "                        training_stats['stats/actor_entropy'].update(-log_pi.mean().item())\n",
    "                        training_stats['loss/actor'].update(actor_loss.item())\n",
    "                        \n",
    "                        # Automatic entropy tuning (if enabled)\n",
    "                        if args.autotune:\n",
    "                            with torch.no_grad():\n",
    "                                _, log_pi, _ = actor.get_action(data.observations)\n",
    "                            alpha_loss = (-log_alpha * (log_pi + target_entropy)).mean()\n",
    "\n",
    "                            a_optimizer.zero_grad()\n",
    "                            alpha_loss.backward()\n",
    "                            a_optimizer.step()\n",
    "                            alpha = log_alpha.exp().item()\n",
    "                            \n",
    "                            training_stats['loss/alpha'].update(alpha_loss.item())\n",
    "\n",
    "                        training_stats['stats/alpha'].update(alpha)\n",
    "                        \n",
    "                # Soft update target Q-networks\n",
    "                if global_step % args.target_network_update_period == 0:\n",
    "                    for q, q_t in zip(qf_ensemble, qf_ensemble_target):\n",
    "                        for param, target_param in zip(q.parameters(), q_t.parameters()):\n",
    "                            target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "                # --- 5. LOGGING LOSS (METODO SNAPSHOT/ISTANTANEO) ---\n",
    "                if global_step % args.loss_log_interval == 0:\n",
    "\n",
    "                    # COSTRUZIONE DIZIONARIO SNAPSHOT\n",
    "                    training_stats_divided = {}\n",
    "                    for key in training_stats:\n",
    "                        splitted = key.split('/')\n",
    "                        if splitted[0] not in training_stats_divided:\n",
    "                            training_stats_divided[splitted[0]] = {}\n",
    "                        training_stats_divided[splitted[0]][splitted[1]] = training_stats[key].mean\n",
    "                        \n",
    "                        # reset\n",
    "                        training_stats[key].reset()\n",
    "                        \n",
    "                    current_time = time.time()\n",
    "                    training_stats_divided['time']['SPS'] = global_step / (current_time - start_time + 1e-6)\n",
    "                    \n",
    "                    # log stats su wandb\n",
    "                    if args.wandb:\n",
    "                        log_stats_to_wandb(wandb_run, list(training_stats_divided.values()), list(training_stats_divided.keys()), global_step)\n",
    "                        print(f\"[{global_step}/{args.total_timesteps}] Logged training stats to wandb\")  \n",
    "                            \n",
    "            elif global_step == args.learning_starts:\n",
    "                print(\"Start Learning\")\n",
    "\n",
    "            # Step counter\n",
    "            global_step += 1\n",
    "            \n",
    "except Exception as e:  \n",
    "    print(f\"[{global_step}/{args.total_timesteps}] An error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c8b7d",
   "metadata": {},
   "source": [
    "# Close Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb5d4909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing environment\n",
      "Closing wandb run\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>all_ep/SPL</td><td>▁▁▁▁▁▅█▇▆▅▄▄▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>all_ep/average_speed</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>all_ep/collisions</td><td>▁▃▃▃▄▅▅▄▅▅▅▄▆▅█▇▆▆▆▆▇▇▆</td></tr><tr><td>all_ep/distance_traveled</td><td>▁▁▂▄▄▄▅▅▆▆▆▆▆▆▆▇▇▇██▇██</td></tr><tr><td>all_ep/ep_count</td><td>▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>all_ep/global_avg_dispersion</td><td>▃▅▇▇▅▂▁▁▃▂▅▄▆█▇▆▅▆▃▃▂▃▁</td></tr><tr><td>all_ep/global_avg_dist_obstacle</td><td>███▆▅▄▄▄▄▃▃▃▄▅▄▄▃▃▂▂▂▂▁</td></tr><tr><td>all_ep/global_avg_visibility</td><td>▁▃▆▅▄▂▂▃▄▂▅▄▆█▇▇▅▅▄▃▃▄▃</td></tr><tr><td>all_ep/global_characteristic_dimension</td><td>▁▃▅▅▅▄▅▅▆▅▆▅▇██▇▆▆▅▅▅▅▅</td></tr><tr><td>all_ep/length</td><td>█████▅▁▂▃▄▅▅▆▆▇▇▇▇▇▇███</td></tr><tr><td>+47</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>all_ep/SPL</td><td>0.00137</td></tr><tr><td>all_ep/average_speed</td><td>0</td></tr><tr><td>all_ep/collisions</td><td>4.58799</td></tr><tr><td>all_ep/distance_traveled</td><td>2.73625</td></tr><tr><td>all_ep/ep_count</td><td>230</td></tr><tr><td>all_ep/global_avg_dispersion</td><td>7.3958</td></tr><tr><td>all_ep/global_avg_dist_obstacle</td><td>3.26906</td></tr><tr><td>all_ep/global_avg_visibility</td><td>10.15503</td></tr><tr><td>all_ep/global_characteristic_dimension</td><td>10.28331</td></tr><tr><td>all_ep/length</td><td>997.7999</td></tr><tr><td>+47</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">old_pers_3987336</strong> at: <a href='https://wandb.ai/giacomo-aru/UASRL/runs/l2gzk4gv' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL/runs/l2gzk4gv</a><br> View project at: <a href='https://wandb.ai/giacomo-aru/UASRL' target=\"_blank\">https://wandb.ai/giacomo-aru/UASRL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260126_171926-l2gzk4gv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Closing environment\")\n",
    "env.close()\n",
    "\n",
    "print(\"Closing wandb run\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a243c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16982/100000] Models saved, suffix: _final\n"
     ]
    }
   ],
   "source": [
    "# save trained networks, actor and critics\n",
    "save_models(actor, qf_ensemble, qf_ensemble_target, save_path, suffix='_final')\n",
    "print(f\"[{global_step}/{args.total_timesteps}] Models saved, suffix: _final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
