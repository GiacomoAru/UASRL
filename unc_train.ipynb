{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5539a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Librerie Standard e Utilità ---\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Machine Learning e Processamento Dati ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Ottimizzazione e Monitoraggio ---\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# --- Moduli Personalizzati ---\n",
    "from training_utils import *\n",
    "from testing_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d444b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "def parse_args(default_config_path=\"./config/uncertainty_debug.yaml\"):\n",
    "    \"\"\"\n",
    "    Parse arguments from CLI or notebook.\n",
    "    - In notebook: usa il default se non passato\n",
    "    - In CLI: permette override dei parametri nel config\n",
    "    \"\"\"\n",
    "    # --- Gestione notebook: evita crash su ipykernel args ---\n",
    "    argv = sys.argv[1:]\n",
    "    # Se siamo in notebook o non è passato il config_path, inseriamo il default\n",
    "    if len(argv) == 0 or \"--f=\" in \" \".join(argv):\n",
    "        argv = [default_config_path]\n",
    "\n",
    "    # --- Pre-parser per leggere il config_path ---\n",
    "    pre_parser = argparse.ArgumentParser(add_help=False)\n",
    "    pre_parser.add_argument(\n",
    "        \"config_path\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=default_config_path,\n",
    "        help=\"Main config file path\"\n",
    "    )\n",
    "    initial_args, remaining_argv = pre_parser.parse_known_args(argv)\n",
    "    CONFIG_PATH = initial_args.config_path\n",
    "    print(f\"Config path: {CONFIG_PATH}\")\n",
    "\n",
    "    # --- Legge parametri dal file di config ---\n",
    "    file_config_dict = parse_config_file(CONFIG_PATH)\n",
    "\n",
    "    # --- Parser principale ---\n",
    "    parser = argparse.ArgumentParser(description=\"Training Script\")\n",
    "    parser.add_argument(\n",
    "        \"config_path\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=CONFIG_PATH,\n",
    "        help=\"Main config file path\"\n",
    "    )\n",
    "\n",
    "    # Aggiunge parametri dal config file, con tipi corretti\n",
    "    for key, value in file_config_dict.items():\n",
    "        if isinstance(value, bool):\n",
    "            parser.add_argument(f\"--{key}\", type=str2bool, default=value)\n",
    "        elif value is None:\n",
    "            parser.add_argument(f\"--{key}\", type=str, default=value)\n",
    "        else:\n",
    "            parser.add_argument(f\"--{key}\", type=type(value), default=value)\n",
    "\n",
    "    # --- Parse finale con remaining_argv per ignorare args extra Jupyter ---\n",
    "    args, unknown = parser.parse_known_args(remaining_argv)\n",
    "    if unknown:\n",
    "        print(\"Ignored unknown args:\", unknown)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f21f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. IL MODELLO ---\n",
    "class ProbabilisticNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu_head = nn.Linear(hidden_size, output_dim)\n",
    "        self.logvar_head = nn.Linear(hidden_size, output_dim)\n",
    "        \n",
    "        # Limiti per stabilità numerica (Softplus)\n",
    "        self.max_logvar = nn.Parameter(torch.ones(1, output_dim) / 2.0)\n",
    "        self.min_logvar = nn.Parameter(-torch.ones(1, output_dim) * 10.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.network(x)\n",
    "        mu = self.mu_head(features)\n",
    "        logvar = self.logvar_head(features)\n",
    "        \n",
    "        # Clamping morbido\n",
    "        logvar = self.max_logvar - nn.functional.softplus(self.max_logvar - logvar)\n",
    "        logvar = self.min_logvar + nn.functional.softplus(logvar - self.min_logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "# --- 2. EARLY STOPPING ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, save_path=None):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.Inf\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        if self.save_path:\n",
    "            # os.path.dirname estrae la cartella dal path completo (es: \"models/test.pth\" -> \"models\")\n",
    "            dir_name = os.path.dirname(self.save_path)\n",
    "            \n",
    "            # Creiamo la cartella solo se dir_name non è vuoto\n",
    "            if dir_name and not os.path.exists(dir_name):\n",
    "                print(f\" Creazione cartella: {dir_name}\")\n",
    "                os.makedirs(dir_name, exist_ok=True) # exist_ok evita errori se la cartella appare nel mentre\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.save_path:\n",
    "                torch.save(model.state_dict(), self.save_path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ade4920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(raw_data, \n",
    "                        actor_model, \n",
    "                        \n",
    "                        RAYCASY_SIZE,\n",
    "                        INPUT_STACK,\n",
    "                        STATE_SIZE, \n",
    "                        DEVICE,\n",
    "                        \n",
    "                        shuffle=True):\n",
    "    print(\">>> Caricamento e Processamento Dati (Split per Episodi)...\")\n",
    "    \n",
    "    # 1. SPLIT DEGLI EPISODI (PRIMA DI TUTTO)\n",
    "    # Copiamo raw_data per non modificare la lista originale fuori dalla funzione\n",
    "    all_episodes = list(raw_data) \n",
    "    \n",
    "    if shuffle:\n",
    "        print(\"Shuffling degli episodi...\")\n",
    "        random.shuffle(all_episodes)\n",
    "\n",
    "    total_episodes = len(all_episodes)\n",
    "    n_train = int(total_episodes * 0.8) # 80%\n",
    "    n_val = int(total_episodes * 0.1)   # 10%\n",
    "    # Il restante 10% va al test\n",
    "\n",
    "    train_episodes = all_episodes[:n_train]\n",
    "    val_episodes = all_episodes[n_train : n_train + n_val]\n",
    "    test_episodes = all_episodes[n_train + n_val:]\n",
    "\n",
    "    print(f\"Split Episodi -> Train: {len(train_episodes)}, Val: {len(val_episodes)}, Test: {len(test_episodes)}\")\n",
    "\n",
    "    # Assicuriamoci che l'actor sia in eval\n",
    "    actor_model.eval()\n",
    "\n",
    "    # --- FUNZIONE INTERNA PER PROCESSARE UNA LISTA DI EPISODI ---\n",
    "    def process_dataset_subset(episodes_subset, subset_name):\n",
    "        if not episodes_subset:\n",
    "            print(f\"Warning: {subset_name} set is empty!\")\n",
    "            return torch.tensor([]), torch.tensor([])\n",
    "\n",
    "        inputs_list = []\n",
    "        outputs_list = []\n",
    "        \n",
    "        print(f\"Processing {subset_name} ({len(episodes_subset)} episodes)...\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for all_observations in episodes_subset:\n",
    "\n",
    "                # Saltiamo episodi troppo corti se necessario, o gestiamo l'errore\n",
    "                if len(all_observations) < 2:\n",
    "                    continue\n",
    "\n",
    "                for t in range(len(all_observations) - 1):\n",
    "                    # --- A. Recupera Input Corrente ---\n",
    "                    actual_obs = all_observations[t][:-2]\n",
    "                    \n",
    "                    # --- B. Calcola Next Observation (Logica Custom) ---\n",
    "                    # Nota: Qui assumiamo che la struttura di episode[0] supporti questo slicing\n",
    "                    next_obs = all_observations[t + 1][(INPUT_STACK - 1)*RAYCASY_SIZE: (INPUT_STACK)*RAYCASY_SIZE] + all_observations[t + 1][-STATE_SIZE - 2: - 2]\n",
    "                    \n",
    "                    # --- C. Processamento Actor ---\n",
    "                    obs_tensor = torch.FloatTensor(actual_obs).to(DEVICE)\n",
    "                    \n",
    "                    actor_distrib = actor_model(obs_tensor)\n",
    "                    actor_distrib = torch.cat(actor_distrib).detach().cpu()\n",
    "                    \n",
    "                    # --- D. Creazione Input Finale ---\n",
    "                    # Riportiamo obs su CPU per unirlo\n",
    "                    input_tensor = torch.cat([obs_tensor.cpu(), actor_distrib])\n",
    "                    output_tensor = torch.FloatTensor(next_obs)\n",
    "                    \n",
    "                    inputs_list.append(input_tensor)\n",
    "                    outputs_list.append(output_tensor)\n",
    "        \n",
    "        # Stacking finale per questo subset\n",
    "        if len(inputs_list) > 0:\n",
    "            X = torch.stack(inputs_list).float()\n",
    "            y = torch.stack(outputs_list).float()\n",
    "            return X, y\n",
    "        else:\n",
    "            return torch.tensor([]), torch.tensor([])\n",
    "\n",
    "    # 2. ESEGUIAMO IL PROCESSAMENTO SUI 3 GRUPPI SEPARATI\n",
    "    X_train, y_train = process_dataset_subset(train_episodes, \"Train\")\n",
    "    X_val, y_val = process_dataset_subset(val_episodes, \"Validation\")\n",
    "    X_test, y_test = process_dataset_subset(test_episodes, \"Test\")\n",
    "\n",
    "    input_dim = X_train.shape[1] if len(X_train) > 0 else 0\n",
    "    output_dim = y_train.shape[1] if len(y_train) > 0 else 0\n",
    "    \n",
    "    print(f\"Final Dataset Shapes:\")\n",
    "    print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Val:   X={X_val.shape}, y={y_val.shape}\")\n",
    "    print(f\"Test:  X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), input_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ef287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. OPTIMIZATION LOOP (OPTUNA) ---\n",
    "def objective(trial, train_data, val_data, input_dim, output_dim, args, DEVICE):\n",
    "    # Suggerisci parametri\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [128, 256, 512])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [256, 512, 1024])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    \n",
    "    model = ProbabilisticNetwork(input_dim, output_dim, hidden_size).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.GaussianNLLLoss().to(DEVICE)\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_val, y_val = val_data\n",
    "    \n",
    "    y_val = y_val.to(DEVICE)\n",
    "    X_val = X_val.to(DEVICE)\n",
    "    \n",
    "    # Training Loop Breve\n",
    "    for epoch in range(args.hpo_epochs):\n",
    "        model.train()\n",
    "        # Batching semplificato per HPO\n",
    "        permutation = torch.randperm(X_train.size(0))\n",
    "        batch_size = batch_size\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        for i in range(0, X_train.size(0), batch_size):\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            mu, logvar = model(batch_x.to(DEVICE))\n",
    "            loss = loss_fn(mu, batch_y.to(DEVICE), torch.exp(logvar) + 1e-6) # epsilon to avoid instability\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            v_mu, v_logvar = model(X_val.to(DEVICE))\n",
    "            val_loss = loss_fn(v_mu, y_val, torch.exp(v_logvar) + 1e-6).item()\n",
    "        \n",
    "        # Pruning (Optuna ferma i trial che vanno male subito)\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(train_data, val_data, input_dim, output_dim, config, DEVICE):\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(f\" FASE 2: TRAINING ENSEMBLE ({config['k_models_total']} MODELLI)\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # 1. Preparazione Dati\n",
    "    X_train, y_train = train_data \n",
    "    X_val, y_val = val_data      \n",
    "    \n",
    "    # Per la validazione usiamo tutto il set su GPU (se entra in memoria)\n",
    "    X_val_gpu = X_val.to(DEVICE)\n",
    "    y_val_gpu = y_val.to(DEVICE)\n",
    "\n",
    "    loss_fn = nn.GaussianNLLLoss()\n",
    "    mse_fn = nn.MSELoss() \n",
    "    trained_model_infos = [] \n",
    "    \n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    # --- BARRA ESTERNA (Loop sui Modelli) ---\n",
    "    # Monitora il progresso totale (es. 1/5, 2/5...)\n",
    "    pbar_ensemble = tqdm(range(config['k_models_total']), desc=\"Ensemble Progress\", unit=\"model\")\n",
    "\n",
    "    for i in pbar_ensemble:\n",
    "        \n",
    "        # --- IMPLEMENTAZIONE BOOTSTRAPPING ---\n",
    "        num_samples = len(X_train)\n",
    "        indices = torch.randint(0, num_samples, (num_samples,))\n",
    "        \n",
    "        X_train_boot = X_train[indices]\n",
    "        y_train_boot = y_train[indices]\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_boot, y_train_boot)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "        # -------------------------------------\n",
    "        \n",
    "        # Inizializza modello e optimizer\n",
    "        model = ProbabilisticNetwork(input_dim, output_dim, config['hidden_size']).to(DEVICE)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config.get('weight_decay', 0))\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "        if i == 0: \n",
    "            wandb.watch(model, log=\"gradients\", log_freq=100)\n",
    "            \n",
    "        # Setup Early Stopping\n",
    "        # Correzione path: meglio assicurarsi che la cartella esista\n",
    "        save_dir = f\"{config['save_path']}unc_{config['p_name']}\"\n",
    "        os.makedirs(save_dir, exist_ok=True) \n",
    "        save_path = f\"{save_dir}/{i}_best.pth\"\n",
    "        \n",
    "        stopper = EarlyStopping(patience=config['patience'], save_path=save_path)\n",
    "        \n",
    "        # --- BARRA INTERNA (Loop Epoche) ---\n",
    "        # leave=False fa sparire la barra quando il modello finisce\n",
    "        pbar_epochs = tqdm(range(config['final_epochs']), \n",
    "                           desc=f\"Model {i+1}/{config['k_models_total']}\", \n",
    "                           leave=False,\n",
    "                           unit=\"epoch\")\n",
    "        \n",
    "        for epoch in pbar_epochs:\n",
    "            \n",
    "            model.train()\n",
    "            epoch_nll_acc = 0.0\n",
    "            epoch_mse_acc = 0.0 \n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x = batch_x.to(DEVICE)\n",
    "                batch_y = batch_y.to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                mu, logvar = model(batch_x)\n",
    "                \n",
    "                # Calcolo Loss (Gaussian NLL) -> Per l'ottimizzazione\n",
    "                loss = loss_fn(mu, batch_y, torch.exp(logvar) + 1e-6)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # --- CALCOLI PER LOGGING ---\n",
    "                with torch.no_grad():\n",
    "                    batch_mse = mse_fn(mu, batch_y)\n",
    "                    epoch_mse_acc += batch_mse.item() \n",
    "                \n",
    "                epoch_nll_acc += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            # Medie per epoca\n",
    "            avg_train_nll = epoch_nll_acc / num_batches\n",
    "            avg_train_mse = epoch_mse_acc / num_batches\n",
    "            \n",
    "            # --- VALIDATION ---\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                v_mu, v_logvar = model(X_val_gpu)\n",
    "                v_var = torch.exp(v_logvar) \n",
    "                \n",
    "                val_nll = loss_fn(v_mu, y_val_gpu, v_var + 1e-6).item()\n",
    "                val_mse = mse_fn(v_mu, y_val_gpu).item() \n",
    "            \n",
    "            scheduler.step(val_nll)\n",
    "\n",
    "            # --- AGGIORNAMENTO BARRA TQDM ---\n",
    "            # Questo mostra i numeri direttamente sulla barra di caricamento!\n",
    "            pbar_epochs.set_postfix({\n",
    "                \"T_NLL\": f\"{avg_train_nll:.3f}\", \n",
    "                \"V_NLL\": f\"{val_nll:.3f}\", \n",
    "                \"Best\": f\"{stopper.best_loss:.3f}\"\n",
    "            })\n",
    "\n",
    "            # --- WANDB LOGGING ---\n",
    "            # Ho corretto 'ensamble' in 'ensemble' (typo comune)\n",
    "            metrics = {\n",
    "                f\"ensemble/train_nll\": avg_train_nll,\n",
    "                f\"ensemble/train_mse\": avg_train_mse,\n",
    "                f\"ensemble/val_nll\": val_nll,\n",
    "                f\"ensemble/val_mse\": val_mse,\n",
    "                f\"ensemble/max_logvar\": model.max_logvar.mean().item(),\n",
    "                f\"ensemble/min_logvar\": model.min_logvar.mean().item(),\n",
    "                f\"ensemble/predicted_var_mean\": v_var.mean().item(),\n",
    "                f\"ensemble/lr\": optimizer.param_groups[0]['lr'],\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            wandb.log(metrics)\n",
    "\n",
    "            # Check Early Stopping\n",
    "            stopper(val_nll, model)\n",
    "            \n",
    "            if stopper.early_stop:\n",
    "                # Opzionale: stampa se vuoi evidenziare lo stop\n",
    "                # tqdm.write(f\"-> Early stop Model {i+1} at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # 4. Fine training modello corrente\n",
    "        # Ricarichiamo i pesi migliori salvati dall'EarlyStopping\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "        \n",
    "        trained_model_infos.append({\n",
    "            \"id\": i,\n",
    "            \"best_val_loss\": stopper.best_loss,\n",
    "            \"model\": model,    \n",
    "            \"path\": save_path\n",
    "        })\n",
    "        \n",
    "    return trained_model_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe9a7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config path: ./config/uncertainty_debug.yaml\n",
      "Using device: cuda:0\n",
      "Loading data from ./remote_results/UTS/UTS_4612312...\n",
      "Loading actor network\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "\n",
    "if torch.cuda.is_available() and args.cuda >= 0:\n",
    "    # F-string per inserire l'indice: diventa \"cuda:2\"\n",
    "    device_str = f\"cuda:{args.cuda}\"\n",
    "else:\n",
    "    device_str = \"cpu\"\n",
    "DEVICE = torch.device(device_str)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "splitted = args.data_test_name.rsplit('_', 1)\n",
    "full_data_path = args.data_path + splitted[0] + '/' + args.data_test_name\n",
    "print(f\"Loading data from {full_data_path}...\")\n",
    "with open(full_data_path + '_transitions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "with open(full_data_path + '_info.json', 'r') as f:\n",
    "    info_test = json.load(f)\n",
    "\n",
    "RAY_PER_DIRECTION = info_test['metadata']['other_config']['rays_per_direction']\n",
    "RAYCAST_SIZE = 2*RAY_PER_DIRECTION + 1\n",
    "STATE_SIZE = info_test['metadata']['other_config']['state_observation_size'] - 1\n",
    "\n",
    "ACTION_SIZE = info_test['metadata']['other_config']['action_size']\n",
    "ACTION_MIN = info_test['metadata']['other_config']['min_action']\n",
    "ACTION_MAX = info_test['metadata']['other_config']['max_action']\n",
    "\n",
    "INPUT_STACK = info_test['metadata']['train_config']['input_stack']\n",
    "TOTAL_STATE_SIZE = (STATE_SIZE + RAYCAST_SIZE)*INPUT_STACK\n",
    "\n",
    "print(f\"Loading actor network\")\n",
    "actor = OldDenseActor(\n",
    "    TOTAL_STATE_SIZE,\n",
    "    ACTION_SIZE,\n",
    "    ACTION_MIN,\n",
    "    ACTION_MAX,\n",
    "    info_test['metadata']['test_config']['policy_layers'][info_test['metadata']['test_config']['policy_names'].index(args.p_name)]\n",
    ").to(DEVICE)\n",
    "load_models(actor, save_path='./models/' + args.p_name, suffix='_best', DEVICE=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f301a041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Caricamento e Processamento Dati (Split per Episodi)...\n",
      "Shuffling degli episodi...\n",
      "Split Episodi -> Train: 4000, Val: 500, Test: 500\n",
      "Processing Train (4000 episodes)...\n",
      "Processing Validation (500 episodes)...\n",
      "Processing Test (500 episodes)...\n",
      "Final Dataset Shapes:\n",
      "Train: X=torch.Size([341264, 116]), y=torch.Size([341264, 28])\n",
      "Val:   X=torch.Size([42869, 116]), y=torch.Size([42869, 28])\n",
      "Test:  X=torch.Size([42853, 116]), y=torch.Size([42853, 28])\n"
     ]
    }
   ],
   "source": [
    "# 1. Dati\n",
    "train_data, val_data, test_data, input_dim, output_dim = load_and_split_data(\n",
    "                                                    data,\n",
    "                                                    actor,\n",
    "                                                    RAYCAST_SIZE,\n",
    "                                                    INPUT_STACK,\n",
    "                                                    STATE_SIZE,\n",
    "                                                    DEVICE\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c49c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 10:09:14,917]\u001b[0m A new study created in memory with name: no-name-0e4290f3-9dae-4c47-8f03-a97c64e6cc5a\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " FASE 1: RICERCA IPERPARAMETRI (Optuna)\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 10:10:39,717]\u001b[0m Trial 0 finished with value: -1.9441558122634888 and parameters: {'lr': 1.7262590020459734e-05, 'hidden_size': 256, 'batch_size': 64, 'weight_decay': 1.5625011297620395e-05}. Best is trial 0 with value: -1.9441558122634888.\u001b[0m\n",
      "\u001b[32m[I 2026-02-03 10:11:59,216]\u001b[0m Trial 1 finished with value: -2.1721174716949463 and parameters: {'lr': 7.329781612787779e-05, 'hidden_size': 128, 'batch_size': 64, 'weight_decay': 7.03430678149681e-05}. Best is trial 1 with value: -2.1721174716949463.\u001b[0m\n",
      "\u001b[32m[I 2026-02-03 10:13:17,761]\u001b[0m Trial 2 finished with value: -1.881776213645935 and parameters: {'lr': 1.4644096773136937e-05, 'hidden_size': 256, 'batch_size': 64, 'weight_decay': 0.0004741167961394823}. Best is trial 1 with value: -2.1721174716949463.\u001b[0m\n",
      "\u001b[32m[I 2026-02-03 10:13:58,793]\u001b[0m Trial 3 finished with value: -2.630431890487671 and parameters: {'lr': 0.000606930844892509, 'hidden_size': 256, 'batch_size': 128, 'weight_decay': 2.1618791041476967e-06}. Best is trial 3 with value: -2.630431890487671.\u001b[0m\n",
      "\u001b[32m[I 2026-02-03 10:14:19,906]\u001b[0m Trial 4 finished with value: -2.4117801189422607 and parameters: {'lr': 0.0019588497978082087, 'hidden_size': 128, 'batch_size': 256, 'weight_decay': 1.5487453864220027e-05}. Best is trial 3 with value: -2.630431890487671.\u001b[0m\n",
      "\u001b[32m[I 2026-02-03 10:14:43,828]\u001b[0m Trial 5 finished with value: -2.44382381439209 and parameters: {'lr': 0.00022742142830974054, 'hidden_size': 512, 'batch_size': 256, 'weight_decay': 6.0056776729960526e-06}. Best is trial 3 with value: -2.630431890487671.\u001b[0m\n",
      "\u001b[32m[I 2026-02-03 10:14:47,246]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-03 10:15:01,223]\u001b[0m Trial 7 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Migliori Parametri Trovati: {'lr': 0.000606930844892509, 'hidden_size': 256, 'batch_size': 128, 'weight_decay': 2.1618791041476967e-06}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FASE 1: Hyperparameter Optimization (HPO)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" FASE 1: RICERCA IPERPARAMETRI (Optuna)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda t: objective(t, train_data, val_data, input_dim, output_dim, args, DEVICE), \n",
    "                n_trials=args.hpo_trials)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"\\n>>> Migliori Parametri Trovati: {best_params}\")\n",
    "\n",
    "# Uniamo la config globale con i parametri ottimizzati\n",
    "FINAL_CONFIG = vars(args).copy()\n",
    "FINAL_CONFIG.update(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb71f547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\cicci\\Desktop\\UASRL\\wandb\\run-20260203_101506-9mb6mge4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giacomo-aru/Unc_UASRL/runs/9mb6mge4' target=\"_blank\">hearty-dawn-7</a></strong> to <a href='https://wandb.ai/giacomo-aru/Unc_UASRL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giacomo-aru/Unc_UASRL' target=\"_blank\">https://wandb.ai/giacomo-aru/Unc_UASRL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giacomo-aru/Unc_UASRL/runs/9mb6mge4' target=\"_blank\">https://wandb.ai/giacomo-aru/Unc_UASRL/runs/9mb6mge4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/giacomo-aru/Unc_UASRL/runs/9mb6mge4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x24e4a628970>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=args.project_name,\n",
    "    config=FINAL_CONFIG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3b8f640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " FASE 2: TRAINING ENSEMBLE (5 MODELLI)\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Progress:   0%|          | 0/5 [00:00<?, ?model/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Progress:  20%|██        | 1/5 [03:05<12:22, 185.64s/model]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 8 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Ensemble Progress: 100%|██████████| 5/5 [14:48<00:00, 177.74s/model]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " FASE 3: SELEZIONE ELITE\n",
      "Migliori modelli selezionati (ID): [2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FASE 2: Training Ensemble Completo\n",
    "# ---------------------------------------------------------\n",
    "all_models_info = train_ensemble(train_data, val_data, input_dim, output_dim, FINAL_CONFIG, DEVICE)\n",
    "\n",
    "# 4. SELEZIONE ELITE\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" FASE 3: SELEZIONE ELITE\")\n",
    "# Ordiniamo in base alla validation loss ritornata dalla funzione\n",
    "all_models_info.sort(key=lambda x: x[\"best_val_loss\"])\n",
    "\n",
    "# Prendiamo i primi N\n",
    "elites_info = all_models_info[:FINAL_CONFIG[\"n_elites\"]]\n",
    "elite_indices = [m[\"id\"] for m in elites_info]\n",
    "elite_models = [m[\"model\"] for m in elites_info]\n",
    "\n",
    "print(f\"Migliori modelli selezionati (ID): {elite_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da206bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " FASE 3: SELEZIONE ELITE\n",
      "========================================\n",
      "Migliori modelli selezionati (ID): [2, 0]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FASE 3: Selezione Elite\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" FASE 3: SELEZIONE ELITE\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Ordina modelli per validation loss\n",
    "all_models_info.sort(key=lambda x: x[\"best_val_loss\"])\n",
    "\n",
    "# Prendi i primi N\n",
    "elites_info = all_models_info[:FINAL_CONFIG[\"n_elites\"]]\n",
    "elite_indices = [m[\"id\"] for m in elites_info]\n",
    "elite_models = [m[\"model\"] for m in elites_info]\n",
    "\n",
    "print(f\"Migliori modelli selezionati (ID): {elite_indices}\")\n",
    "# wandb.log({\"elite_indices\": elite_indices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f06171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " FASE 4: TEST SET & METRICHE INCERTEZZA\n",
      "========================================\n",
      "TEST MSE: 0.01363\n",
      "Mean Aleatoric Unc: 0.01475\n",
      "Mean Epistemic Unc: 0.00038\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FASE 4: Test e Incertezza\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" FASE 4: TEST SET & METRICHE INCERTEZZA\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "X_test, y_test = test_data\n",
    "X_test = X_test.to(DEVICE)\n",
    "y_test = y_test.to(DEVICE)\n",
    "\n",
    "# Liste per raccogliere predizioni di tutti gli elite\n",
    "mus_list = []\n",
    "vars_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for model in elite_models:\n",
    "        model.eval()\n",
    "        mu, logvar = model(X_test)\n",
    "        mus_list.append(mu.unsqueeze(0))         # [1, N_data, Dim]\n",
    "        vars_list.append(torch.exp(logvar).unsqueeze(0))\n",
    "        \n",
    "# Stack: [N_Elites, N_data, Dim]\n",
    "ensemble_mus = torch.cat(mus_list, dim=0)\n",
    "ensemble_vars = torch.cat(vars_list, dim=0)\n",
    "\n",
    "# Calcoli Mixture of Gaussians\n",
    "# 1. Predizione finale (Media delle medie)\n",
    "final_mean = torch.mean(ensemble_mus, dim=0)\n",
    "\n",
    "# 2. Incertezza Aleatoria (Media delle varianze)\n",
    "aleatoric = torch.mean(ensemble_vars, dim=0)\n",
    "\n",
    "# 3. Incertezza Epistemica (Varianza delle medie)\n",
    "epistemic = torch.var(ensemble_mus, dim=0, unbiased=False)\n",
    "\n",
    "# 4. Errore MSE\n",
    "mse = nn.MSELoss()(final_mean, y_test)\n",
    "\n",
    "print(f\"TEST MSE: {mse.item():.5f}\")\n",
    "print(f\"Mean Aleatoric Unc: {aleatoric.mean().item():.5f}\")\n",
    "print(f\"Mean Epistemic Unc: {epistemic.mean().item():.5f}\")\n",
    "\n",
    "# Log metriche finali\n",
    "wandb.log({\n",
    "    \"test_mse\": mse.item(),\n",
    "    \"aleatoric_uncertainty_mean\": aleatoric.mean().item(),\n",
    "    \"epistemic_uncertainty_mean\": epistemic.mean().item()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "643a660d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./unc_models/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5ad0321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Salvataggio Checkpoint Finale...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>aleatoric_uncertainty_mean</td><td>▁</td></tr><tr><td>ensemble/lr</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ensemble/max_logvar</td><td>██▇▄▂▂▁▁▁▁</td></tr><tr><td>ensemble/min_logvar</td><td>▃█▇▆▅▄▃▂▁▁</td></tr><tr><td>ensemble/predicted_var_mean</td><td>▆█▄▃▆▁▄▄▄▆</td></tr><tr><td>ensemble/train_mse</td><td>█▂▂▂▁▁▁▁▁▁</td></tr><tr><td>ensemble/train_nll</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>ensemble/val_mse</td><td>█▅▄▃▄▂▂▂▁▁</td></tr><tr><td>ensemble/val_nll</td><td>█▆▄▄▅▃▂▃▁▂</td></tr><tr><td>epistemic_uncertainty_mean</td><td>▁</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>aleatoric_uncertainty_mean</td><td>0.01475</td></tr><tr><td>ensemble/lr</td><td>0.00061</td></tr><tr><td>ensemble/max_logvar</td><td>-0.09408</td></tr><tr><td>ensemble/min_logvar</td><td>-8.80488</td></tr><tr><td>ensemble/predicted_var_mean</td><td>0.01531</td></tr><tr><td>ensemble/train_mse</td><td>0.01423</td></tr><tr><td>ensemble/train_nll</td><td>-2.90345</td></tr><tr><td>ensemble/val_mse</td><td>0.01495</td></tr><tr><td>ensemble/val_nll</td><td>-2.84226</td></tr><tr><td>epistemic_uncertainty_mean</td><td>0.00038</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-dawn-7</strong> at: <a href='https://wandb.ai/giacomo-aru/Unc_UASRL/runs/9mb6mge4' target=\"_blank\">https://wandb.ai/giacomo-aru/Unc_UASRL/runs/9mb6mge4</a><br> View project at: <a href='https://wandb.ai/giacomo-aru/Unc_UASRL' target=\"_blank\">https://wandb.ai/giacomo-aru/Unc_UASRL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260203_101506-9mb6mge4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE COMPLETATA CORRETTAMENTE.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FASE 5: Salvataggio Finale\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nSalvataggio Checkpoint Finale...\")\n",
    "\n",
    "checkpoint = {\n",
    "    \"config\": FINAL_CONFIG,\n",
    "    \"elite_indices\": elite_indices,\n",
    "    \"best_params\": best_params,\n",
    "    \"test_metrics\": {\n",
    "        \"mse\": mse.item()\n",
    "    }\n",
    "}\n",
    "torch.save(checkpoint, f\"{args.save_path}unc_{args.p_name}/info.pth\")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"PIPELINE COMPLETATA CORRETTAMENTE.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
