{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5539a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import numpy as np\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader # Assicurati di importarlo\n",
    "import json\n",
    "\n",
    "from training_utils import *\n",
    "from testing_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d444b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "def parse_args(default_config_path=\"./config/uncertainty_debug.yaml\"):\n",
    "    \"\"\"\n",
    "    Parse arguments from CLI or notebook.\n",
    "    - In notebook: usa il default se non passato\n",
    "    - In CLI: permette override dei parametri nel config\n",
    "    \"\"\"\n",
    "    # --- Gestione notebook: evita crash su ipykernel args ---\n",
    "    argv = sys.argv[1:]\n",
    "    # Se siamo in notebook o non è passato il config_path, inseriamo il default\n",
    "    if len(argv) == 0 or \"--f=\" in \" \".join(argv):\n",
    "        argv = [default_config_path]\n",
    "\n",
    "    # --- Pre-parser per leggere il config_path ---\n",
    "    pre_parser = argparse.ArgumentParser(add_help=False)\n",
    "    pre_parser.add_argument(\n",
    "        \"config_path\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=default_config_path,\n",
    "        help=\"Main config file path\"\n",
    "    )\n",
    "    initial_args, remaining_argv = pre_parser.parse_known_args(argv)\n",
    "    CONFIG_PATH = initial_args.config_path\n",
    "    print(f\"Config path: {CONFIG_PATH}\")\n",
    "\n",
    "    # --- Legge parametri dal file di config ---\n",
    "    file_config_dict = parse_config_file(CONFIG_PATH)\n",
    "\n",
    "    # --- Parser principale ---\n",
    "    parser = argparse.ArgumentParser(description=\"Training Script\")\n",
    "    parser.add_argument(\n",
    "        \"config_path\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=CONFIG_PATH,\n",
    "        help=\"Main config file path\"\n",
    "    )\n",
    "\n",
    "    # Aggiunge parametri dal config file, con tipi corretti\n",
    "    for key, value in file_config_dict.items():\n",
    "        if isinstance(value, bool):\n",
    "            parser.add_argument(f\"--{key}\", type=str2bool, default=value)\n",
    "        elif value is None:\n",
    "            parser.add_argument(f\"--{key}\", type=str, default=value)\n",
    "        else:\n",
    "            parser.add_argument(f\"--{key}\", type=type(value), default=value)\n",
    "\n",
    "    # --- Parse finale con remaining_argv per ignorare args extra Jupyter ---\n",
    "    args, unknown = parser.parse_known_args(remaining_argv)\n",
    "    if unknown:\n",
    "        print(\"Ignored unknown args:\", unknown)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f21f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. IL MODELLO ---\n",
    "class ProbabilisticNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu_head = nn.Linear(hidden_size, output_dim)\n",
    "        self.logvar_head = nn.Linear(hidden_size, output_dim)\n",
    "        \n",
    "        # Limiti per stabilità numerica (Softplus)\n",
    "        self.max_logvar = nn.Parameter(torch.ones(1, output_dim) / 2.0)\n",
    "        self.min_logvar = nn.Parameter(-torch.ones(1, output_dim) * 10.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.network(x)\n",
    "        mu = self.mu_head(features)\n",
    "        logvar = self.logvar_head(features)\n",
    "        \n",
    "        # Clamping morbido\n",
    "        logvar = self.max_logvar - nn.functional.softplus(self.max_logvar - logvar)\n",
    "        logvar = self.min_logvar + nn.functional.softplus(logvar - self.min_logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "# --- 2. EARLY STOPPING ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, save_path=None):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.Inf\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        if self.save_path:\n",
    "            # os.path.dirname estrae la cartella dal path completo (es: \"models/test.pth\" -> \"models\")\n",
    "            dir_name = os.path.dirname(self.save_path)\n",
    "            \n",
    "            # Creiamo la cartella solo se dir_name non è vuoto\n",
    "            if dir_name and not os.path.exists(dir_name):\n",
    "                print(f\" Creazione cartella: {dir_name}\")\n",
    "                os.makedirs(dir_name, exist_ok=True) # exist_ok evita errori se la cartella appare nel mentre\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.save_path:\n",
    "                torch.save(model.state_dict(), self.save_path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ade4920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def load_and_split_data(raw_data, \n",
    "                        actor_model, \n",
    "                        \n",
    "                        RAYCASY_SIZE,\n",
    "                        INPUT_STACK,\n",
    "                        STATE_SIZE, \n",
    "                        DEVICE,\n",
    "                        \n",
    "                        shuffle=True):\n",
    "    print(\">>> Caricamento e Processamento Dati (Split per Episodi)...\")\n",
    "    \n",
    "    # 1. SPLIT DEGLI EPISODI (PRIMA DI TUTTO)\n",
    "    # Copiamo raw_data per non modificare la lista originale fuori dalla funzione\n",
    "    all_episodes = list(raw_data) \n",
    "    \n",
    "    if shuffle:\n",
    "        print(\"Shuffling degli episodi...\")\n",
    "        random.shuffle(all_episodes)\n",
    "\n",
    "    total_episodes = len(all_episodes)\n",
    "    n_train = int(total_episodes * 0.8) # 80%\n",
    "    n_val = int(total_episodes * 0.1)   # 10%\n",
    "    # Il restante 10% va al test\n",
    "\n",
    "    train_episodes = all_episodes[:n_train]\n",
    "    val_episodes = all_episodes[n_train : n_train + n_val]\n",
    "    test_episodes = all_episodes[n_train + n_val:]\n",
    "\n",
    "    print(f\"Split Episodi -> Train: {len(train_episodes)}, Val: {len(val_episodes)}, Test: {len(test_episodes)}\")\n",
    "\n",
    "    # Assicuriamoci che l'actor sia in eval\n",
    "    actor_model.eval()\n",
    "\n",
    "    # --- FUNZIONE INTERNA PER PROCESSARE UNA LISTA DI EPISODI ---\n",
    "    def process_dataset_subset(episodes_subset, subset_name):\n",
    "        if not episodes_subset:\n",
    "            print(f\"Warning: {subset_name} set is empty!\")\n",
    "            return torch.tensor([]), torch.tensor([])\n",
    "\n",
    "        inputs_list = []\n",
    "        outputs_list = []\n",
    "        \n",
    "        print(f\"Processing {subset_name} ({len(episodes_subset)} episodes)...\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for episode in episodes_subset:\n",
    "                # episode[0] sono le osservazioni\n",
    "                all_observations = episode[0]\n",
    "\n",
    "                # Saltiamo episodi troppo corti se necessario, o gestiamo l'errore\n",
    "                if len(all_observations) < 2:\n",
    "                    continue\n",
    "\n",
    "                for t in range(len(all_observations) - 1):\n",
    "                    # --- A. Recupera Input Corrente ---\n",
    "                    actual_obs_and_action = all_observations[t]\n",
    "                    \n",
    "                    # --- B. Calcola Next Observation (Logica Custom) ---\n",
    "                    # Nota: Qui assumiamo che la struttura di episode[0] supporti questo slicing\n",
    "                    next_obs = all_observations[t + 1][(INPUT_STACK - 1)*RAYCASY_SIZE: (INPUT_STACK)*RAYCASY_SIZE] + all_observations[t + 1][-STATE_SIZE - 2:-2]\n",
    "                    \n",
    "                    # --- C. Processamento Actor ---\n",
    "                    obs_tensor = torch.FloatTensor(actual_obs_and_action[:-2]).to(DEVICE)\n",
    "                    \n",
    "                    actor_distrib = actor_model(obs_tensor)\n",
    "                    \n",
    "                    if isinstance(actor_distrib, (tuple, list)):\n",
    "                        actor_distrib = torch.cat(actor_distrib).detach().cpu()\n",
    "                    else:\n",
    "                        actor_distrib = actor_distrib.detach().cpu()\n",
    "                    \n",
    "                    # --- D. Creazione Input Finale ---\n",
    "                    # Riportiamo obs su CPU per unirlo\n",
    "                    input_tensor = torch.cat([torch.FloatTensor(actual_obs_and_action), actor_distrib])\n",
    "                    output_tensor = torch.FloatTensor(next_obs)\n",
    "                    \n",
    "                    inputs_list.append(input_tensor)\n",
    "                    outputs_list.append(output_tensor)\n",
    "        \n",
    "        # Stacking finale per questo subset\n",
    "        if len(inputs_list) > 0:\n",
    "            X = torch.stack(inputs_list).float()\n",
    "            y = torch.stack(outputs_list).float()\n",
    "            return X, y\n",
    "        else:\n",
    "            return torch.tensor([]), torch.tensor([])\n",
    "\n",
    "    # 2. ESEGUIAMO IL PROCESSAMENTO SUI 3 GRUPPI SEPARATI\n",
    "    X_train, y_train = process_dataset_subset(train_episodes, \"Train\")\n",
    "    X_val, y_val = process_dataset_subset(val_episodes, \"Validation\")\n",
    "    X_test, y_test = process_dataset_subset(test_episodes, \"Test\")\n",
    "\n",
    "    input_dim = X_train.shape[1] if len(X_train) > 0 else 0\n",
    "    output_dim = y_train.shape[1] if len(y_train) > 0 else 0\n",
    "    \n",
    "    print(f\"Final Dataset Shapes:\")\n",
    "    print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Val:   X={X_val.shape}, y={y_val.shape}\")\n",
    "    print(f\"Test:  X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), input_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14ef287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. OPTIMIZATION LOOP (OPTUNA) ---\n",
    "def objective(trial, train_data, val_data, input_dim, output_dim, args, DEVICE):\n",
    "    # Suggerisci parametri\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [128, 256, 512])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    \n",
    "    model = ProbabilisticNetwork(input_dim, output_dim, hidden_size).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.GaussianNLLLoss().to(DEVICE)\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_val, y_val = val_data\n",
    "    \n",
    "    y_val = y_val.to(DEVICE)\n",
    "    X_val = X_val.to(DEVICE)\n",
    "    \n",
    "    # Training Loop Breve\n",
    "    for epoch in range(args.hpo_epochs):\n",
    "        model.train()\n",
    "        # Batching semplificato per HPO\n",
    "        permutation = torch.randperm(X_train.size(0))\n",
    "        batch_size = batch_size\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        for i in range(0, X_train.size(0), batch_size):\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            mu, logvar = model(batch_x.to(DEVICE))\n",
    "            loss = loss_fn(mu, batch_y.to(DEVICE), torch.exp(logvar) + 1e-6) # epsilon to avoid instability\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            v_mu, v_logvar = model(X_val.to(DEVICE))\n",
    "            val_loss = loss_fn(v_mu, y_val, torch.exp(v_logvar) + 1e-6).item()\n",
    "        \n",
    "        # Pruning (Optuna ferma i trial che vanno male subito)\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "477a33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(train_data, val_data, input_dim, output_dim, config, DEVICE):\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(f\" FASE 2: TRAINING ENSEMBLE ({config['k_models_total']} MODELLI)\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # 1. Preparazione Dati\n",
    "    X_train, y_train = train_data \n",
    "    X_val, y_val = val_data      \n",
    "    \n",
    "    # Per la validazione usiamo tutto il set su GPU (se entra in memoria)\n",
    "    X_val_gpu = X_val.to(DEVICE)\n",
    "    y_val_gpu = y_val.to(DEVICE)\n",
    "\n",
    "    loss_fn = nn.GaussianNLLLoss()\n",
    "    mse_fn = nn.MSELoss() ### NEW: Serve per calcolare l'errore puro\n",
    "    trained_model_infos = [] \n",
    "    \n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    # 2. Loop sui K modelli dell'Ensemble\n",
    "    for i in range(config['k_models_total']):\n",
    "        \n",
    "        print(f\"\\n--- Sampling Training Data Modello {i+1} ---\")\n",
    "        \n",
    "        # --- IMPLEMENTAZIONE BOOTSTRAPPING ---\n",
    "        num_samples = len(X_train)\n",
    "        indices = torch.randint(0, num_samples, (num_samples,))\n",
    "        \n",
    "        X_train_boot = X_train[indices]\n",
    "        y_train_boot = y_train[indices]\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_boot, y_train_boot)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        # -------------------------------------\n",
    "        \n",
    "        # Inizializza modello e optimizer\n",
    "        model = ProbabilisticNetwork(input_dim, output_dim, config['hidden_size']).to(DEVICE)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config.get('weight_decay', 0))\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "        if i == 0: \n",
    "            wandb.watch(model, log=\"gradients\", log_freq=100)\n",
    "            \n",
    "        # Setup Early Stopping\n",
    "        save_path = f\"{config['save_path']}unc_{config['p_name']}_{i}_best.pth\"\n",
    "        stopper = EarlyStopping(patience=config['patience'], save_path=save_path)\n",
    "        \n",
    "        # 3. Training Loop (Epoche)\n",
    "        for epoch in range(config['final_epochs']):\n",
    "            model.train()\n",
    "            epoch_nll_acc = 0.0\n",
    "            epoch_mse_acc = 0.0 ### NEW: Accumulatore per MSE\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x = batch_x.to(DEVICE)\n",
    "                batch_y = batch_y.to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                mu, logvar = model(batch_x)\n",
    "                \n",
    "                # Calcolo Loss (Gaussian NLL) -> Per l'ottimizzazione\n",
    "                loss = loss_fn(mu, batch_y, torch.exp(logvar) + 1e-6)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # --- CALCOLI PER LOGGING ---\n",
    "                # Usiamo no_grad per risparmiare memoria, calcoliamo MSE puro\n",
    "                with torch.no_grad():\n",
    "                    batch_mse = mse_fn(mu, batch_y)\n",
    "                    epoch_mse_acc += batch_mse.item() ### NEW: Aggiornamento accumulatore\n",
    "                \n",
    "                epoch_nll_acc += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            # Medie per epoca\n",
    "            avg_train_nll = epoch_nll_acc / num_batches\n",
    "            avg_train_mse = epoch_mse_acc / num_batches\n",
    "            \n",
    "            # --- VALIDATION ---\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                v_mu, v_logvar = model(X_val_gpu)\n",
    "                v_var = torch.exp(v_logvar) ### NEW: Calcolo esplicito varianza per log\n",
    "                \n",
    "                val_nll = loss_fn(v_mu, y_val_gpu, v_var + 1e-6).item()\n",
    "                val_mse = mse_fn(v_mu, y_val_gpu).item() ### NEW: Calcolo MSE validation\n",
    "            \n",
    "            scheduler.step(val_nll)\n",
    "\n",
    "            # --- WANDB LOGGING AVANZATO ---\n",
    "            metrics = {\n",
    "                f\"model_{i}/train_nll\": avg_train_nll,\n",
    "                f\"model_{i}/train_mse\": avg_train_mse,\n",
    "                f\"model_{i}/val_nll\": val_nll,\n",
    "                f\"model_{i}/val_mse\": val_mse,\n",
    "                # Monitoraggio parametri interni\n",
    "                f\"model_{i}/max_logvar\": model.max_logvar.mean().item(),\n",
    "                f\"model_{i}/min_logvar\": model.min_logvar.mean().item(),\n",
    "                # Media varianza predetta\n",
    "                f\"model_{i}/predicted_var_mean\": v_var.mean().item(),\n",
    "                # Monitoraggio Learning Rate\n",
    "                f\"model_{i}/lr\": optimizer.param_groups[0]['lr'],\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            wandb.log(metrics)\n",
    "\n",
    "            # Check Early Stopping\n",
    "            stopper(val_nll, model)\n",
    "            \n",
    "            if stopper.early_stop:\n",
    "                print(f\"  -> Early stopping all'epoca {epoch}. Best Val NLL: {stopper.best_loss:.4f}\")\n",
    "                break\n",
    "        \n",
    "        # 4. Fine training modello corrente\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "        \n",
    "        trained_model_infos.append({\n",
    "            \"id\": i,\n",
    "            \"best_val_loss\": stopper.best_loss,\n",
    "            \"model\": model,    \n",
    "            \"path\": save_path\n",
    "        })\n",
    "        \n",
    "    return trained_model_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe9a7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config path: ./config/uncertainty_debug.yaml\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "\n",
    "if torch.cuda.is_available() and args.cuda >= 0:\n",
    "    # F-string per inserire l'indice: diventa \"cuda:2\"\n",
    "    device_str = f\"cuda:{args.cuda}\"\n",
    "else:\n",
    "    device_str = \"cpu\"\n",
    "DEVICE = torch.device(device_str)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "with open(args.data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "RAY_PER_DIRECTION = data['metadata']['other_config']['rays_per_direction']\n",
    "RAYCAST_SIZE = 2*RAY_PER_DIRECTION + 1\n",
    "STATE_SIZE = data['metadata']['other_config']['state_observation_size'] - 1\n",
    "\n",
    "ACTION_SIZE = data['metadata']['other_config']['action_size']\n",
    "ACTION_MIN = data['metadata']['other_config']['min_action']\n",
    "ACTION_MAX = data['metadata']['other_config']['max_action']\n",
    "\n",
    "INPUT_STACK = data['metadata']['train_config']['input_stack']\n",
    "TOTAL_STATE_SIZE = (STATE_SIZE + RAYCAST_SIZE)*INPUT_STACK\n",
    "\n",
    "actor = OldDenseActor(\n",
    "    TOTAL_STATE_SIZE,\n",
    "    ACTION_SIZE,\n",
    "    ACTION_MIN,\n",
    "    ACTION_MAX,\n",
    "    data['metadata']['test_config']['policy_layers'][data['metadata']['test_config']['policy_names'].index(args.p_name)]\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301a041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Caricamento e Processamento Dati (Split per Episodi)...\n",
      "Shuffling degli episodi...\n",
      "Split Episodi -> Train: 800, Val: 100, Test: 100\n",
      "Processing Train (800 episodes)...\n"
     ]
    }
   ],
   "source": [
    "# 1. Dati\n",
    "train_data, val_data, test_data, input_dim, output_dim = load_and_split_data(\n",
    "                                                    data['data'],\n",
    "                                                    actor,\n",
    "                                                    RAYCAST_SIZE,\n",
    "                                                    INPUT_STACK,\n",
    "                                                    STATE_SIZE,\n",
    "                                                    DEVICE\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c49c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 18:38:49,693]\u001b[0m A new study created in memory with name: no-name-9c152841-5c23-4e3b-9708-f061d540f85a\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " FASE 1: RICERCA IPERPARAMETRI (Optuna)\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 18:38:54,712]\u001b[0m Trial 0 finished with value: -2.0146889686584473 and parameters: {'lr': 0.0027012497464462613, 'hidden_size': 256, 'batch_size': 512, 'weight_decay': 1.1422615500996474e-06}. Best is trial 0 with value: -2.0146889686584473.\u001b[0m\n",
      "\u001b[32m[I 2026-02-02 18:38:57,982]\u001b[0m Trial 1 finished with value: -1.3331449031829834 and parameters: {'lr': 0.003095509400757411, 'hidden_size': 256, 'batch_size': 64, 'weight_decay': 9.708318570866548e-06}. Best is trial 0 with value: -2.0146889686584473.\u001b[0m\n",
      "\u001b[32m[I 2026-02-02 18:39:01,067]\u001b[0m Trial 2 finished with value: -0.4854564368724823 and parameters: {'lr': 1.657135435101725e-05, 'hidden_size': 128, 'batch_size': 512, 'weight_decay': 3.1532236165252983e-06}. Best is trial 0 with value: -2.0146889686584473.\u001b[0m\n",
      "\u001b[32m[I 2026-02-02 18:39:04,127]\u001b[0m Trial 3 finished with value: -1.7332886457443237 and parameters: {'lr': 0.00020589311465886908, 'hidden_size': 256, 'batch_size': 512, 'weight_decay': 9.675183811524894e-06}. Best is trial 0 with value: -2.0146889686584473.\u001b[0m\n",
      "\u001b[32m[I 2026-02-02 18:39:07,732]\u001b[0m Trial 4 finished with value: -1.4346095323562622 and parameters: {'lr': 2.0761742721377622e-05, 'hidden_size': 512, 'batch_size': 128, 'weight_decay': 0.00015731275014124253}. Best is trial 0 with value: -2.0146889686584473.\u001b[0m\n",
      "\u001b[32m[I 2026-02-02 18:39:10,924]\u001b[0m Trial 5 finished with value: -1.8359200954437256 and parameters: {'lr': 0.00015452536759820662, 'hidden_size': 512, 'batch_size': 512, 'weight_decay': 4.432803870629456e-06}. Best is trial 0 with value: -2.0146889686584473.\u001b[0m\n",
      "\u001b[32m[I 2026-02-02 18:39:12,380]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-02 18:39:13,936]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-02 18:39:17,671]\u001b[0m Trial 8 finished with value: -1.6943269968032837 and parameters: {'lr': 0.00011328087686764353, 'hidden_size': 512, 'batch_size': 64, 'weight_decay': 1.211560358090776e-05}. Best is trial 0 with value: -2.0146889686584473.\u001b[0m\n",
      "\u001b[32m[I 2026-02-02 18:39:20,865]\u001b[0m Trial 9 finished with value: -1.9422626495361328 and parameters: {'lr': 0.005490799498415131, 'hidden_size': 128, 'batch_size': 512, 'weight_decay': 2.157203019265162e-06}. Best is trial 0 with value: -2.0146889686584473.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Migliori Parametri Trovati: {'lr': 0.0027012497464462613, 'hidden_size': 256, 'batch_size': 512, 'weight_decay': 1.1422615500996474e-06}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FASE 1: Hyperparameter Optimization (HPO)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" FASE 1: RICERCA IPERPARAMETRI (Optuna)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda t: objective(t, train_data, val_data, input_dim, output_dim, args, DEVICE), \n",
    "                n_trials=args.hpo_trials)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"\\n>>> Migliori Parametri Trovati: {best_params}\")\n",
    "\n",
    "# Uniamo la config globale con i parametri ottimizzati\n",
    "FINAL_CONFIG = vars(args).copy()\n",
    "FINAL_CONFIG.update(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71f547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgiacomoaru\u001b[0m (\u001b[33mgiacomo-aru\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\cicci\\Desktop\\UASRL\\wandb\\run-20260202_183921-ci7qme8d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/giacomo-aru/Unc_UASRL/runs/ci7qme8d' target=\"_blank\">confused-darkness-3</a></strong> to <a href='https://wandb.ai/giacomo-aru/Unc_UASRL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/giacomo-aru/Unc_UASRL' target=\"_blank\">https://wandb.ai/giacomo-aru/Unc_UASRL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/giacomo-aru/Unc_UASRL/runs/ci7qme8d' target=\"_blank\">https://wandb.ai/giacomo-aru/Unc_UASRL/runs/ci7qme8d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/giacomo-aru/Unc_UASRL/runs/ci7qme8d?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1a0f1bfe620>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=args.project_name,\n",
    "    config=FINAL_CONFIG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8f640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " FASE 2: TRAINING ENSEMBLE (5 MODELLI)\n",
      "========================================\n",
      "\n",
      "--- Sampling Training Data Modello 1 ---\n",
      " Creazione cartella: ./unc_models\n",
      "  -> Early stopping all'epoca 6. Best Val NLL: -2.0078\n",
      "\n",
      "--- Sampling Training Data Modello 2 ---\n",
      "  -> Early stopping all'epoca 8. Best Val NLL: -2.0843\n",
      "\n",
      "--- Sampling Training Data Modello 3 ---\n",
      "\n",
      "--- Sampling Training Data Modello 4 ---\n",
      "  -> Early stopping all'epoca 9. Best Val NLL: -2.2251\n",
      "\n",
      "--- Sampling Training Data Modello 5 ---\n",
      "\n",
      "========================================\n",
      " FASE 3: SELEZIONE ELITE\n",
      "Migliori modelli selezionati (ID): [4, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FASE 2: Training Ensemble Completo\n",
    "# ---------------------------------------------------------\n",
    "all_models_info = train_ensemble(train_data, val_data, input_dim, output_dim, FINAL_CONFIG, DEVICE)\n",
    "\n",
    "# 4. SELEZIONE ELITE\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" FASE 3: SELEZIONE ELITE\")\n",
    "# Ordiniamo in base alla validation loss ritornata dalla funzione\n",
    "all_models_info.sort(key=lambda x: x[\"best_val_loss\"])\n",
    "\n",
    "# Prendiamo i primi N\n",
    "elites_info = all_models_info[:FINAL_CONFIG[\"n_elites\"]]\n",
    "elite_indices = [m[\"id\"] for m in elites_info]\n",
    "elite_models = [m[\"model\"] for m in elites_info]\n",
    "\n",
    "print(f\"Migliori modelli selezionati (ID): {elite_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da206bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " FASE 3: SELEZIONE ELITE\n",
      "========================================\n",
      "Migliori modelli selezionati (ID): [4, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FASE 3: Selezione Elite\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" FASE 3: SELEZIONE ELITE\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Ordina modelli per validation loss\n",
    "all_models_info.sort(key=lambda x: x[\"best_val_loss\"])\n",
    "\n",
    "# Prendi i primi N\n",
    "elites_info = all_models_info[:FINAL_CONFIG[\"n_elites\"]]\n",
    "elite_indices = [m[\"id\"] for m in elites_info]\n",
    "elite_models = [m[\"model\"] for m in elites_info]\n",
    "\n",
    "print(f\"Migliori modelli selezionati (ID): {elite_indices}\")\n",
    "# wandb.log({\"elite_indices\": elite_indices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f06171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " FASE 4: TEST SET & METRICHE INCERTEZZA\n",
      "========================================\n",
      "TEST MSE: 0.01630\n",
      "Mean Aleatoric Unc: 0.01588\n",
      "Mean Epistemic Unc: 0.00088\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FASE 4: Test e Incertezza\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" FASE 4: TEST SET & METRICHE INCERTEZZA\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "X_test, y_test = test_data\n",
    "X_test = X_test.to(DEVICE)\n",
    "y_test = y_test.to(DEVICE)\n",
    "\n",
    "# Liste per raccogliere predizioni di tutti gli elite\n",
    "mus_list = []\n",
    "vars_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for model in elite_models:\n",
    "        model.eval()\n",
    "        mu, logvar = model(X_test)\n",
    "        mus_list.append(mu.unsqueeze(0))         # [1, N_data, Dim]\n",
    "        vars_list.append(torch.exp(logvar).unsqueeze(0))\n",
    "        \n",
    "# Stack: [N_Elites, N_data, Dim]\n",
    "ensemble_mus = torch.cat(mus_list, dim=0)\n",
    "ensemble_vars = torch.cat(vars_list, dim=0)\n",
    "\n",
    "# Calcoli Mixture of Gaussians\n",
    "# 1. Predizione finale (Media delle medie)\n",
    "final_mean = torch.mean(ensemble_mus, dim=0)\n",
    "\n",
    "# 2. Incertezza Aleatoria (Media delle varianze)\n",
    "aleatoric = torch.mean(ensemble_vars, dim=0)\n",
    "\n",
    "# 3. Incertezza Epistemica (Varianza delle medie)\n",
    "epistemic = torch.var(ensemble_mus, dim=0, unbiased=False)\n",
    "\n",
    "# 4. Errore MSE\n",
    "mse = nn.MSELoss()(final_mean, y_test)\n",
    "\n",
    "print(f\"TEST MSE: {mse.item():.5f}\")\n",
    "print(f\"Mean Aleatoric Unc: {aleatoric.mean().item():.5f}\")\n",
    "print(f\"Mean Epistemic Unc: {epistemic.mean().item():.5f}\")\n",
    "\n",
    "# Log metriche finali\n",
    "wandb.log({\n",
    "    \"test_mse\": mse.item(),\n",
    "    \"aleatoric_uncertainty_mean\": aleatoric.mean().item(),\n",
    "    \"epistemic_uncertainty_mean\": epistemic.mean().item()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad0321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Salvataggio Checkpoint Finale...\n",
      "PIPELINE COMPLETATA CORRETTAMENTE.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FASE 5: Salvataggio Finale\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nSalvataggio Checkpoint Finale...\")\n",
    "\n",
    "checkpoint = {\n",
    "    \"config\": FINAL_CONFIG,\n",
    "    \"elite_indices\": elite_indices,\n",
    "    \"best_params\": best_params,\n",
    "    \"test_metrics\": {\n",
    "        \"mse\": mse.item()\n",
    "    }\n",
    "}\n",
    "torch.save(checkpoint, \"final_pipeline_metadata.pt\")\n",
    "\n",
    "\n",
    "# wandb.finish()\n",
    "print(\"PIPELINE COMPLETATA CORRETTAMENTE.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
