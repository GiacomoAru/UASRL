{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2428efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.buffers import DictReplayBuffer\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "\n",
    "from utils_policy_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423c94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = './train_config/standard.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd055c9a",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ba1ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actor_network_layers': [128, 128, 128],\n",
      " 'alpha': 1.0,\n",
      " 'alpha_lr': 0.0004,\n",
      " 'autotune': True,\n",
      " 'batch_size': 256,\n",
      " 'bootstrap_batch_proportion': 0.8,\n",
      " 'buffer_size': 120000,\n",
      " 'cuda': True,\n",
      " 'env_id': 'std',\n",
      " 'exp_name': 'base+wp',\n",
      " 'gamma': 0.995,\n",
      " 'learning_starts': 1000,\n",
      " 'loss_log_interval': 100,\n",
      " 'metrics_log_interval': 300,\n",
      " 'metrics_smoothing': 0.985,\n",
      " 'noise_clip': 0.5,\n",
      " 'policy_frequency': 4,\n",
      " 'policy_lr': 0.0004,\n",
      " 'q_ensemble_n': 5,\n",
      " 'q_lr': 0.0004,\n",
      " 'q_network_layers': [128, 128, 128],\n",
      " 'seed': 59409,\n",
      " 'target_network_frequency': 1,\n",
      " 'tau': 0.005,\n",
      " 'torch_deterministic': True,\n",
      " 'total_timesteps': 50000,\n",
      " 'update_per_step': 1}\n"
     ]
    }
   ],
   "source": [
    "args = parse_args_from_file(CONFIG)\n",
    "args.seed = random.randint(0, 2**16)\n",
    "# args.name = generate_funny_name()\n",
    "\n",
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1cd5e",
   "metadata": {},
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dcecf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c69761",
   "metadata": {},
   "source": [
    "# Start Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce3cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the channel\n",
    "env_info = CustomChannel()\n",
    "\n",
    "# env setup\n",
    "env = UnityEnvironment(None, seed=args.seed, side_channels=[env_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "398130bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b073c",
   "metadata": {},
   "source": [
    "# Environment Variables and Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f2cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_name = f\"{args.exp_name}_{int(time.time()) - 1751796000}\"\n",
    "args.full_name = run_name\n",
    "\n",
    "# writer to track performance\n",
    "writer = SummaryWriter(f\"ens_train/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"Algorithm Hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "for dict in env_info.settings:\n",
    "    writer.add_text(\n",
    "        dict,\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in env_info.settings[dict].items()])),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f92202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the correct agent's behavoir (works if there is only one behavoir)\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "if len(behavior_names) > 1:\n",
    "    raise Exception(\"Multiple behaviors found.\")\n",
    "BEHAVIOUR_NAME = behavior_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce49ae1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_settings': {'step_after_goal': 10,\n",
       "  'max_step': 1000,\n",
       "  'max_movement_speed': 3.0,\n",
       "  'max_turn_speed': 160.0,\n",
       "  'agent_spawn_offset': 0.15000000596046448,\n",
       "  'move_smooth_time': 0.10000000149011612,\n",
       "  'goal_reward': 10.0,\n",
       "  'wall_hit_penalty': 0.0,\n",
       "  'wall_hit_speed_weight': 0.0,\n",
       "  'progress_reward': 0.05000000074505806,\n",
       "  'stagnation_penalty': -0.019999999552965164,\n",
       "  'ema_range_penalty': 5.0,\n",
       "  'ema_smoothing': 0.010999999940395355},\n",
       " 'ray_sensor_settings': {'sensor_name': 'RayPerceptionSensor',\n",
       "  'rays_per_direction': 8,\n",
       "  'max_ray_degrees': 75.0,\n",
       "  'sphere_cast_radius': 0.0,\n",
       "  'ray_length': 8.0,\n",
       "  'observation_stacks': 4,\n",
       "  'alternating_ray_order': False,\n",
       "  'use_batched_raycasts': True,\n",
       "  'min_observation': 0.0,\n",
       "  'max_observation': 1.0,\n",
       "  'ignore_last_ray': False},\n",
       " 'behavior_parameters_settings': {'behavior_name': 'turtlebot?team=0',\n",
       "  'observation_size': 7,\n",
       "  'stacked_vector': 4,\n",
       "  'min_observation': -256.0,\n",
       "  'max_observation': 256.0,\n",
       "  'continuous_actions': 2,\n",
       "  'min_action': -1.0,\n",
       "  'max_action': 1.0},\n",
       " 'environment_settings': {'goal_scale': 0.30000001192092896,\n",
       "  'goal_spawn_height': 0.20000000298023224,\n",
       "  'obstacles_reset_interval': 1,\n",
       "  'obstacles_reset_counter': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b43995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEHAVIOUR_NAME = env_info.settings['behavoir_parameters_settings']['behavior_name']\n",
    "\n",
    "RAY_STACK = env_info.settings['ray_sensor_settings']['observation_stacks']\n",
    "RAY_PER_DIRECTION = env_info.settings['ray_sensor_settings']['rays_per_direction']\n",
    "RAYCAST_MIN = env_info.settings['ray_sensor_settings']['min_observation']\n",
    "RAYCAST_MAX = env_info.settings['ray_sensor_settings']['max_observation']\n",
    "DELETE_LAST_RAY = env_info.settings['ray_sensor_settings']['ignore_last_ray']\n",
    "\n",
    "\n",
    "STATE_STACK = env_info.settings['behavior_parameters_settings']['stacked_vector']\n",
    "STATE_SIZE = env_info.settings['behavior_parameters_settings']['observation_size']\n",
    "STATE_MIN = env_info.settings['behavior_parameters_settings']['min_observation']\n",
    "STATE_MAX = env_info.settings['behavior_parameters_settings']['max_observation']\n",
    "\n",
    "ACTION_SIZE = env_info.settings['behavior_parameters_settings']['continuous_actions']\n",
    "ACTION_MIN = env_info.settings['behavior_parameters_settings']['min_action']\n",
    "ACTION_MAX = env_info.settings['behavior_parameters_settings']['max_action']\n",
    "\n",
    "if DELETE_LAST_RAY:\n",
    "    RAYCAST_SHAPE = (RAY_STACK, 2*RAY_PER_DIRECTION) \n",
    "else:\n",
    "    RAYCAST_SHAPE = (RAY_STACK, 2*RAY_PER_DIRECTION + 1) \n",
    "STATE_SHAPE = (STATE_SIZE*STATE_STACK, )\n",
    "ACTION_SHAPE = (ACTION_SIZE, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4736a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating the training networks\n",
    "actor = DenseActor(RAYCAST_SHAPE, STATE_SHAPE[0], ACTION_SHAPE[0], ACTION_MIN, ACTION_MAX, args.actor_network_layers).to(device)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)\n",
    "\n",
    "qf_ensemble = [DenseSoftQNetwork(RAYCAST_SHAPE, STATE_SHAPE[0], ACTION_SHAPE[0], args.q_network_layers).to(device) for _ in range(args.q_ensemble_n)]\n",
    "qf_ensemble_target = [DenseSoftQNetwork(RAYCAST_SHAPE, STATE_SHAPE[0], ACTION_SHAPE[0], args.q_network_layers).to(device) for _ in range(args.q_ensemble_n)]\n",
    "for q_t, q in zip(qf_ensemble_target, qf_ensemble):\n",
    "    q_t.load_state_dict(q.state_dict())\n",
    "\n",
    "par = []\n",
    "for q in qf_ensemble:\n",
    "    par += list(q.parameters())\n",
    "qf_optimizer = torch.optim.Adam(\n",
    "    par,\n",
    "    lr=args.q_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754acdf",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb74125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# definition of the gym spaces for the replay buffer\n",
    "observation_space = spaces.Dict({\n",
    "    \"raycast\": spaces.Box(low=RAYCAST_MIN, high=RAYCAST_MAX, shape=RAYCAST_SHAPE, dtype=np.float32),\n",
    "    \"state\": spaces.Box(low=STATE_MIN, high=STATE_MAX, shape=STATE_SHAPE, dtype=np.float32),\n",
    "})\n",
    "action_space = spaces.Box(low=ACTION_MIN, high=ACTION_MAX, shape=ACTION_SHAPE, dtype=np.float32)\n",
    "\n",
    "# initialization of the tailored replay buffer\n",
    "rb = DictReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    observation_space=observation_space,\n",
    "    action_space=action_space,\n",
    "    device=device,\n",
    "    handle_timeout_termination=True,\n",
    "    optimize_memory_usage=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a11d9",
   "metadata": {},
   "source": [
    "# start algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62f83fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic entropy tuning\n",
    "if args.autotune:\n",
    "    target_entropy = -torch.prod(torch.Tensor(ACTION_SHAPE).to(device)).item()\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = log_alpha.exp().clamp(min=1e-4).item()\n",
    "    a_optimizer = optim.Adam([log_alpha], lr=args.alpha_lr)\n",
    "else:\n",
    "    alpha = args.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8bde39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# savepath and variables to the model checkpointing\n",
    "save_path = './new_models/' + run_name\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "best_reward = -float('inf')\n",
    "\n",
    "# to keep track of the episode length\n",
    "episodic_stats = None\n",
    "initial_movements = {}  # agent_id -> movement\n",
    "obs = collect_data_after_step(env, env_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56d39f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Start Learning\n",
      "[1200/50000] |length: 1001.00000|reward: -19.47134|success: 0.00000|collisions: 2.97023| SPS: 10\n",
      "[1500/50000] |length: 1001.00000|reward: -18.69947|success: 0.00000|collisions: 3.14229| SPS: 10\n",
      "[1800/50000] |length: 1001.00000|reward: -18.66213|success: 0.00000|collisions: 3.13532| SPS: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cicci\\AppData\\Local\\Temp\\ipykernel_23140\\3125174291.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  action, _, _, _ = actor.get_action(torch.Tensor([obs[id][0]]).to(device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2100/50000] |length: 1001.00000|reward: -18.55758|success: 0.00000|collisions: 3.09548| SPS: 10\n",
      "[2400/50000] |length: 1001.00000|reward: -18.48240|success: 0.00000|collisions: 3.14854| SPS: 10\n",
      "[2700/50000] |length: 1001.00000|reward: -18.37394|success: 0.00000|collisions: 3.16728| SPS: 10\n",
      "[3000/50000] |length: 1001.00000|reward: -18.12937|success: 0.00000|collisions: 3.30433| SPS: 10\n",
      "[3300/50000] |length: 1001.00000|reward: -17.96807|success: 0.00000|collisions: 3.32542| SPS: 10\n",
      "[3600/50000] |length: 1001.00000|reward: -17.94221|success: 0.00000|collisions: 3.39455| SPS: 10\n",
      "[3900/50000] |length: 993.22850|reward: -17.39589|success: 0.01455|collisions: 3.37553| SPS: 10\n",
      "[4200/50000] |length: 993.57300|reward: -17.27667|success: 0.01391|collisions: 3.47575| SPS: 10\n",
      "[4500/50000] |length: 994.21686|reward: -17.06456|success: 0.01270|collisions: 3.69544| SPS: 10\n",
      "[4800/50000] |length: 980.31978|reward: -16.53815|success: 0.02696|collisions: 3.85774| SPS: 10\n",
      "[5100/50000] |length: 982.11258|reward: -16.41332|success: 0.02462|collisions: 3.88464| SPS: 10\n",
      "[5400/50000] |length: 982.94983|reward: -16.29817|success: 0.02353|collisions: 3.88930| SPS: 10\n",
      "[5700/50000] |length: 984.51463|reward: -16.12185|success: 0.02149|collisions: 4.05660| SPS: 10\n",
      "[6000/50000] |length: 972.77672|reward: -15.66509|success: 0.03523|collisions: 4.00872| SPS: 10\n",
      "[6300/50000] |length: 967.16088|reward: -15.29023|success: 0.04673|collisions: 3.97538| SPS: 10\n",
      "[6600/50000] |length: 969.14600|reward: -15.05665|success: 0.04399|collisions: 4.00616| SPS: 10\n",
      "[6900/50000] |length: 971.01466|reward: -15.14832|success: 0.04141|collisions: 3.97690| SPS: 10\n",
      "[7200/50000] |length: 973.19710|reward: -15.25695|success: 0.03839|collisions: 3.99534| SPS: 10\n",
      "[7500/50000] |length: 961.22035|reward: -14.66505|success: 0.05015|collisions: 3.96922| SPS: 10\n",
      "[7800/50000] |length: 951.10793|reward: -14.54547|success: 0.06014|collisions: 3.88217| SPS: 10\n",
      "[8100/50000] |length: 954.03477|reward: -14.66276|success: 0.05661|collisions: 3.81541| SPS: 10\n",
      "[8400/50000] |length: 945.24307|reward: -14.42324|success: 0.06749|collisions: 3.75344| SPS: 10\n",
      "[8700/50000] |length: 949.30125|reward: -14.22658|success: 0.06258|collisions: 3.91388| SPS: 10\n",
      "[9000/50000] |length: 952.33408|reward: -14.24934|success: 0.05891|collisions: 3.84660| SPS: 10\n",
      "[9300/50000] |length: 955.87615|reward: -14.22168|success: 0.05462|collisions: 3.88732| SPS: 10\n",
      "[9600/50000] |length: 958.52327|reward: -14.05003|success: 0.05142|collisions: 3.86394| SPS: 10\n",
      "[9900/50000] |length: 953.21756|reward: -13.54691|success: 0.06129|collisions: 3.83600| SPS: 10\n",
      "[10200/50000] |length: 944.77802|reward: -13.15632|success: 0.07095|collisions: 3.73390| SPS: 10\n",
      "[10500/50000] |length: 938.65805|reward: -12.95174|success: 0.08113|collisions: 3.77884| SPS: 10\n",
      "[10800/50000] |length: 915.24032|reward: -11.89358|success: 0.11621|collisions: 3.61268| SPS: 10\n",
      "[11100/50000] |length: 908.16492|reward: -11.56150|success: 0.12209|collisions: 3.53809| SPS: 10\n",
      "[11400/50000] |length: 880.16619|reward: -10.36460|success: 0.16599|collisions: 3.40316| SPS: 10\n",
      "[11700/50000] |length: 866.04316|reward: -9.50276|success: 0.18049|collisions: 3.40931| SPS: 10\n",
      "[12000/50000] |length: 864.73245|reward: -9.09652|success: 0.18190|collisions: 3.40643| SPS: 10\n",
      "[12300/50000] |length: 854.87296|reward: -8.49366|success: 0.19799|collisions: 3.32018| SPS: 10\n",
      "[12600/50000] |length: 861.53360|reward: -7.96102|success: 0.19858|collisions: 3.39964| SPS: 10\n",
      "[12900/50000] |length: 869.71518|reward: -7.91272|success: 0.18693|collisions: 3.36142| SPS: 10\n",
      "[13200/50000] |length: 879.27055|reward: -7.68089|success: 0.17333|collisions: 3.45354| SPS: 10\n",
      "[13500/50000] |length: 866.31794|reward: -6.72792|success: 0.18785|collisions: 3.25256| SPS: 10\n",
      "[13800/50000] |length: 871.67276|reward: -6.00491|success: 0.18830|collisions: 3.40971| SPS: 10\n",
      "[14100/50000] |length: 879.25954|reward: -5.85082|success: 0.17725|collisions: 3.41589| SPS: 10\n",
      "[14400/50000] |length: 876.01344|reward: -4.99863|success: 0.17689|collisions: 3.40719| SPS: 10\n",
      "[14700/50000] |length: 871.51667|reward: -4.08133|success: 0.19001|collisions: 3.18439| SPS: 10\n",
      "[15000/50000] |length: 873.07882|reward: -3.35003|success: 0.20820|collisions: 3.05691| SPS: 10\n",
      "[15300/50000] |length: 868.21204|reward: -2.78258|success: 0.22260|collisions: 2.93762| SPS: 10\n",
      "[15600/50000] |length: 854.87433|reward: -1.45712|success: 0.25398|collisions: 2.73104| SPS: 10\n",
      "[15900/50000] |length: 838.54283|reward: -0.80924|success: 0.27851|collisions: 2.59072| SPS: 10\n",
      "[16200/50000] |length: 835.34940|reward: -0.16904|success: 0.30256|collisions: 2.46017| SPS: 10\n",
      "[16500/50000] |length: 808.07857|reward: 0.87871|success: 0.35480|collisions: 2.33415| SPS: 10\n",
      "[16800/50000] |length: 790.55874|reward: 1.78124|success: 0.38654|collisions: 2.15374| SPS: 10\n",
      "[17100/50000] |length: 772.22315|reward: 2.34573|success: 0.41967|collisions: 2.11312| SPS: 10\n",
      "[17400/50000] |length: 760.83072|reward: 2.65258|success: 0.43578|collisions: 2.05916| SPS: 10\n",
      "[17700/50000] |length: 712.45229|reward: 3.63710|success: 0.49404|collisions: 1.94975| SPS: 10\n",
      "[18000/50000] |length: 672.78847|reward: 4.51164|success: 0.54306|collisions: 1.85617| SPS: 10\n",
      "[18300/50000] |length: 618.44705|reward: 5.42894|success: 0.60430|collisions: 1.98929| SPS: 10\n",
      "[18600/50000] |length: 559.26528|reward: 6.29928|success: 0.65338|collisions: 2.01290| SPS: 10\n",
      "[18900/50000] |length: 534.35106|reward: 6.81164|success: 0.69778|collisions: 2.16730| SPS: 10\n",
      "[19200/50000] |length: 529.64849|reward: 7.00275|success: 0.70818|collisions: 2.38387| SPS: 10\n",
      "[19500/50000] |length: 511.51794|reward: 7.46244|success: 0.73521|collisions: 2.52594| SPS: 10\n",
      "[19800/50000] |length: 506.96703|reward: 7.19767|success: 0.74430|collisions: 2.73968| SPS: 10\n",
      "[20100/50000] |length: 453.99794|reward: 7.93058|success: 0.78653|collisions: 2.43013| SPS: 10\n",
      "[20400/50000] |length: 426.73259|reward: 8.15850|success: 0.80421|collisions: 2.44401| SPS: 10\n",
      "[20700/50000] |length: 407.20121|reward: 8.63894|success: 0.82191|collisions: 2.33947| SPS: 10\n",
      "[21000/50000] |length: 379.44849|reward: 9.33208|success: 0.85012|collisions: 2.16581| SPS: 10\n",
      "[21300/50000] |length: 333.33886|reward: 10.20175|success: 0.88922|collisions: 1.88552| SPS: 10\n",
      "[21600/50000] |length: 319.59246|reward: 10.49681|success: 0.91432|collisions: 1.71545| SPS: 10\n",
      "[21900/50000] |length: 318.35457|reward: 9.99553|success: 0.90008|collisions: 1.65721| SPS: 10\n",
      "[22200/50000] |length: 330.01619|reward: 9.39411|success: 0.87445|collisions: 1.50318| SPS: 10\n",
      "[22500/50000] |length: 352.74069|reward: 9.43348|success: 0.88706|collisions: 1.55261| SPS: 10\n",
      "[22800/50000] |length: 331.49878|reward: 9.71292|success: 0.88949|collisions: 1.36034| SPS: 10\n",
      "[23100/50000] |length: 346.47890|reward: 8.89398|success: 0.86176|collisions: 1.39420| SPS: 10\n",
      "[23400/50000] |length: 356.79729|reward: 8.85459|success: 0.85615|collisions: 1.32114| SPS: 10\n",
      "[23700/50000] |length: 324.48627|reward: 9.64549|success: 0.89041|collisions: 1.24769| SPS: 10\n",
      "[24000/50000] |length: 312.47125|reward: 10.16991|success: 0.90254|collisions: 1.25552| SPS: 10\n",
      "[24300/50000] |length: 277.22049|reward: 10.50169|success: 0.93011|collisions: 1.24704| SPS: 10\n",
      "[24600/50000] |length: 279.15336|reward: 10.62744|success: 0.92951|collisions: 1.22400| SPS: 10\n",
      "[24900/50000] |length: 265.12320|reward: 10.58659|success: 0.93397|collisions: 1.09240| SPS: 10\n",
      "[25200/50000] |length: 267.36616|reward: 10.81913|success: 0.94493|collisions: 1.07186| SPS: 10\n",
      "[25500/50000] |length: 308.27835|reward: 9.92053|success: 0.90837|collisions: 1.29447| SPS: 10\n",
      "[25800/50000] |length: 314.69456|reward: 10.14834|success: 0.89814|collisions: 1.38318| SPS: 10\n",
      "[26100/50000] |length: 313.60752|reward: 10.44708|success: 0.90425|collisions: 1.49684| SPS: 10\n",
      "[26400/50000] |length: 277.89597|reward: 10.98251|success: 0.93029|collisions: 1.29815| SPS: 10\n",
      "[26700/50000] |length: 276.02399|reward: 11.01664|success: 0.93511|collisions: 1.16279| SPS: 10\n",
      "[27000/50000] |length: 269.63734|reward: 11.37502|success: 0.94905|collisions: 1.04525| SPS: 10\n",
      "[27300/50000] |length: 276.93258|reward: 11.63964|success: 0.96000|collisions: 1.13784| SPS: 10\n",
      "[27600/50000] |length: 250.95263|reward: 11.93193|success: 0.97043|collisions: 0.92980| SPS: 10\n",
      "[27900/50000] |length: 261.64705|reward: 11.22876|success: 0.95008|collisions: 0.97819| SPS: 10\n",
      "[28200/50000] |length: 282.01199|reward: 10.85088|success: 0.91779|collisions: 1.19958| SPS: 10\n",
      "[28500/50000] |length: 271.25027|reward: 11.33309|success: 0.93737|collisions: 1.12960| SPS: 10\n",
      "[28800/50000] |length: 256.08711|reward: 11.79228|success: 0.95301|collisions: 1.03366| SPS: 10\n",
      "[29100/50000] |length: 269.72960|reward: 11.33075|success: 0.93454|collisions: 1.05241| SPS: 10\n",
      "[29400/50000] |length: 278.39582|reward: 10.89364|success: 0.91790|collisions: 1.19852| SPS: 10\n",
      "[29700/50000] |length: 265.06879|reward: 11.24492|success: 0.93554|collisions: 1.17513| SPS: 10\n",
      "[30000/50000] |length: 268.64909|reward: 11.47385|success: 0.93875|collisions: 1.23892| SPS: 10\n",
      "[30300/50000] |length: 250.44918|reward: 11.82472|success: 0.95541|collisions: 1.07703| SPS: 10\n",
      "[30600/50000] |length: 224.99224|reward: 12.08409|success: 0.96850|collisions: 0.97759| SPS: 10\n",
      "[30900/50000] |length: 234.79815|reward: 12.03984|success: 0.96313|collisions: 0.92273| SPS: 10\n",
      "[31200/50000] |length: 234.30068|reward: 12.26804|success: 0.97148|collisions: 0.90363| SPS: 10\n",
      "[31500/50000] |length: 231.95661|reward: 12.12540|success: 0.96714|collisions: 0.82289| SPS: 10\n",
      "[31800/50000] |length: 230.26100|reward: 12.25076|success: 0.97571|collisions: 0.87594| SPS: 10\n",
      "[32100/50000] |length: 225.68394|reward: 12.49219|success: 0.98150|collisions: 0.85532| SPS: 10\n",
      "[32400/50000] |length: 229.45428|reward: 12.04684|success: 0.97299|collisions: 0.82325| SPS: 10\n",
      "[32700/50000] |length: 222.04807|reward: 11.81657|success: 0.97016|collisions: 1.02505| SPS: 10\n",
      "[33000/50000] |length: 214.15012|reward: 12.07918|success: 0.97761|collisions: 0.95880| SPS: 10\n",
      "[33300/50000] |length: 204.82518|reward: 12.21295|success: 0.98489|collisions: 1.07201| SPS: 10\n",
      "[33600/50000] |length: 208.04256|reward: 11.90199|success: 0.97366|collisions: 1.11796| SPS: 10\n",
      "[33900/50000] |length: 203.16179|reward: 12.12692|success: 0.98111|collisions: 1.03141| SPS: 10\n",
      "[34200/50000] |length: 217.13158|reward: 11.87643|success: 0.96994|collisions: 1.02764| SPS: 10\n",
      "[34500/50000] |length: 237.36985|reward: 11.82055|success: 0.96046|collisions: 1.24664| SPS: 10\n",
      "[34800/50000] |length: 242.45592|reward: 11.57741|success: 0.95792|collisions: 1.36055| SPS: 10\n",
      "[35100/50000] |length: 236.88361|reward: 11.63302|success: 0.95874|collisions: 1.30247| SPS: 10\n",
      "[35400/50000] |length: 229.60391|reward: 12.05711|success: 0.96950|collisions: 1.24185| SPS: 10\n",
      "[35700/50000] |length: 229.44692|reward: 12.00250|success: 0.96320|collisions: 1.19554| SPS: 10\n",
      "[36000/50000] |length: 235.74847|reward: 11.71122|success: 0.95407|collisions: 1.16643| SPS: 10\n",
      "[36300/50000] |length: 263.66516|reward: 10.87685|success: 0.92421|collisions: 1.33013| SPS: 10\n",
      "[36600/50000] |length: 238.75243|reward: 11.35561|success: 0.94565|collisions: 1.20584| SPS: 10\n",
      "[36900/50000] |length: 240.90696|reward: 11.13599|success: 0.94467|collisions: 1.18253| SPS: 10\n",
      "[37200/50000] |length: 240.26843|reward: 11.15267|success: 0.94393|collisions: 1.11554| SPS: 10\n",
      "[37500/50000] |length: 224.39365|reward: 11.56474|success: 0.95979|collisions: 1.09978| SPS: 10\n",
      "[37800/50000] |length: 228.79901|reward: 11.46861|success: 0.95396|collisions: 1.06818| SPS: 10\n",
      "[38100/50000] |length: 246.05675|reward: 10.94256|success: 0.93624|collisions: 1.11182| SPS: 10\n",
      "[38400/50000] |length: 243.23909|reward: 10.55391|success: 0.92739|collisions: 1.06936| SPS: 10\n",
      "[38700/50000] |length: 241.11535|reward: 11.05921|success: 0.94299|collisions: 0.94796| SPS: 10\n",
      "[39000/50000] |length: 236.65858|reward: 11.35421|success: 0.94786|collisions: 0.92483| SPS: 10\n",
      "[39300/50000] |length: 217.85969|reward: 11.70520|success: 0.96261|collisions: 0.86787| SPS: 10\n",
      "[39600/50000] |length: 243.06122|reward: 11.08225|success: 0.92972|collisions: 0.90211| SPS: 10\n",
      "[39900/50000] |length: 254.18914|reward: 10.89166|success: 0.92748|collisions: 1.08498| SPS: 10\n",
      "[40200/50000] |length: 246.18875|reward: 10.97163|success: 0.93542|collisions: 1.13845| SPS: 10\n",
      "[40500/50000] |length: 233.78566|reward: 11.47654|success: 0.95080|collisions: 1.03775| SPS: 10\n",
      "[40800/50000] |length: 233.45988|reward: 11.64051|success: 0.95293|collisions: 1.07504| SPS: 10\n",
      "[41100/50000] |length: 241.09832|reward: 11.26992|success: 0.93535|collisions: 0.97772| SPS: 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 190\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m q, q_t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(qf_ensemble, qf_ensemble_target):\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m param, target_param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(q\u001b[38;5;241m.\u001b[39mparameters(), q_t\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[1;32m--> 190\u001b[0m             \u001b[43mtarget_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Log training losses and stats\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mloss_log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "print('Start Training')\n",
    "start_time = time.time()\n",
    "global_step = 0\n",
    "while global_step < args.total_timesteps:\n",
    "\n",
    "    # actions for each agent in the environment\n",
    "    # dim = (naagents, action_space)\n",
    "    for id in obs:\n",
    "        agent_obs = obs[id]\n",
    "        \n",
    "        # terminated agents are not considered\n",
    "        if agent_obs[4]:\n",
    "            continue\n",
    "        \n",
    "        # algo logic\n",
    "        if global_step < args.learning_starts * 2:\n",
    "            # change this to use the handcrafted starting policy or a previously trained policy\n",
    "            \n",
    "            action = get_initial_action(id, initial_movements)\n",
    "            # action, _, _ = old_actor.get_action(torch.Tensor([obs[id][0]]), \n",
    "            #                                 torch.Tensor([obs[id][1]]),\n",
    "            #                                 0.5)\n",
    "            # action = action[0].detach().numpy()\n",
    "        else:\n",
    "            # training policy\n",
    "            action, _, _, _ = actor.get_action(torch.Tensor([obs[id][0]]).to(device), \n",
    "                                            torch.Tensor([obs[id][1]]).to(device))\n",
    "            action = action[0].detach().cpu().numpy()\n",
    "        \n",
    "        # memorize the action taken for the next step\n",
    "        agent_obs[3] = action\n",
    "        \n",
    "        # the first dimention of the action is the \"number of agent\"\n",
    "        # Always 1 if \"set_action_for_agent\" is used\n",
    "        a = ActionTuple(continuous=np.array([action]))\n",
    "        env.set_action_for_agent(BEHAVIOUR_NAME, id, a)\n",
    "    \n",
    "    # environment step\n",
    "    env.step()\n",
    "    next_obs = collect_data_after_step(env, env_info)\n",
    "         \n",
    "    # asynchronous reception of other info from ended episode\n",
    "    while env_info.msg_queue:\n",
    "        msg = env_info.msg_queue.pop()\n",
    "        \n",
    "        if global_step >= args.learning_starts:\n",
    "            if episodic_stats == None:\n",
    "                episodic_stats = {\n",
    "                    \"length\": msg[\"length\"],\n",
    "                    \"reward\": msg[\"reward\"],\n",
    "                    \"success\": msg[\"success\"],\n",
    "                    \"collisions\": msg[\"collisions\"],\n",
    "                }\n",
    "            else:\n",
    "                for s in episodic_stats:\n",
    "                    episodic_stats[s] = episodic_stats[s]*args.metrics_smoothing + (1 - args.metrics_smoothing)*msg[s]\n",
    "        \n",
    "    # save data to reply buffer; handle `terminal_observation`\n",
    "    for id in obs:\n",
    "        prev_agent_obs = obs[id]\n",
    "        # consider every agent that in the previous step was not terminated\n",
    "        # in this way are excluded the agents that are already considered before and don't have a \n",
    "        # couple prev_obs - next_obs and a reward\n",
    "        if prev_agent_obs[4] or id not in next_obs:\n",
    "            continue\n",
    "            \n",
    "        next_agent_obs = next_obs[id]\n",
    "        \n",
    "        # add the data to the replay buffer\n",
    "        rb.add(obs = {'raycast': prev_agent_obs[0], 'state': prev_agent_obs[1]}, \n",
    "               next_obs = {'raycast': next_agent_obs[0], 'state': next_agent_obs[1]},\n",
    "               action = np.array(prev_agent_obs[3]), \n",
    "               reward = next_agent_obs[2], \n",
    "               done = next_agent_obs[4],\n",
    "               infos = [{}])\n",
    "        \n",
    "    # crucial step, easy to overlook, update the previous observation\n",
    "    obs = next_obs\n",
    "    \n",
    "    # Training loop\n",
    "    for _ in range(args.update_per_step):\n",
    "\n",
    "        # Log episodic stats periodically\n",
    "        if episodic_stats is not None and global_step % args.metrics_log_interval == 0:\n",
    "            print_text = f\"[{global_step}/{args.total_timesteps}] \"\n",
    "            for s in episodic_stats:\n",
    "                writer.add_scalar(\"episodic_stats/\" + s, episodic_stats[s], global_step)\n",
    "                print_text += f\"|{s}: {episodic_stats[s]:.5f}\"\n",
    "            print_text += f'| SPS: {int(global_step / (time.time() - start_time))}'\n",
    "            print(print_text)\n",
    "\n",
    "        # Save best models based on reward\n",
    "        if episodic_stats is not None and episodic_stats[\"reward\"] > best_reward:\n",
    "            best_reward = episodic_stats[\"reward\"]\n",
    "            torch.save(actor.state_dict(), os.path.join(save_path, 'actor_best.pth'))\n",
    "            for i, qf in enumerate(qf_ensemble):\n",
    "                torch.save(qf.state_dict(), os.path.join(save_path, f'qf{i+1}_best.pth'))\n",
    "            for i, qft in enumerate(qf_ensemble_target):\n",
    "                torch.save(qft.state_dict(), os.path.join(save_path, f'qf{i+1}_target_best.pth'))\n",
    "\n",
    "        # Start learning after a warm-up phase\n",
    "        if global_step > args.learning_starts:\n",
    "\n",
    "            # Sample a batch from replay buffer\n",
    "            data = rb.sample(args.batch_size)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Compute target action with exploration noise\n",
    "                next_action, next_log_pi, _, _ = actor.get_action(\n",
    "                    data.next_observations['raycast'], \n",
    "                    data.next_observations['state']\n",
    "                )\n",
    "\n",
    "                if args.noise_clip > 0:\n",
    "                    noise = torch.randn_like(next_action) * args.noise_clip\n",
    "                    next_action = torch.clamp(next_action + noise, -1, 1)\n",
    "\n",
    "                # Compute target Q-value (min over ensemble)\n",
    "                target_q_values = []\n",
    "                for q_target in qf_ensemble_target:\n",
    "                    q_val = q_target(\n",
    "                        data.next_observations['raycast'], \n",
    "                        data.next_observations['state'], \n",
    "                        next_action\n",
    "                    )\n",
    "                    target_q_values.append(q_val)\n",
    "                stacked_target_q = torch.stack(target_q_values)\n",
    "                min_qf_next_target = stacked_target_q.min(dim=0).values - alpha * next_log_pi\n",
    "                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * min_qf_next_target.view(-1)\n",
    "\n",
    "            # Q-function updates (with bootstrapping)\n",
    "            q_losses = []\n",
    "            q_vals = []\n",
    "            batch_size = int(data.actions.shape[0] * args.bootstrap_batch_proportion)\n",
    "            for q in qf_ensemble:\n",
    "                # Bootstrap indices\n",
    "                indices = torch.randint(0, batch_size, (batch_size,), device=data.actions.device)\n",
    "                \n",
    "                obs_raycast = data.observations['raycast'][indices]\n",
    "                obs_state = data.observations['state'][indices]\n",
    "                actions = data.actions[indices]\n",
    "                target = next_q_value[indices]\n",
    "\n",
    "                # Compute Q loss\n",
    "                q_val = q(obs_raycast, obs_state, actions).view(-1)\n",
    "                loss = F.mse_loss(q_val, target)\n",
    "                q_losses.append(loss)\n",
    "                q_vals.append(q_val)\n",
    "                \n",
    "            total_q_loss = torch.stack(q_losses).mean()\n",
    "            qf_optimizer.zero_grad()\n",
    "            total_q_loss.backward()\n",
    "            qf_optimizer.step()\n",
    "            \n",
    "            # Track Q-value statistics\n",
    "            q_std = torch.stack(q_vals).std(dim=0).mean().item()\n",
    "            q_vals = torch.stack(q_vals).mean()\n",
    "            \n",
    "            # Delayed policy (actor) update\n",
    "            if global_step % args.policy_frequency == 0:\n",
    "                for _ in range(args.policy_frequency):\n",
    "                    pi, log_pi, _, _ = actor.get_action(data.observations['raycast'], data.observations['state'])\n",
    "                    actor_entropy = - (log_pi.exp() * log_pi).sum(dim=-1).mean()\n",
    "\n",
    "                    q_pi_vals = [q(data.observations['raycast'], data.observations['state'], pi) for q in qf_ensemble]\n",
    "                    min_qf_pi = torch.min(torch.stack(q_pi_vals), dim=0).values.view(-1)\n",
    "\n",
    "                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "                    actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_optimizer.step()\n",
    "\n",
    "                    # Automatic entropy tuning (if enabled)\n",
    "                    if args.autotune:\n",
    "                        with torch.no_grad():\n",
    "                            _, log_pi, _, _ = actor.get_action(data.observations['raycast'], data.observations['state'])\n",
    "                        alpha_loss = (-log_alpha * (log_pi + target_entropy)).mean()\n",
    "\n",
    "                        a_optimizer.zero_grad()\n",
    "                        alpha_loss.backward()\n",
    "                        a_optimizer.step()\n",
    "                        alpha = log_alpha.exp().item()\n",
    "\n",
    "            # Soft update target Q-networks\n",
    "            if global_step % args.target_network_frequency == 0:\n",
    "                for q, q_t in zip(qf_ensemble, qf_ensemble_target):\n",
    "                    for param, target_param in zip(q.parameters(), q_t.parameters()):\n",
    "                        target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "            # Log training losses and stats\n",
    "            if global_step % args.loss_log_interval == 0:\n",
    "                for i in range(len(qf_ensemble)):\n",
    "                    writer.add_scalar(f\"q_loss/qf{i+1}\", q_losses[i].item(), global_step)\n",
    "\n",
    "                writer.add_scalar(\"loss/qf_mean\", torch.stack(q_losses).mean().item(), global_step)\n",
    "                writer.add_scalar(\"loss/actor\", actor_loss.item(), global_step)\n",
    "                \n",
    "                writer.add_scalar(\"stats/qf_val_mean \", q_vals, global_step)\n",
    "                writer.add_scalar(\"stats/qf_val_var \", q_std, global_step)\n",
    "                \n",
    "                writer.add_scalar(\"stats/policy_entropy\", actor_entropy.item(), global_step)\n",
    "                writer.add_scalar(\"stats/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "                if args.autotune:\n",
    "                    writer.add_scalar(\"loss/alpha\", alpha, global_step)\n",
    "                    writer.add_scalar(\"loss/alpha_loss\", alpha_loss.item(), global_step)\n",
    "\n",
    "        elif global_step == args.learning_starts:\n",
    "            print(\"Start Learning\")\n",
    "\n",
    "        # Step counter\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c8b7d",
   "metadata": {},
   "source": [
    "# Close Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb5d4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a243c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained networks, actor and critics\n",
    "torch.save(actor.state_dict(), os.path.join(save_path, 'actor_final.pth'))\n",
    "for i, qf in enumerate(qf_ensemble):\n",
    "    torch.save(qf.state_dict(), os.path.join(save_path, f'qf{i+1}_final.pth'))\n",
    "for i, qft in enumerate(qf_ensemble_target):\n",
    "    torch.save(qft.state_dict(), os.path.join(save_path, f'qf{i+1}_target_final.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
