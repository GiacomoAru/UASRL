# ==========================================
# 1. SETUP ESPERIMENTO E HARDWARE
# ==========================================
exp_name: "forse_va"
machine_name: "personal"
seed: 180618
cuda: 0                 # indice gpu o -1 per cpu
torch_deterministic: false
wandb: true
worker_id: 0

# ==========================================
# 2. AMBIENTE E CONFIGURAZIONE UNITY
# ==========================================
env_id: "6xstd"
n_envs: 6
build_path: "./unity_build/6xnew_reward_wind/UASRL.exe"
headless: false
test_lib: false
base_time: 1765457030

# Percorsi configurazioni specifiche
agent_config_path: "./train_config/agent_new_rew.yaml"
obstacles_config_path: "./train_config/obstacles_new_rew.yaml"
other_config_path: "./train_config/other_new_rew.yaml"

# ==========================================
# 3. TRAINING FLOW & MEMORY
# ==========================================
total_timesteps: 2000000
learning_starts: 1000          # Passi casuali iniziali prima di allenare
buffer_size: 100000
batch_size: 512

# Bootstrapping (Ensemble sampling)
bootstrap: false               # Se usare il bootstrap nell'ensemble Q
bootstrap_batch_proportion: 0.8

# ==========================================
# 4. ARCHITETTURA RETI NEURALI
# ==========================================
input_stack: 4
q_ensemble_n: 5                # Numero di reti Q nell'ensemble
actor_network_layers: [128, 128, 128]
q_network_layers: [128, 128, 128]

# ==========================================
# 5. IPERPARAMETRI SAC (OPTIMIZATION)
# ==========================================
gamma: 0.99                    # Discount factor
tau: 0.005                      # Soft update per target networks
reward_scale: 1.0

# Learning Rates
policy_lr: 0.005 
q_lr: 0.005
alpha_lr: 0.0002                 # default solitamente = q_lr

# Entropia (Alpha)
alpha: 1.0                     # Coefficiente entropia
autotune: true                 # Tuning automatico di alpha

# ==========================================
# 6. LOGICA DI AGGIORNAMENTO
# ==========================================
update_frequency: 2            # Quanti gradient updates per step ambientale
policy_frequency: 1            # Ogni quanti step gradiente aggiornare la policy
target_network_update_period: 2 # Ogni quanti step aggiornare la target net

# ==========================================
# 7. CURRICULUM LEARNING
# ==========================================
curriculum_steps: 5            # applicato a inital_fill_percentage, 1 = no curriculum
min_success_rate: 0.8         # soglia per passare al passo successivo
min_episodes_per_curriculum: 50 # episodi minimi tra un livello e l'altro

# ==========================================
# 8. LOGGING E METRICHE
# ==========================================
loss_log_interval: 500         # gradient steps
metrics_log_interval: 5       # episodes
metrics_smoothing: 0.96        # smoothing esponenziale (0.96 ~= ultimi 50 ep)